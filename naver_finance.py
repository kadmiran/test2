#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ê¸ˆìœµ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í¬ë¡¤ëŸ¬
ì¢…ëª©ë¶„ì„ ë° ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
import os
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import fitz  # PyMuPDF
from config import config


class NaverFinanceCrawler:
    """ë„¤ì´ë²„ ê¸ˆìœµ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, llm_orchestrator=None):
        """
        ì´ˆê¸°í™”
        
        Args:
            llm_orchestrator: LLM Orchestrator (íšŒì‚¬ëª… ë³€í˜• ì¶”ì²œìš©, ì„ íƒì‚¬í•­)
        """
        self.llm_orchestrator = llm_orchestrator
        # configì—ì„œ User-Agent ê°€ì ¸ì˜¤ê¸°
        self.headers = {
            'User-Agent': config.CRAWLER_USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': config.NAVER_FINANCE_BASE_URL
        }
    
    def _get_company_name_variations(self, company_name):
        """
        LLM Orchestratorë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì‚¬ëª…ì˜ ë‹¤ì–‘í•œ í‘œê¸° ê°€ì ¸ì˜¤ê¸°
        
        Args:
            company_name: ì…ë ¥ëœ íšŒì‚¬ëª…
            
        Returns:
            list: ê°€ëŠ¥í•œ íšŒì‚¬ëª… í‘œê¸° ë¦¬ìŠ¤íŠ¸
        """
        if not self.llm_orchestrator:
            return [company_name]
        
        try:
            prompt = f"""
í•œêµ­ ìƒì¥íšŒì‚¬ "{company_name}"ì˜ ê³µì‹ ëª…ì¹­ê³¼ ê°€ëŠ¥í•œ ëª¨ë“  í‘œê¸° ë°©ë²•ì„ ë‚˜ì—´í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
- "KT" ì…ë ¥ â†’ ["KT", "ì¼€ì´í‹°", "ì£¼ì‹íšŒì‚¬ ì¼€ì´í‹°", "ì£¼ì‹íšŒì‚¬ KT", "Korea Telecom"]
- "ì‚¼ì„±ì „ì" ì…ë ¥ â†’ ["ì‚¼ì„±ì „ì", "ì‚¼ì„±ì „ìì£¼ì‹íšŒì‚¬", "Samsung Electronics"]
- "SKí…”ë ˆì½¤" ì…ë ¥ â†’ ["SKí…”ë ˆì½¤", "ì—ìŠ¤ì¼€ì´í…”ë ˆì½¤", "SK Telecom"]

"{company_name}"ì— ëŒ€í•´ ê°€ëŠ¥í•œ ëª¨ë“  í‘œê¸°ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì˜¤ì§ íšŒì‚¬ëª… ë¦¬ìŠ¤íŠ¸ë§Œ ì‘ì„±í•˜ê³ , ì„¤ëª…ì€ ë„£ì§€ ë§ˆì„¸ìš”.
í˜•ì‹: í‘œê¸°1, í‘œê¸°2, í‘œê¸°3
"""
            
            variations_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='name_variation'
            ).strip()
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            variations = [v.strip() for v in variations_text.split(',')]
            
            # ì›ë˜ ì…ë ¥ê°’ë„ í¬í•¨
            if company_name not in variations:
                variations.insert(0, company_name)
            
            print(f"   ğŸ¤– LLM ì¶”ì²œ ê²€ìƒ‰ì–´: {variations}")
            return variations
            
        except Exception as e:
            print(f"   âš ï¸  LLM ê²€ìƒ‰ì–´ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return [company_name]
    
    def _extract_text_from_pdf(self, pdf_path):
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            print(f"      ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {os.path.basename(pdf_path)}")
            text = ""
            
            with fitz.open(pdf_path) as doc:
                total_pages = len(doc)
                
                for page_num, page in enumerate(doc, 1):
                    page_text = page.get_text()
                    text += f"\n--- í˜ì´ì§€ {page_num} ---\n{page_text}"
                    
                    if page_num % 10 == 0:
                        print(f"         âœ“ {page_num}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
                
                return text
                
        except Exception as e:
            print(f"      âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def search_company_reports(self, company_name, max_reports=5):
        """
        ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ
        
        Args:
            company_name: íšŒì‚¬ëª…
            max_reports: ìµœëŒ€ ë‹¤ìš´ë¡œë“œ ìˆ˜
            
        Returns:
            list: [{name, date, content, url}, ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“Š ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ '{company_name}' ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ ì¤‘...")
            
            # LLM Orchestratorë¡œ íšŒì‚¬ëª… ë³€í˜• ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸°
            if self.llm_orchestrator:
                print(f"ğŸ¤– LLM Orchestratorì—ê²Œ '{company_name}'ì˜ ê²€ìƒ‰ì–´ ì¶”ì²œ ìš”ì²­ ì¤‘...")
                search_variations = self._get_company_name_variations(company_name)
            else:
                search_variations = [company_name]
            
            # ë„¤ì´ë²„ ì¦ê¶Œ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ URL
            base_url = "https://finance.naver.com/research/company_list.naver"
            
            # ëª¨ë“  ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ ì‹œë„
            all_reports = []
            seen_urls = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            for variation_idx, search_term in enumerate(search_variations):
                print(f"\n   ğŸ” [{variation_idx + 1}/{len(search_variations)}] ê²€ìƒ‰ì–´ '{search_term}'ë¡œ ê²€ìƒ‰ ì¤‘...")
                
                try:
                    # ê²€ìƒ‰ì–´ EUC-KR ì¸ì½”ë”©
                    keyword_euckr = search_term.encode('euc-kr')
                    encoded_keyword = quote(keyword_euckr)
                    
                    # URL ì§ì ‘ êµ¬ì„±
                    search_url = f"{base_url}?searchType=keyword&keyword={encoded_keyword}&brokerCode=&writeFromDate=&writeToDate=&itemName=&itemCode="
                    
                    print(f"      EUC-KR ì¸ì½”ë”©: {encoded_keyword}")
                    
                    # ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ
                    response = requests.get(search_url, headers=self.headers, timeout=30)
                    response.raise_for_status()
                    response.encoding = 'euc-kr'
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´ì¼ ë•Œë§Œ HTML ë””ë²„ê¹… íŒŒì¼ ì €ì¥
                    if variation_idx == 0:
                        debug_html_path = 'downloads/debug_company_list.html'
                        os.makedirs('downloads', exist_ok=True)
                        with open(debug_html_path, 'w', encoding='utf-8') as f:
                            f.write(soup.prettify())
                        print(f"      ğŸ” HTML ì €ì¥ë¨: {debug_html_path}")
                    
                    # í…Œì´ë¸” ì°¾ê¸°
                    table = soup.find('table', class_='type_1')
                    if not table:
                        tables = soup.find_all('table')
                        for t in tables:
                            rows = t.find_all('tr')
                            if len(rows) > 5:
                                table = t
                                break
                    
                    if not table:
                        print(f"      âš ï¸  í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        continue
                    
                    rows = table.find_all('tr')
                    
                    # í—¤ë” ë¶„ì„ (ì²« ê²€ìƒ‰ì–´ì¼ ë•Œë§Œ)
                    if variation_idx == 0:
                        header_map = {}
                        if len(rows) > 0:
                            first_row = rows[0]
                            header_cells = first_row.find_all(['td', 'th'])
                            print(f"      ğŸ“Š í—¤ë”: {len(header_cells)}ê°œ ì—´")
                            for idx, cell in enumerate(header_cells):
                                header_text = cell.get_text(strip=True)
                                header_map[header_text] = idx
                                print(f"         ì—´ {idx}: '{header_text}'")
                        
                        stock_idx = header_map.get('ì¢…ëª©ëª…', 0)
                        title_idx = header_map.get('ì œëª©', 1)
                        broker_idx = header_map.get('ì¦ê¶Œì‚¬', 2)
                        pdf_idx = header_map.get('ì²¨ë¶€', 3)
                        date_idx = header_map.get('ì‘ì„±ì¼', 4)
                    
                    # ë¦¬í¬íŠ¸ íŒŒì‹±
                    found_count = 0
                    for row in rows[1:]:  # í—¤ë” ì œì™¸
                        try:
                            cells = row.find_all('td')
                            if len(cells) < 5:
                                continue
                            
                            # ë°ì´í„° ì¶”ì¶œ
                            stock_name = cells[stock_idx].get_text(strip=True) if len(cells) > stock_idx else ""
                            
                            title_cell = cells[title_idx]
                            title_link = title_cell.find('a')
                            if not title_link:
                                continue
                            
                            title = title_link.get_text(strip=True)
                            if not title:
                                continue
                            
                            broker = cells[broker_idx].get_text(strip=True) if len(cells) > broker_idx else "ì¦ê¶Œì‚¬"
                            
                            pdf_cell = cells[pdf_idx]
                            pdf_link = pdf_cell.find('a')
                            if not pdf_link or not pdf_link.get('href'):
                                continue
                            
                            pdf_url = pdf_link.get('href')
                            
                            # ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if not pdf_url.startswith('http'):
                                if pdf_url.startswith('//'):
                                    pdf_url = 'https:' + pdf_url
                                else:
                                    pdf_url = urljoin('https://stock.pstatic.net', pdf_url)
                            
                            # ì¤‘ë³µ ì²´í¬
                            if pdf_url in seen_urls:
                                continue
                            seen_urls.add(pdf_url)
                            
                            date = cells[date_idx].get_text(strip=True) if len(cells) > date_idx else "ë‚ ì§œë¯¸ìƒ"
                            
                            all_reports.append({
                                'stock_name': stock_name,
                                'title': title,
                                'broker': broker,
                                'date': date,
                                'pdf_url': pdf_url
                            })
                            
                            found_count += 1
                            
                        except Exception as row_error:
                            continue
                    
                    print(f"      âœ“ {found_count}ê°œ ë¦¬í¬íŠ¸ ë°œê²¬")
                    
                    # ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ë¥¼ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                    if len(all_reports) >= max_reports * 2:
                        print(f"      â„¹ï¸  ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘, ê²€ìƒ‰ ì¤‘ë‹¨")
                        break
                    
                    # ë‹¤ìŒ ê²€ìƒ‰ì–´ ì‚¬ì´ì— ì§§ì€ ì§€ì—°
                    time.sleep(0.5)
                    
                except Exception as search_error:
                    print(f"      âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨: {search_error}")
                    continue
            
            if not all_reports:
                print(f"\n   âš ï¸  '{company_name}'ì— ëŒ€í•œ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
            def parse_date(date_str):
                """YY.MM.DD í˜•ì‹ì„ ì •ë ¬ ê°€ëŠ¥í•œ ê°’ìœ¼ë¡œ ë³€í™˜"""
                try:
                    if not date_str or date_str == "ë‚ ì§œë¯¸ìƒ":
                        return "00.00.00"
                    return date_str
                except:
                    return "00.00.00"
            
            all_reports.sort(key=lambda x: parse_date(x['date']), reverse=True)
            
            # ìƒìœ„ Nê°œë§Œ ì„ íƒ
            selected_reports = all_reports[:max_reports]
            
            print(f"\n   âœ… ì´ {len(all_reports)}ê°œ ë¦¬í¬íŠ¸ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)")
            print(f"   ğŸ“‹ ì„ ì •ëœ ë¦¬í¬íŠ¸ (ìµœì‹  {len(selected_reports)}ê°œ):")
            for idx, report in enumerate(selected_reports, 1):
                print(f"      [{idx}] {report['stock_name']} - {report['title']}")
                print(f"          ì¦ê¶Œì‚¬: {report['broker']}, ë‚ ì§œ: {report['date']}")
                print(f"          PDF: {report['pdf_url'][:80]}...")
            print()
            
            # PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_reports = []
            
            for i, report in enumerate(selected_reports, 1):
                try:
                    print(f"\n   [{i}/{len(selected_reports)}] PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {report['title'][:50]}...")
                    
                    # PDF ë‹¤ìš´ë¡œë“œ
                    pdf_response = requests.get(report['pdf_url'], headers=self.headers, timeout=60)
                    pdf_response.raise_for_status()
                    
                    # ì„ì‹œ PDF íŒŒì¼ ì €ì¥
                    pdf_filename = f"downloads/naver_report_{i}_{int(time.time())}.pdf"
                    os.makedirs('downloads', exist_ok=True)
                    
                    with open(pdf_filename, 'wb') as f:
                        f.write(pdf_response.content)
                    
                    print(f"      ğŸ’¾ PDF ì €ì¥: {len(pdf_response.content):,} bytes")
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_content = self._extract_text_from_pdf(pdf_filename)
                    
                    if text_content:
                        extracted_reports.append({
                            'name': f"[{report['broker']}] {report['title']}",
                            'date': report['date'],
                            'content': text_content,
                            'url': report['pdf_url'],
                            'stock_name': report['stock_name']
                        })
                        print(f"      âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text_content):,}ì")
                    else:
                        print(f"      âš ï¸  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    
                    # PDF íŒŒì¼ ì‚­ì œ
                    try:
                        os.remove(pdf_filename)
                    except:
                        pass
                    
                    # ì§§ì€ ì§€ì—°
                    time.sleep(1)
                    
                except Exception as download_error:
                    print(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {download_error}")
                    continue
            
            print(f"\n   âœ… ìµœì¢… {len(extracted_reports)}ê°œ ë¦¬í¬íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
            return extracted_reports
            
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_industry_reports(self, industry_keywords, max_reports=5):
        """
        ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ
        
        Args:
            industry_keywords: ê²€ìƒ‰í•  ì‚°ì—… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬"])
            max_reports: ìµœëŒ€ ë‹¤ìš´ë¡œë“œ ìˆ˜
            
        Returns:
            list: [{name, date, content, url}, ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“Š ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ ì¤‘...")
            print(f"   ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {industry_keywords}")
            
            # í‚¤ì›Œë“œê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(industry_keywords, str):
                industry_keywords = [kw.strip() for kw in industry_keywords.split(',')]
            
            # ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            search_variations = industry_keywords
            
            # ë„¤ì´ë²„ ì¦ê¶Œ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ URL
            base_url = "https://finance.naver.com/research/industry_list.naver"
            
            # ëª¨ë“  ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ ì‹œë„
            all_reports = []
            seen_urls = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            for variation_idx, search_term in enumerate(search_variations):
                print(f"\n   ğŸ” [{variation_idx + 1}/{len(search_variations)}] ê²€ìƒ‰ì–´ '{search_term}'ë¡œ ê²€ìƒ‰ ì¤‘...")
                
                try:
                    # ê²€ìƒ‰ì–´ EUC-KR ì¸ì½”ë”©
                    keyword_euckr = search_term.encode('euc-kr')
                    encoded_keyword = quote(keyword_euckr)
                    
                    # URL ì§ì ‘ êµ¬ì„±
                    search_url = f"{base_url}?searchType=keyword&keyword={encoded_keyword}&brokerCode=&writeFromDate=&writeToDate=&upjong="
                    
                    print(f"      EUC-KR ì¸ì½”ë”©: {encoded_keyword}")
                    
                    # ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ
                    response = requests.get(search_url, headers=self.headers, timeout=30)
                    response.raise_for_status()
                    response.encoding = 'euc-kr'
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´ì¼ ë•Œë§Œ HTML ë””ë²„ê¹… íŒŒì¼ ì €ì¥
                    if variation_idx == 0:
                        debug_html_path = 'downloads/debug_industry_list.html'
                        os.makedirs('downloads', exist_ok=True)
                        with open(debug_html_path, 'w', encoding='utf-8') as f:
                            f.write(soup.prettify())
                        print(f"      ğŸ” HTML ì €ì¥ë¨: {debug_html_path}")
                    
                    # í…Œì´ë¸” ì°¾ê¸°
                    table = soup.find('table', class_='type_1')
                    if not table:
                        tables = soup.find_all('table')
                        for t in tables:
                            rows = t.find_all('tr')
                            if len(rows) > 5:
                                table = t
                                break
                    
                    if not table:
                        print(f"      âš ï¸  í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        continue
                    
                    rows = table.find_all('tr')
                    
                    # í—¤ë” ë¶„ì„ (ì²« ê²€ìƒ‰ì–´ì¼ ë•Œë§Œ)
                    if variation_idx == 0:
                        header_map = {}
                        if len(rows) > 0:
                            first_row = rows[0]
                            header_cells = first_row.find_all(['td', 'th'])
                            print(f"      ğŸ“Š í—¤ë”: {len(header_cells)}ê°œ ì—´")
                            for idx, cell in enumerate(header_cells):
                                header_text = cell.get_text(strip=True)
                                header_map[header_text] = idx
                                print(f"         ì—´ {idx}: '{header_text}'")
                        
                        title_idx = header_map.get('ì œëª©', 0)
                        broker_idx = header_map.get('ì¦ê¶Œì‚¬', 1)
                        pdf_idx = header_map.get('ì²¨ë¶€', 2)
                        date_idx = header_map.get('ì‘ì„±ì¼', 3)
                    
                    # ë¦¬í¬íŠ¸ íŒŒì‹±
                    found_count = 0
                    for row in rows[1:]:  # í—¤ë” ì œì™¸
                        try:
                            cells = row.find_all('td')
                            if len(cells) < 4:
                                continue
                            
                            # ì œëª© (ë§í¬)
                            title_cell = cells[title_idx]
                            title_link = title_cell.find('a')
                            if not title_link:
                                continue
                            
                            title = title_link.get_text(strip=True)
                            if not title:
                                continue
                            
                            # ì¦ê¶Œì‚¬
                            broker = cells[broker_idx].get_text(strip=True) if len(cells) > broker_idx else "ì¦ê¶Œì‚¬"
                            
                            # PDF ì²¨ë¶€
                            pdf_cell = cells[pdf_idx]
                            pdf_link = pdf_cell.find('a')
                            if not pdf_link or not pdf_link.get('href'):
                                continue
                            
                            pdf_url = pdf_link.get('href')
                            
                            # ì ˆëŒ€ URLë¡œ ë³€í™˜
                            if not pdf_url.startswith('http'):
                                if pdf_url.startswith('//'):
                                    pdf_url = 'https:' + pdf_url
                                else:
                                    pdf_url = urljoin('https://stock.pstatic.net', pdf_url)
                            
                            # ì¤‘ë³µ ì²´í¬
                            if pdf_url in seen_urls:
                                continue
                            seen_urls.add(pdf_url)
                            
                            # ì‘ì„±ì¼
                            date = cells[date_idx].get_text(strip=True) if len(cells) > date_idx else "ë‚ ì§œë¯¸ìƒ"
                            
                            all_reports.append({
                                'title': title,
                                'broker': broker,
                                'date': date,
                                'pdf_url': pdf_url
                            })
                            
                            found_count += 1
                            
                        except Exception as row_error:
                            continue
                    
                    print(f"      âœ“ {found_count}ê°œ ë¦¬í¬íŠ¸ ë°œê²¬")
                    
                    # ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ë¥¼ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                    if len(all_reports) >= max_reports * 2:
                        print(f"      â„¹ï¸  ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ ìˆ˜ì§‘, ê²€ìƒ‰ ì¤‘ë‹¨")
                        break
                    
                    # ë‹¤ìŒ ê²€ìƒ‰ì–´ ì‚¬ì´ì— ì§§ì€ ì§€ì—°
                    time.sleep(0.5)
                    
                except Exception as search_error:
                    print(f"      âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨: {search_error}")
                    continue
            
            if not all_reports:
                print(f"\n   âš ï¸  '{company_name}'ì— ëŒ€í•œ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
            def parse_date(date_str):
                """YY.MM.DD í˜•ì‹ì„ ì •ë ¬ ê°€ëŠ¥í•œ ê°’ìœ¼ë¡œ ë³€í™˜"""
                try:
                    if not date_str or date_str == "ë‚ ì§œë¯¸ìƒ":
                        return "00.00.00"
                    return date_str
                except:
                    return "00.00.00"
            
            all_reports.sort(key=lambda x: parse_date(x['date']), reverse=True)
            
            # ìƒìœ„ Nê°œë§Œ ì„ íƒ
            selected_reports = all_reports[:max_reports]
            
            print(f"\n   âœ… ì´ {len(all_reports)}ê°œ ë¦¬í¬íŠ¸ ë°œê²¬ (ì¤‘ë³µ ì œê±° í›„)")
            print(f"   ğŸ“‹ ì„ ì •ëœ ë¦¬í¬íŠ¸ (ìµœì‹  {len(selected_reports)}ê°œ):")
            for idx, report in enumerate(selected_reports, 1):
                print(f"      [{idx}] {report['title']}")
                print(f"          ì¦ê¶Œì‚¬: {report['broker']}, ë‚ ì§œ: {report['date']}")
                print(f"          PDF: {report['pdf_url'][:80]}...")
            print()
            
            # PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_reports = []
            
            for i, report in enumerate(selected_reports, 1):
                try:
                    print(f"\n   [{i}/{len(selected_reports)}] PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {report['title'][:50]}...")
                    
                    # PDF ë‹¤ìš´ë¡œë“œ
                    pdf_response = requests.get(report['pdf_url'], headers=self.headers, timeout=60)
                    pdf_response.raise_for_status()
                    
                    # ì„ì‹œ PDF íŒŒì¼ ì €ì¥
                    pdf_filename = f"downloads/naver_industry_{i}_{int(time.time())}.pdf"
                    os.makedirs('downloads', exist_ok=True)
                    
                    with open(pdf_filename, 'wb') as f:
                        f.write(pdf_response.content)
                    
                    print(f"      ğŸ’¾ PDF ì €ì¥: {len(pdf_response.content):,} bytes")
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_content = self._extract_text_from_pdf(pdf_filename)
                    
                    if text_content:
                        extracted_reports.append({
                            'name': f"[{report['broker']}] {report['title']}",
                            'date': report['date'],
                            'content': text_content,
                            'url': report['pdf_url']
                        })
                        print(f"      âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text_content):,}ì")
                    else:
                        print(f"      âš ï¸  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    
                    # PDF íŒŒì¼ ì‚­ì œ
                    try:
                        os.remove(pdf_filename)
                    except:
                        pass
                    
                    # ì§§ì€ ì§€ì—°
                    time.sleep(1)
                    
                except Exception as download_error:
                    print(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {download_error}")
                    continue
            
            print(f"\n   âœ… ìµœì¢… {len(extracted_reports)}ê°œ ë¦¬í¬íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
            return extracted_reports
            
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

