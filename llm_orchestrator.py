#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Orchestrator
ë‹¤ì–‘í•œ LLM Providerë¥¼ í†µí•© ê´€ë¦¬í•˜ê³ , ìž‘ì—… ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ LLMì„ ìžë™ ì„ íƒ
"""

# Windows ì½˜ì†” ì¸ì½”ë”© ë¬¸ì œ í•´ê²° (ì´ëª¨ì§€ ì¶œë ¥ìš©)
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

from abc import ABC, abstractmethod
from typing import Optional, Dict
import google.generativeai as genai
import requests
import json


class LLMProvider(ABC):
    """LLM Provider ì¶”ìƒ í´ëž˜ìŠ¤"""
    
    @abstractmethod
    def generate_content(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """Provider ì´ë¦„ ë°˜í™˜"""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> dict:
        """Provider ëŠ¥ë ¥ ë°˜í™˜ (context_window, cost, speed ë“±)"""
        pass


class GeminiProvider(LLMProvider):
    """Google Gemini AI Provider"""
    
    def __init__(self, api_key: str, model_candidates: list = None):
        """
        Gemini Provider ì´ˆê¸°í™”
        
        Args:
            api_key: Gemini API í‚¤
            model_candidates: ì‹œë„í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        """
        genai.configure(api_key=api_key)
        self.model = self._initialize_model(model_candidates or ['gemini-2.5-pro-preview-03-25', 'gemini-pro'])
    
    def _initialize_model(self, candidates):
        """ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ ìžë™ ì„ íƒ"""
        print("ðŸ” Gemini API ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸ ì¤‘...")
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_models = []
            for model in genai.list_models():
                if 'generateContent' in model.supported_generation_methods:
                    available_models.append(model.name)
                    print(f"   - {model.name}")
            
            if available_models:
                selected_model = available_models[0]
                model_obj = genai.GenerativeModel(selected_model)
                print(f"âœ… Gemini ëª¨ë¸ ì„¤ì • ì™„ë£Œ: {selected_model}")
                return model_obj
        except Exception as e:
            print(f"âš ï¸  ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            print("   ê¸°ë³¸ ëª¨ë¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
        
        # ê¸°ë³¸ ëª¨ë¸ë“¤ ì‹œë„
        for model_name in candidates:
            try:
                test_model = genai.GenerativeModel(model_name)
                # ì‹¤ì œë¡œ ìž‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
                test_response = test_model.generate_content("test")
                print(f"âœ… Gemini ëª¨ë¸ ì„¤ì • ì™„ë£Œ: {model_name}")
                return test_model
            except Exception as model_error:
                print(f"   âŒ {model_name} ì‹¤íŒ¨: {str(model_error)[:100]}")
                continue
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ gemini-proë¡œ í´ë°±
        print("âš ï¸  ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨. gemini-proë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return genai.GenerativeModel('gemini-pro')
    
    def generate_content(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        response = self.model.generate_content(prompt)
        return response.text
    
    def get_name(self) -> str:
        """Provider ì´ë¦„ ë°˜í™˜"""
        return "gemini"
    
    def get_capabilities(self) -> dict:
        """Provider ëŠ¥ë ¥ ë°˜í™˜"""
        return {
            'context_window': 1_000_000,
            'supports_long_context': True,
            'supports_korean': True,
            'cost': 'medium',
            'speed': 'fast'
        }


class MidmProvider(LLMProvider):
    """Friendli AI (Midm) Provider"""
    
    def __init__(self, api_token: str, base_url: str, endpoint_id: str):
        """
        Midm Provider ì´ˆê¸°í™”
        
        Args:
            api_token: Friendli API Token
            base_url: Friendli API Base URL (https://api.friendli.ai/dedicated/v1)
            endpoint_id: ë°°í¬ëœ ì—”ë“œí¬ì¸íŠ¸ ID
        
        Reference:
            https://friendli.ai/docs/guides/dedicated_endpoints/quickstart
        """
        self.api_token = api_token
        self.base_url = base_url.rstrip('/')
        self.endpoint_id = endpoint_id
        self.api_url = f"{self.base_url}/chat/completions"
        
        print(f"ðŸ”§ Midm Provider ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   Endpoint ID: {endpoint_id}")
        print(f"   API URL: {self.api_url}")
    
    def generate_content(self, prompt: str, **kwargs) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ìƒì„±í•  í”„ë¡¬í”„íŠ¸
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (max_tokens, temperature, top_p ë“±)
        
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        try:
            # ê°€ì´ë“œì— ë‚˜ì˜¨ í˜•ì‹: Authorization Bearer í† í°
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_token}"
            }
            
            # ê°€ì´ë“œì— ë‚˜ì˜¨ íŽ˜ì´ë¡œë“œ í˜•ì‹ ê·¸ëŒ€ë¡œ
            payload = {
                "model": self.endpoint_id,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": kwargs.get('max_tokens', 512),
                "temperature": kwargs.get('temperature', 0.7),
                "top_p": kwargs.get('top_p', 0.9),
            }
            
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code != 200:
                error_msg = f"HTTP {response.status_code}"
                try:
                    error_detail = response.json()
                    error_msg += f": {error_detail}"
                except:
                    error_msg += f": {response.text[:200]}"
                print(f"   âŒ {error_msg}")
            
            response.raise_for_status()
            
            result = response.json()
            
            # ê°€ì´ë“œì˜ ì‘ë‹µ í˜•ì‹: choices[0].message.content
            if 'choices' in result and len(result['choices']) > 0:
                message = result['choices'][0].get('message', {})
                content = message.get('content', '')
                if content:
                    return content.strip()
            
            raise ValueError(f"ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: {result}")
                
        except requests.exceptions.RequestException as e:
            error_msg = f"Midm API ìš”ì²­ ì˜¤ë¥˜: {e}"
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.json()
                    error_msg += f"\n   ìƒì„¸: {error_detail}"
                except:
                    error_msg += f"\n   ì‘ë‹µ: {e.response.text[:300]}"
            print(f"âŒ {error_msg}")
            raise
        except Exception as e:
            print(f"âŒ Midm API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            raise
    
    def get_name(self) -> str:
        """Provider ì´ë¦„ ë°˜í™˜"""
        return "midm"
    
    def get_capabilities(self) -> dict:
        """Provider ëŠ¥ë ¥ ë°˜í™˜"""
        return {
            'context_window': 4096,  # Friendli AI ê¸°ë³¸ê°’
            'supports_long_context': False,
            'supports_korean': True,
            'cost': 'low',
            'speed': 'very_fast'  # Dedicated endpointë¼ ë¹ ë¦„
        }


class LLMOrchestrator:
    """LLM Provider ê´€ë¦¬ ë° ìžë™ ì„ íƒ"""
    
    def __init__(self):
        """Orchestrator ì´ˆê¸°í™”"""
        self.providers: Dict[str, LLMProvider] = {}
        self.default_provider: Optional[str] = None
        self.task_routing: Dict[str, str] = {}
    
    def register_provider(self, provider: LLMProvider, is_default: bool = False):
        """
        Provider ë“±ë¡
        
        Args:
            provider: LLMProvider ì¸ìŠ¤í„´ìŠ¤
            is_default: ê¸°ë³¸ Providerë¡œ ì„¤ì •í• ì§€ ì—¬ë¶€
        """
        name = provider.get_name()
        self.providers[name] = provider
        print(f"âœ… LLM Provider ë“±ë¡: {name}")
        
        if is_default or self.default_provider is None:
            self.default_provider = name
            print(f"   ðŸ“Œ ê¸°ë³¸ Provider ì„¤ì •: {name}")
    
    def set_task_routing(self, task_type: str, provider_name: str):
        """
        ìž‘ì—… ìœ í˜•ë³„ Provider ë¼ìš°íŒ… ì„¤ì •
        
        Args:
            task_type: ìž‘ì—… ìœ í˜• (ì˜ˆ: 'query_analysis', 'long_context_analysis')
            provider_name: ì‚¬ìš©í•  Provider ì´ë¦„
        """
        if provider_name not in self.providers:
            raise ValueError(f"Provider '{provider_name}' ë¯¸ë“±ë¡")
        self.task_routing[task_type] = provider_name
        print(f"   ðŸ”€ ë¼ìš°íŒ… ì„¤ì •: {task_type} â†’ {provider_name}")
    
    def select_provider(self, task_type: Optional[str] = None) -> LLMProvider:
        """
        ìž‘ì—… ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ Provider ì„ íƒ
        
        Args:
            task_type: ìž‘ì—… ìœ í˜• (ì„ íƒì‚¬í•­)
            
        Returns:
            ì„ íƒëœ LLMProvider
        """
        # 1. ìž‘ì—… ìœ í˜•ë³„ ë¼ìš°íŒ…ì´ ì„¤ì •ë˜ì–´ ìžˆìœ¼ë©´ í•´ë‹¹ Provider ì‚¬ìš©
        if task_type and task_type in self.task_routing:
            provider_name = self.task_routing[task_type]
            print(f"   ðŸ”€ ë¼ìš°íŒ… ì ìš©: {task_type} â†’ {provider_name}")
            return self.providers[provider_name]
        
        # 2. ìž‘ì—… ìœ í˜•ë³„ ìžë™ ì„ íƒ ë¡œì§ (í–¥í›„ í™•ìž¥)
        if task_type == 'long_context_analysis':
            # ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” Gemini ìš°ì„ 
            for name, provider in self.providers.items():
                if provider.get_capabilities().get('supports_long_context'):
                    return provider
        
        elif task_type == 'quick_analysis':
            # ë¹ ë¥¸ ë¶„ì„ì€ ì†ë„ ìš°ì„  (í–¥í›„: Claude-Haiku, GPT-3.5 ë“±)
            pass
        
        # 3. ê¸°ë³¸ Provider ì‚¬ìš©
        if self.default_provider and self.default_provider in self.providers:
            print(f"   ðŸ”€ ê¸°ë³¸ Provider ì‚¬ìš©: {self.default_provider}")
            return self.providers[self.default_provider]
        
        # 4. ì•„ë¬´ Providerë¼ë„ ë°˜í™˜ (í´ë°±)
        if self.providers:
            fallback_name = list(self.providers.keys())[0]
            print(f"   ðŸ”€ í´ë°± Provider ì‚¬ìš©: {fallback_name}")
            return list(self.providers.values())[0]
        
        raise RuntimeError("ë“±ë¡ëœ LLM Providerê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def generate(self, prompt: str, task_type: Optional[str] = None, **kwargs) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„± (ìžë™ Provider ì„ íƒ)
        
        Args:
            prompt: ìƒì„±í•  í”„ë¡¬í”„íŠ¸
            task_type: ìž‘ì—… ìœ í˜• (ì„ íƒì‚¬í•­)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        provider = self.select_provider(task_type)
        print(f"ðŸ¤– ì‚¬ìš© LLM: {provider.get_name()}")
        return provider.generate_content(prompt, **kwargs)
    
    def list_providers(self) -> list:
        """
        ë“±ë¡ëœ Provider ëª©ë¡ ë°˜í™˜
        
        Returns:
            Provider ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        return [
            {
                'name': name,
                'capabilities': provider.get_capabilities(),
                'is_default': name == self.default_provider
            }
            for name, provider in self.providers.items()
        ]


# í–¥í›„ ì¶”ê°€ ê°€ëŠ¥í•œ Provider ì˜ˆì‹œ:
"""
class OpenAIProvider(LLMProvider):
    '''OpenAI GPT Provider'''
    
    def __init__(self, api_key: str, model: str = "gpt-4"):
        import openai
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
    
    def generate_content(self, prompt: str, **kwargs) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    
    def get_name(self) -> str:
        return "openai"
    
    def get_capabilities(self) -> dict:
        return {
            'context_window': 128_000,
            'supports_long_context': True,
            'supports_korean': True,
            'cost': 'high',
            'speed': 'medium'
        }


class ClaudeProvider(LLMProvider):
    '''Anthropic Claude Provider'''
    
    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229"):
        import anthropic
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = model
    
    def generate_content(self, prompt: str, **kwargs) -> str:
        response = self.client.messages.create(
            model=self.model,
            max_tokens=4096,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
    
    def get_name(self) -> str:
        return "claude"
    
    def get_capabilities(self) -> dict:
        return {
            'context_window': 200_000,
            'supports_long_context': True,
            'supports_korean': True,
            'cost': 'high',
            'speed': 'medium'
        }
"""

