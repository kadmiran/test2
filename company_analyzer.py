#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íšŒì‚¬ ë³´ê³ ì„œ ë¶„ì„ê¸°
ì˜¤í”ˆë‹¤íŠ¸ APIë¡œ ë³´ê³ ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  Gemini APIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import requests
import zipfile
import xml.etree.ElementTree as ET
import os
import re
from datetime import datetime, timedelta
import fitz  # PyMuPDF for PDF processing
from vector_store import VectorStore
from naver_finance import NaverFinanceCrawler
from config import config
import shutil
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import time
import logging
from prompt_manager import get_prompt_manager

# Logger ì„¤ì •
logger = logging.getLogger(__name__)

class CompanyAnalyzer:
    """íšŒì‚¬ ë³´ê³ ì„œ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, dart_api_key, llm_orchestrator):
        """
        ì´ˆê¸°í™”
        
        Args:
            dart_api_key: ì˜¤í”ˆë‹¤íŠ¸ API í‚¤
            llm_orchestrator: LLMOrchestrator ì¸ìŠ¤í„´ìŠ¤ (Dependency Injection)
        """
        self.dart_api_key = dart_api_key
        self.llm_orchestrator = llm_orchestrator
        self.base_url = config.DART_BASE_URL
        
        # Prompt Manager
        self.prompt_manager = get_prompt_manager()
        
        # VectorStoreëŠ” lazy initialization (í•„ìš”í•  ë•Œ ìƒì„±)
        self._vector_store = None
        
        # ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬ (lazy initialization)
        self._naver_crawler = None
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°± (ë‹¤ì´ì–´ê·¸ë¨ ì—…ë°ì´íŠ¸ìš©)
        self.status_callback = None
    
    @property
    def vector_store(self):
        """VectorStore lazy initialization - ì²˜ìŒ ì ‘ê·¼í•  ë•Œë§Œ ìƒì„±"""
        if self._vector_store is None:
            self._vector_store = VectorStore()
        return self._vector_store
    
    @property
    def naver_crawler(self):
        """NaverFinanceCrawler lazy initialization - ì²˜ìŒ ì ‘ê·¼í•  ë•Œë§Œ ìƒì„±"""
        if self._naver_crawler is None:
            self._naver_crawler = NaverFinanceCrawler(llm_orchestrator=self.llm_orchestrator)
        return self._naver_crawler
        
    def _get_company_name_variations(self, company_name):
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì‚¬ëª…ì˜ ë‹¤ì–‘í•œ í‘œê¸° ê°€ì ¸ì˜¤ê¸°
        
        Args:
            company_name: ì…ë ¥ëœ íšŒì‚¬ëª…
            
        Returns:
            list: ê°€ëŠ¥í•œ íšŒì‚¬ëª… í‘œê¸° ë¦¬ìŠ¤íŠ¸
        """
        try:
            prompt = self.prompt_manager.get_prompt('name_variation', company_name=company_name)
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 60)
            logger.info("ğŸ“ FULL PROMPT (Name Variation)")
            logger.info("=" * 60)
            logger.info(prompt)
            logger.info("=" * 60)
            
            variations_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='name_variation'
            ).strip()
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            variations = [v.strip() for v in variations_text.split(',')]
            
            # ì›ë˜ ì…ë ¥ê°’ë„ í¬í•¨
            if company_name not in variations:
                variations.insert(0, company_name)
            
            # ì‚¬ìš©ëœ LLM í‘œì‹œ
            used_llm = self.llm_orchestrator.select_provider('name_variation').get_name().upper()
            logger.info(f"   âœ… {used_llm} ì¶”ì²œ ê²€ìƒ‰ì–´: {variations}")
            return variations
            
        except Exception as e:
            logger.warning(f"   âš ï¸  LLM ê²€ìƒ‰ì–´ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë³€í˜•ë§Œ ë°˜í™˜
            return [company_name]
    
    def get_corp_code(self, company_name):
        """
        íšŒì‚¬ëª…ìœ¼ë¡œ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ
        
        Args:
            company_name: íšŒì‚¬ëª…
            
        Returns:
            tuple: (corp_code, corp_name, stock_code)
        """
        logger.info(f"ğŸ” '{company_name}' íšŒì‚¬ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ ì¤‘...")
        
        # 1. LLMìœ¼ë¡œ íšŒì‚¬ëª… ë³€í˜• ê°€ì ¸ì˜¤ê¸° (í•­ìƒ ì‹¤í–‰)
        logger.info(f"ğŸ¤– LLMì—ê²Œ '{company_name}'ì˜ ë‹¤ì–‘í•œ í‘œê¸° í™•ì¸ ìš”ì²­ ì¤‘...")
        search_variations = self._get_company_name_variations(company_name)
        
        try:
            # ê¸°ì—… ê³ ìœ ë²ˆí˜¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            corp_code_url = f'{self.base_url}/corpCode.xml?crtfc_key={self.dart_api_key}'
            response = requests.get(corp_code_url, timeout=30)
            response.raise_for_status()
            
            # ZIP íŒŒì¼ ì €ì¥ ë° ì••ì¶• í•´ì œ
            with open('corp_code.zip', 'wb') as f:
                f.write(response.content)
            
            with zipfile.ZipFile('corp_code.zip', 'r') as zip_ref:
                zip_ref.extractall()
            
            # XML íŒŒì¼ íŒŒì‹±
            tree = ET.parse('CORPCODE.xml')
            root = tree.getroot()
            
            # íšŒì‚¬ ê²€ìƒ‰ - LLM ì¶”ì²œ ê²€ìƒ‰ì–´ë¡œ ë‹¤ë‹¨ê³„ ê²€ìƒ‰
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: ì´ {len(search_variations)}ê°œ í‘œê¸°ë¡œ ê²€ìƒ‰")
            exact_matches = []
            
            # ëª¨ë“  ë³€í˜•ì— ëŒ€í•´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” íšŒì‚¬ ê²€ìƒ‰ (ëª¨ë“  ë™ëª… íšŒì‚¬ ìˆ˜ì§‘)
            for variation in search_variations:
                logger.info(f"   ğŸ“Œ ê²€ìƒ‰ì–´: '{variation}'")
                
                found_count = 0
                for child in root:
                    corp_name_elem = child.find('corp_name')
                    corp_code_elem = child.find('corp_code')
                    stock_code_elem = child.find('stock_code')
                    
                    if corp_name_elem is not None and corp_code_elem is not None:
                        name = corp_name_elem.text
                        code = corp_code_elem.text
                        stock = stock_code_elem.text if stock_code_elem is not None and stock_code_elem.text else ''
                        
                        # ì •í™•íˆ ì¼ì¹˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                        if name.upper() == variation.upper():
                            # ì¤‘ë³µ ë°©ì§€
                            if not any(c[0] == code for c in exact_matches):
                                exact_matches.append((code, name, stock))
                                stock_display = stock if stock else 'ì—†ìŒ'
                                logger.info(f"      âœ… ë°œê²¬: {name} (ê³ ìœ ë²ˆí˜¸: {code}, ì¢…ëª©: {stock_display})")
                                found_count += 1
                
                # ì´ ê²€ìƒ‰ì–´ë¡œ ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ ê²€ìƒ‰ì–´ëŠ” ì‹œë„ ì•ˆ í•¨
                if found_count > 0:
                    break
            
            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” íšŒì‚¬ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
            if exact_matches:
                logger.info(f"\n   âœ… ì´ {len(exact_matches)}ê°œì˜ ë™ëª… íšŒì‚¬ ë°œê²¬")
                
                # ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ê²ƒë§Œ í•„í„°ë§
                with_stock = [c for c in exact_matches if c[2] and c[2].strip()]
                
                if with_stock:
                    # ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ê²ƒ ì¤‘ ì²« ë²ˆì§¸
                    result = with_stock[0]
                    logger.info(f"ìµœì¢… ì„ íƒ (ì¢…ëª©ì½”ë“œ ìˆìŒ): {result[1]} (ê³ ìœ ë²ˆí˜¸: {result[0]}, ì¢…ëª©: {result[2]})")
                else:
                    # ì¢…ëª©ì½”ë“œê°€ ì—†ì–´ë„ ì²« ë²ˆì§¸ ì„ íƒ
                    result = exact_matches[0]
                    logger.info(f"ìµœì¢… ì„ íƒ (ì¢…ëª©ì½”ë“œ ì—†ìŒ): {result[1]} (ê³ ìœ ë²ˆí˜¸: {result[0]})")
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for file in ['corp_code.zip', 'CORPCODE.xml']:
                    if os.path.exists(file):
                        os.remove(file)
                
                return result
            
            # 2ë‹¨ê³„: ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ í¬í•¨ë˜ëŠ” ê²½ìš° ê²€ìƒ‰
            logger.info("2ë‹¨ê³„: ë³€í˜• ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•˜ëŠ” íšŒì‚¬ ê²€ìƒ‰ ì¤‘...")
            contains_matches = []
            
            # ëª¨ë“  ë³€í˜•ì— ëŒ€í•´ í¬í•¨ ê²€ìƒ‰
            for variation in search_variations:
                for child in root:
                    corp_name_elem = child.find('corp_name')
                    corp_code_elem = child.find('corp_code')
                    stock_code_elem = child.find('stock_code')
                    
                    if corp_name_elem is not None and corp_code_elem is not None:
                        name = corp_name_elem.text
                        code = corp_code_elem.text
                        stock = stock_code_elem.text if stock_code_elem is not None else ''
                        
                        # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ê²½ìš°
                        if variation.upper() in name.upper():
                            # ì¤‘ë³µ ë°©ì§€
                            if not any(c[0] == code for c in contains_matches):
                                contains_matches.append((code, name, stock))
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for file in ['corp_code.zip', 'CORPCODE.xml']:
                if os.path.exists(file):
                    os.remove(file)
            
            if contains_matches:
                logger.warning(f"ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” '{company_name}'ëŠ” ì—†ìŠµë‹ˆë‹¤.")
                logger.info(f"ìœ ì‚¬í•œ íšŒì‚¬ {len(contains_matches)}ê°œ ë°œê²¬")
                
                # ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ê²ƒ ìš°ì„ 
                with_stock = [c for c in contains_matches if c[2] and c[2].strip()]
                if with_stock:
                    result = with_stock[0]
                else:
                    result = contains_matches[0]
                
                logger.info(f"ì²« ë²ˆì§¸ í›„ë³´ ì„ íƒ: {result[1]}")
                return result
            else:
                logger.error(f"'{company_name}' íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            logger.error(f"íšŒì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_time_range(self, user_query):
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            int: ê²€ìƒ‰í•  ì—°ìˆ˜ (ê¸°ë³¸ê°’: 3ë…„)
        """
        try:
            logger.info("Geminiì—ê²Œ ì‹œê°„ ë²”ìœ„ ë¶„ì„ ìš”ì²­ ì¤‘...")
            
            prompt = self.prompt_manager.get_prompt('time_range_extraction', user_query=user_query)
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 60)
            logger.info("ğŸ“ FULL PROMPT (Time Range Extraction)")
            logger.info("=" * 60)
            logger.info(prompt)
            logger.info("=" * 60)
            
            response_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            
            # JSON íŒŒì‹±
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_text = json_match.group(0)
                result = json.loads(json_text)
                
                years = result.get('years', 3)
                reason = result.get('reason', '')
                
                # ìµœëŒ€ 10ë…„ìœ¼ë¡œ ì œí•œ
                years = min(years, 10)
                years = max(years, 1)  # ìµœì†Œ 1ë…„
                
                logger.info(f"ì¶”ì¶œëœ ê¸°ê°„: {years}ë…„ - {reason}")
                
                return years
            else:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ 3ë…„ ì‚¬ìš© - ì‘ë‹µ: {response_text[:200]}")
                return 3
                
        except Exception as e:
            logger.warning(f"ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 3  # ê¸°ë³¸ê°’
    
    def _recommend_report_types(self, user_query, years=3):
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ ë³´ê³ ì„œ íƒ€ì… ì¶”ì²œ
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            years: ê²€ìƒ‰ ì—°ìˆ˜ (ì¥ê¸° ì¶”ì„¸ ë¶„ì„ ì‹œ ì°¸ê³ )
            
        Returns:
            list: ì¶”ì²œëœ ë³´ê³ ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info("Geminiì—ê²Œ ì ì ˆí•œ ë³´ê³ ì„œ íƒ€ì… ì¶”ì²œ ìš”ì²­ ì¤‘...")
            
            prompt = self.prompt_manager.get_prompt('report_type_recommendation', user_query=user_query)
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 60)
            logger.info("ğŸ“ FULL PROMPT (Report Type Recommendation)")
            logger.info("=" * 60)
            logger.info(prompt)
            logger.info("=" * 60)
            
            response_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            
            # JSON íŒŒì‹±
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` í˜•ì‹ ì²˜ë¦¬)
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_text = json_match.group(0)
                recommendation = json.loads(json_text)
                
                recommended_types = recommendation.get('recommended_types', [])
                reason = recommendation.get('reason', '')
                need_historical = recommendation.get('need_historical_reports', False)
                
                # ì¶”ì²œ íƒ€ì…ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                if not recommended_types:
                    logger.warning("ì¶”ì²œëœ íƒ€ì…ì´ ì—†ìŒ, ê¸°ë³¸ ë³´ê³ ì„œ íƒ€ì… ì‚¬ìš©")
                    recommended_types = ['ì‚¬ì—…ë³´ê³ ì„œ', 'ë°˜ê¸°ë³´ê³ ì„œ']
                
                logger.info(f"ì¶”ì²œëœ ë³´ê³ ì„œ íƒ€ì…: {recommended_types} - {reason}")
                if need_historical:
                    logger.info("ì—¬ëŸ¬ í•´ì˜ ë³´ê³ ì„œ í•„ìš”")
                
                # íŠœí”Œë¡œ ë°˜í™˜ (íƒ€ì… ë¦¬ìŠ¤íŠ¸, ì—°ë„ë³„ ë³´ê³ ì„œ í•„ìš” ì—¬ë¶€)
                return recommended_types, need_historical
            else:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ë³´ê³ ì„œ íƒ€ì… ì‚¬ìš© - ì‘ë‹µ: {response_text[:200]}")
                return ['ì‚¬ì—…ë³´ê³ ì„œ', 'ë°˜ê¸°ë³´ê³ ì„œ'], False
                
        except Exception as e:
            logger.warning(f"ë³´ê³ ì„œ íƒ€ì… ì¶”ì²œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ['ì‚¬ì—…ë³´ê³ ì„œ', 'ë°˜ê¸°ë³´ê³ ì„œ'], False
    
    def get_reports(self, corp_code, report_types=None, user_query=None, years=None):
        """
        íŠ¹ì • íšŒì‚¬ì˜ ë³´ê³ ì„œ ëª©ë¡ ì¡°íšŒ
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            report_types: ì¡°íšŒí•  ë³´ê³ ì„œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ user_query ê¸°ë°˜ ìë™ ì¶”ì²œ)
            user_query: ì‚¬ìš©ì ì§ˆë¬¸ (report_typesê°€ Noneì¼ ë•Œ ì‚¬ìš©)
            years: ê²€ìƒ‰í•  ì—°ìˆ˜ (Noneì´ë©´ user_queryì—ì„œ ìë™ ì¶”ì¶œ)
            
        Returns:
            list: ë³´ê³ ì„œ ëª©ë¡
        """
        # ë³´ê³ ì„œ íƒ€ì…ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ Geminiì—ê²Œ ì¶”ì²œë°›ê¸°
        if report_types is None and user_query:
            logger.info("ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ë³´ê³ ì„œ íƒ€ì… ìë™ ì„ íƒ ì¤‘...")
            report_types, _ = self._recommend_report_types(user_query, years if years else 3)
        
        # report_typesê°€ ì—¬ì „íˆ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not report_types:
            logger.info("ê¸°ë³¸ ë³´ê³ ì„œ íƒ€ì… ì‚¬ìš©: ì‚¬ì—…ë³´ê³ ì„œ, ë°˜ê¸°ë³´ê³ ì„œ")
            report_types = ['ì‚¬ì—…ë³´ê³ ì„œ', 'ë°˜ê¸°ë³´ê³ ì„œ']
        
        # ì‹œê°„ ë²”ìœ„ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ user_queryì—ì„œ ì¶”ì¶œ
        if years is None and user_query:
            years = self._extract_time_range(user_query)
        elif years is None:
            years = 3  # ê¸°ë³¸ê°’
        
        logger.info(f"ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘... (íƒ€ì…: {', '.join(report_types)}, ê¸°ê°„: ìµœê·¼ {years}ë…„)")
        
        try:
            # ì§€ì •ëœ ê¸°ê°„ ê²€ìƒ‰
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')
            
            logger.info(f"ê²€ìƒ‰ ê¸°ê°„: {start_date} ~ {end_date}, íšŒì‚¬ ê³ ìœ ë²ˆí˜¸: {corp_code}")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ëª¨ë“  ë³´ê³ ì„œ ê²€ìƒ‰
            all_reports = []
            page_no = 1
            max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ê²€ìƒ‰ (1000ê°œ ë³´ê³ ì„œ)
            
            while page_no <= max_pages:
                logger.info(f"í˜ì´ì§€ {page_no} ê²€ìƒ‰ ì¤‘...")
                
                params = {
                    'crtfc_key': self.dart_api_key,
                    'corp_code': corp_code,
                    'bgn_de': start_date,
                    'end_de': end_date,
                    'page_no': page_no,
                    'page_count': 100
                }
                
                if page_no == 1:
                    logger.info(f"ğŸ“¤ DART API ìš”ì²­ ì •ë³´:")
                    logger.info(f"   - URL: {self.base_url}/list.json")
                    logger.info(f"   - ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
                    logger.info(f"   - íšŒì‚¬ ê³ ìœ ë²ˆí˜¸: {corp_code}")
                    logger.info(f"   - ê²€ìƒ‰ ê¸°ê°„: {start_date} ~ {end_date}")
                    logger.info(f"   - ë³´ê³ ì„œ íƒ€ì…: {report_types}")
                
                response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
                logger.info(f"ğŸ“¡ DART API ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                
                response.raise_for_status()
                
                data = response.json()
                logger.info(f"ğŸ“‹ DART API ì‘ë‹µ ìƒì„¸:")
                logger.info(f"   - ìƒíƒœ: {data.get('status')}")
                logger.info(f"   - ë©”ì‹œì§€: {data.get('message', 'N/A')}")
                logger.info(f"   - ì „ì²´ ì‘ë‹µ í¬ê¸°: {len(str(data))}ì")
                
                # ì‘ë‹µ ë°ì´í„°ì˜ ì£¼ìš” í•„ë“œë“¤ ë¡œê·¸ ì¶œë ¥
                if 'list' in data:
                    logger.info(f"   - ë³´ê³ ì„œ ëª©ë¡ ê°œìˆ˜: {len(data.get('list', []))}ê°œ")
                    if data.get('list'):
                        sample_report = data['list'][0]
                        logger.info(f"   - ë³´ê³ ì„œ ìƒ˜í”Œ í•„ë“œ: {list(sample_report.keys())}")
                        logger.info(f"   - ì²« ë²ˆì§¸ ë³´ê³ ì„œ: {sample_report.get('report_nm', 'N/A')} ({sample_report.get('rcept_dt', 'N/A')})")
                
                # ì „ì²´ ì‘ë‹µ ë°ì´í„° ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                logger.info(f"ğŸ“„ DART API ì „ì²´ ì‘ë‹µ ë°ì´í„°:")
                logger.info(f"{data}")
                
                if data.get('status') == '000':
                    page_reports = data.get('list', [])
                    logger.debug(f"í˜ì´ì§€ {page_no} ë³´ê³ ì„œ ìˆ˜: {len(page_reports)}ê°œ")
                    
                    if not page_reports:
                        logger.info(f"í˜ì´ì§€ {page_no}ì— ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì¢…ë£Œ.")
                        break
                    
                    all_reports.extend(page_reports)
                    
                    # ì›í•˜ëŠ” ë³´ê³ ì„œ ìœ í˜•ì´ ìˆëŠ”ì§€ í™•ì¸
                    found_target = False
                    for report in page_reports:
                        report_name = report.get('report_nm', '')
                        for report_type in report_types:
                            if report_type in report_name:
                                found_target = True
                                break
                        if found_target:
                            break
                    
                    if found_target:
                        logger.info(f"í˜ì´ì§€ {page_no}ì—ì„œ ì›í•˜ëŠ” ë³´ê³ ì„œ íƒ€ì… ë°œê²¬! ê²€ìƒ‰ ì¢…ë£Œ.")
                        break
                    
                    page_no += 1
                else:
                    logger.error(f"âŒ í˜ì´ì§€ {page_no} ì¡°íšŒ ì‹¤íŒ¨:")
                    logger.error(f"   - ìƒíƒœ ì½”ë“œ: {data.get('status')}")
                    logger.error(f"   - ì˜¤ë¥˜ ë©”ì‹œì§€: {data.get('message', 'N/A')}")
                    logger.error(f"   - ì „ì²´ ì‘ë‹µ: {data}")
                    break
            
            logger.info(f"ì „ì²´ ìˆ˜ì§‘ëœ ë³´ê³ ì„œ ìˆ˜: {len(all_reports)}ê°œ")
            
            if not all_reports:
                logger.warning("ì „ì²´ ë³´ê³ ì„œê°€ 0ê°œì…ë‹ˆë‹¤!")
                return []
            
            # ì›í•˜ëŠ” ë³´ê³ ì„œ ìœ í˜•ë§Œ í•„í„°ë§
            filtered_reports = []
            logger.info(f"ğŸ” ë³´ê³ ì„œ í•„í„°ë§ ì‹œì‘:")
            logger.info(f"   - ì „ì²´ ë³´ê³ ì„œ ìˆ˜: {len(all_reports)}ê°œ")
            logger.info(f"   - ì°¾ëŠ” íƒ€ì…: {report_types}")
            
            match_count = 0
            for i, report in enumerate(all_reports):
                report_name = report.get('report_nm', '')
                report_date = report.get('rcept_dt', '')
                logger.info(f"   [{i+1}/{len(all_reports)}] ê²€ì‚¬ ì¤‘: '{report_name}' ({report_date})")
                
                matched = False
                for report_type in report_types:
                    if report_type in report_name:
                        filtered_reports.append(report)
                        match_count += 1
                        logger.info(f"      âœ… ë§¤ì¹˜! '{report_type}' í¬í•¨ë¨ â†’ í•„í„°ë§ë¨")
                        matched = True
                        break
                
                if not matched:
                    logger.info(f"      âŒ ë§¤ì¹˜ ì•ˆë¨")
            
            logger.info(f"ğŸ¯ í•„í„°ë§ ê²°ê³¼: {match_count}ê°œ ë§¤ì¹˜ë¨")
            
            logger.info(f"ì´ {len(filtered_reports)}ê°œì˜ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            if not filtered_reports:
                logger.warning("í•„í„°ë§ ê²°ê³¼ 0ê°œ! ì›í•˜ëŠ” ë³´ê³ ì„œ íƒ€ì…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return filtered_reports
                
        except Exception as e:
            logger.error(f"âŒ DART ë³´ê³ ì„œ ê²€ìƒ‰ ì˜¤ë¥˜:")
            logger.error(f"   - ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"   - ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            logger.error(f"   - ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: corp_code={corp_code}, years={years}")
            logger.error(f"   - ë³´ê³ ì„œ íƒ€ì…: {report_types}")
            import traceback
            logger.error(f"   - ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return []
    
    def download_report(self, rcept_no, save_path=None, company_name=None, report_name=None, report_date=None):
        """
        ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ë° ë‚´ìš© ì¶”ì¶œ (ë²¡í„°DB ìºì‹œ ìš°ì„ )
        
        Args:
            rcept_no: ì ‘ìˆ˜ë²ˆí˜¸
            save_path: ì €ì¥ ê²½ë¡œ (ì§€ì •í•˜ë©´ íŒŒì¼ë¡œ ì €ì¥)
            company_name: íšŒì‚¬ëª… (ë²¡í„°DB ì €ì¥ìš©)
            report_name: ë³´ê³ ì„œëª… (ë²¡í„°DB ì €ì¥ìš©)
            report_date: ë³´ê³ ì„œ ë‚ ì§œ (ë²¡í„°DB ì €ì¥ìš©)
            
        Returns:
            tuple: (text_content, saved_file_path, extracted_path)
        """
        logger.info(f"ë³´ê³ ì„œ ì¡°íšŒ ì¤‘... (ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no})")
        
        # 1. ë²¡í„°DBì—ì„œ ë¨¼ì € í™•ì¸
        logger.info("VectorDB ìºì‹œ í™•ì¸ ì¤‘...")
        if self.vector_store.check_report_exists(rcept_no):
            cached_content = self.vector_store.get_report_from_cache(rcept_no)
            if cached_content:
                logger.info(f"VectorDB ìºì‹œ ì‚¬ìš© (API í˜¸ì¶œ ìƒëµ) - ìºì‹œëœ ë‚´ìš©: {len(cached_content):,}ì")
                # ìºì‹œëœ ë‚´ìš© ë°˜í™˜ (íŒŒì¼ ê²½ë¡œëŠ” None)
                return cached_content, None, None
        
        # 2. ë²¡í„°DBì— ì—†ìœ¼ë©´ APIë¡œ ë‹¤ìš´ë¡œë“œ
        logger.info("VectorDBì— ì—†ìŒ â†’ DART APIì—ì„œ ë‹¤ìš´ë¡œë“œ")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°±ì´ ìˆìœ¼ë©´ DART ì‹œì‘ ë¡œê·¸ ì „ì†¡
        if hasattr(self, 'status_callback') and self.status_callback:
            self.status_callback(f"ğŸ”µ DART ì‹œì‘: ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ")
        
        try:
            # ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ
            params = {
                'crtfc_key': self.dart_api_key,
                'rcept_no': rcept_no
            }
            
            response = requests.get(f'{self.base_url}/document.xml', params=params, timeout=60)
            response.raise_for_status()
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            if not os.path.exists('downloads'):
                os.makedirs('downloads')
            
            # íŒŒì¼ ì €ì¥
            if not save_path:
                save_path = f'downloads/{rcept_no}.zip'
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            logger.info(f"ë³´ê³ ì„œ ì €ì¥: {save_path} ({len(response.content):,} bytes)")
            
            # ZIP íŒŒì¼ì¸ì§€ í™•ì¸í•˜ê³  ì••ì¶• í•´ì œ
            content = ""
            extracted_path = None
            
            if zipfile.is_zipfile(save_path):
                # ZIP ì••ì¶• í•´ì œ
                extract_dir = save_path.replace('.zip', '_extracted')
                os.makedirs(extract_dir, exist_ok=True)
                
                with zipfile.ZipFile(save_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                    file_list = zip_ref.namelist()
                    
                    if file_list:
                        extracted_path = os.path.join(extract_dir, file_list[0])
                        # ì²« ë²ˆì§¸ XML íŒŒì¼ ì½ê¸°
                        with zip_ref.open(file_list[0]) as xml_file:
                            content = xml_file.read().decode('utf-8', errors='ignore')
                        
                        logger.info(f"ì••ì¶• í•´ì œ: {extract_dir}")
            else:
                # ì§ì ‘ XML ì½ê¸°
                with open(save_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                extracted_path = save_path
            
            # XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° Markdown ë³€í™˜
            logger.info(f"XML â†’ Markdown ë³€í™˜ ì‹œì‘ (ì›ë³¸: {len(content):,}ì)")
            text_content = self._extract_text_from_xml(content)
            
            logger.info(f"ë³´ê³ ì„œ ë³€í™˜ ì™„ë£Œ (Markdown: {len(text_content):,}ì)")
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°±ì´ ìˆìœ¼ë©´ DART ì™„ë£Œ ë¡œê·¸ ì „ì†¡
            if hasattr(self, 'status_callback') and self.status_callback:
                self.status_callback(f"âšª DART ì™„ë£Œ: ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ")
            
            # 3. VectorDBì— ì €ì¥
            if text_content and company_name and report_name and report_date:
                logger.info("VectorDBì— ë³´ê³ ì„œ ì €ì¥ ì¤‘...")
                try:
                    self.vector_store.add_report(
                        rcept_no=rcept_no,
                        report_name=report_name,
                        company_name=company_name,
                        report_date=report_date,
                        content=text_content
                    )
                    logger.info("VectorDB ì €ì¥ ì™„ë£Œ")
                except Exception as ve:
                    logger.warning(f"VectorDB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {ve}")
            
            return text_content, save_path, extracted_path
            
        except Exception as e:
            logger.error(f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return "", None, None
    
    def _extract_text_from_xml(self, xml_content):
        """
        XML íŒŒì¼ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (XML íƒœê·¸ë§Œ ì œê±°, ëª¨ë“  ë‚´ìš© ë³´ì¡´)
        
        Args:
            xml_content: XML ë‚´ìš©
            
        Returns:
            str: ìˆœìˆ˜ í…ìŠ¤íŠ¸ (ì„œì‹ ì—†ìŒ, XML íƒœê·¸ë§Œ ì œê±°)
        """
        try:
            from bs4 import BeautifulSoup
            
            original_size = len(xml_content)
            logger.debug(f"ì›ë³¸ XML í¬ê¸°: {original_size:,}ì ({original_size/1024:.1f}KB)")
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            logger.debug("XML íŒŒì‹± ì¤‘ (BeautifulSoup)...")
            try:
                soup = BeautifulSoup(xml_content, 'xml')
                logger.debug("XML íŒŒì‹± ì„±ê³µ")
            except:
                logger.debug("XML íŒŒì„œ ì‹¤íŒ¨, lxml ì‹œë„...")
                try:
                    soup = BeautifulSoup(xml_content, 'lxml')
                    logger.debug("lxml íŒŒì‹± ì„±ê³µ")
                except:
                    logger.warning("ëª¨ë“  íŒŒì„œ ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ ì „í™˜")
                    return self._simple_text_extraction(xml_content)
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (íƒœê·¸ ì œê±°, ë‚´ìš© ë³´ì¡´, ì œí•œ ì—†ìŒ)
            logger.debug("ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (ì œí•œ ì—†ìŒ)")
            extracted_text = soup.get_text(separator='\n', strip=False)
            
            # ì •ë¦¬
            logger.debug("í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘...")
            
            # 1. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ (ì¤„ë°”ê¿ˆì€ ë³´ì¡´)
            extracted_text = re.sub(r'[ \t]+', ' ', extracted_text)
            
            # 2. 3ê°œ ì´ìƒì˜ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
            extracted_text = re.sub(r'\n{3,}', '\n\n', extracted_text)
            
            # 3. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
            lines = extracted_text.split('\n')
            lines = [line.strip() for line in lines if line.strip()]  # ë¹ˆ ì¤„ ì œê±°
            extracted_text = '\n'.join(lines)
            
            # 4. ì „ì²´ ì•ë’¤ ê³µë°± ì œê±°
            extracted_text = extracted_text.strip()
            
            extracted_size = len(extracted_text)
            
            logger.info(f"XML â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ! ì›ë³¸: {original_size:,}ì â†’ ì¶”ì¶œ: {extracted_size:,}ì (ë³´ì¡´ìœ¨: {(extracted_size/original_size*100):.1f}%)")
            
            if not extracted_text or len(extracted_text) < 1000:
                logger.warning(f"ì¶”ì¶œ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìŒ ({len(extracted_text)}ì), ì •ê·œì‹ìœ¼ë¡œ ì¬ì‹œë„")
                return self._simple_text_extraction(xml_content)
            
            return extracted_text
            
        except Exception as e:
            logger.warning(f"XML íŒŒì‹± ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ ì „í™˜: {e}")
            return self._simple_text_extraction(xml_content)
    
    def _parse_table_to_markdown(self, table_element):
        """í…Œì´ë¸”ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            rows = []
            
            # TR íƒœê·¸ ì°¾ê¸°
            for tr in table_element.iter():
                if 'TR' in tr.tag.upper():
                    cells = []
                    for td in tr:
                        if 'TD' in td.tag.upper() or 'TH' in td.tag.upper() or 'TU' in td.tag.upper():
                            cell_text = td.text.strip() if td.text else ''
                            cells.append(cell_text)
                    
                    if cells:
                        rows.append(cells)
            
            if not rows:
                return ""
            
            # Markdown í…Œì´ë¸” ìƒì„±
            md_table = []
            
            # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ
            if rows:
                header = ' | '.join(rows[0])
                md_table.append(f"| {header} |")
                md_table.append('|' + ' --- |' * len(rows[0]))
                
                # ë‚˜ë¨¸ì§€ í–‰
                for row in rows[1:6]:  # ìµœëŒ€ 5ê°œ í–‰ë§Œ
                    row_text = ' | '.join(row)
                    md_table.append(f"| {row_text} |")
            
            return '\n'.join(md_table) + '\n\n'
            
        except:
            return ""
    
    def _simple_text_extraction(self, xml_content):
        """
        ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°±ì—…ìš©) - ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ XML íƒœê·¸ë§Œ ì œê±°
        ëª¨ë“  ë‚´ìš©ì„ ë³´ì¡´í•˜ê³  XML íƒœê·¸ë§Œ ì œê±°
        """
        logger.debug("ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“œ (ì •ê·œì‹)")
        
        # 1. XML íƒœê·¸ ì œê±° (ë‚´ìš©ì€ ë³´ì¡´)
        # <tag>content</tag> â†’ content
        text = re.sub(r'<[^>]+>', '\n', xml_content)
        
        # 2. XML ì„ ì–¸, DOCTYPE ë“± ì œê±°
        text = re.sub(r'<\?xml[^>]*\?>', '', text)
        text = re.sub(r'<!DOCTYPE[^>]*>', '', text)
        
        # 3. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ (ë‹¨, ì¤„ë°”ê¿ˆì€ ë³´ì¡´)
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 4. 3ê°œ ì´ìƒì˜ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 5. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        text = '\n'.join(lines)
        
        # 6. ë¹ˆ ì¤„ ì—°ì† ì œê±°
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 7. ì „ì²´ ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        logger.debug(f"ì¶”ì¶œ ê²°ê³¼: {len(text):,}ì ({len(text)/1024:.1f}KB)")
        
        return text
    
    def cleanup_downloads(self, keep_latest=0):
        """
        downloads í´ë”ì˜ íŒŒì¼ë“¤ì„ ì •ë¦¬
        ë²¡í„°DBì— ì €ì¥ëœ ë³´ê³ ì„œëŠ” ì‚­ì œí•´ë„ ì•ˆì „í•¨
        
        Args:
            keep_latest: ìµœì‹  íŒŒì¼ ëª‡ ê°œë¥¼ ë‚¨ê¸¸ì§€ (0ì´ë©´ ëª¨ë‘ ì‚­ì œ)
        """
        downloads_dir = 'downloads'
        
        if not os.path.exists(downloads_dir):
            logger.info(f"'{downloads_dir}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ ì‹œì‘... ê²½ë¡œ: {downloads_dir}")
        
        try:
            # ëª¨ë“  íŒŒì¼ ë° í´ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            items = []
            for item in os.listdir(downloads_dir):
                item_path = os.path.join(downloads_dir, item)
                if os.path.isfile(item_path) or os.path.isdir(item_path):
                    # ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
                    mtime = os.path.getmtime(item_path)
                    items.append((item_path, mtime))
            
            # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
            items.sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"ì´ {len(items)}ê°œ í•­ëª© ë°œê²¬")
            
            # ì‚­ì œí•  í•­ëª© ê²°ì •
            items_to_delete = items[keep_latest:] if keep_latest > 0 else items
            items_to_keep = items[:keep_latest] if keep_latest > 0 else []
            
            if items_to_keep:
                print(f"   ğŸ”’ ìµœì‹  {len(items_to_keep)}ê°œ í•­ëª© ìœ ì§€:")
                for path, _ in items_to_keep[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                    print(f"      - {os.path.basename(path)}")
            
            if items_to_delete:
                print(f"   ğŸ—‘ï¸  {len(items_to_delete)}ê°œ í•­ëª© ì‚­ì œ ì¤‘...")
                
                deleted_count = 0
                freed_size = 0
                
                for item_path, _ in items_to_delete:
                    try:
                        # í¬ê¸° ê³„ì‚°
                        if os.path.isfile(item_path):
                            size = os.path.getsize(item_path)
                            os.remove(item_path)
                            deleted_count += 1
                            freed_size += size
                        elif os.path.isdir(item_path):
                            size = sum(
                                os.path.getsize(os.path.join(dirpath, filename))
                                for dirpath, dirnames, filenames in os.walk(item_path)
                                for filename in filenames
                            )
                            shutil.rmtree(item_path)
                            deleted_count += 1
                            freed_size += size
                    except Exception as e:
                        print(f"      âš ï¸  ì‚­ì œ ì‹¤íŒ¨: {os.path.basename(item_path)} - {e}")
                
                print(f"   âœ… ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ í•­ëª© ì‚­ì œ")
                print(f"   ğŸ’¾ í™•ë³´ëœ ê³µê°„: {freed_size / 1024 / 1024:.2f} MB")
            else:
                print(f"   â„¹ï¸  ì‚­ì œí•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def extract_text_from_pdf(self, pdf_path):
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pdf_path}")
            text = ""
            
            with fitz.open(pdf_path) as doc:
                total_pages = len(doc)
                print(f"   ğŸ“– ì´ {total_pages}í˜ì´ì§€")
                
                for page_num, page in enumerate(doc, 1):
                    page_text = page.get_text()
                    text += f"\n--- í˜ì´ì§€ {page_num} ---\n{page_text}"
                    
                    if page_num % 10 == 0:
                        print(f"   âœ“ {page_num}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
                
                print(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text):,}ì")
                return text
                
        except Exception as e:
            print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def get_company_industry(self, corp_code, company_name):
        """
        DART APIë¡œ ê¸°ì—…ì˜ ì‚°ì—…êµ° íŒŒì•…
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            company_name: íšŒì‚¬ëª…
            
        Returns:
            str: ì‚°ì—…êµ° ì •ë³´
        """
        try:
            print(f"ğŸ¢ {company_name}ì˜ ì‚°ì—…êµ° íŒŒì•… ì¤‘...")
            
            # DART ê¸°ì—…ê°œí™© API í˜¸ì¶œ
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code
            }
            
            response = requests.get(f'{self.base_url}/company.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                industry = data.get('induty_code', '')
                
                if industry:
                    print(f"   âœ… ì‚°ì—…êµ°: {industry}")
                    return industry
                else:
                    print(f"   âš ï¸  ì‚°ì—…êµ° ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Geminië¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
                    # Geminië¡œ ì‚°ì—…êµ° ì¶”ë¡ 
                    return self._infer_industry_with_gemini(company_name)
            else:
                print(f"   âš ï¸  API í˜¸ì¶œ ì‹¤íŒ¨, Geminië¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
                return self._infer_industry_with_gemini(company_name)
                
        except Exception as e:
            print(f"   âš ï¸  ì‚°ì—…êµ° ì¡°íšŒ ì‹¤íŒ¨: {e}, Geminië¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
            return self._infer_industry_with_gemini(company_name)
    
    def _infer_industry_with_gemini(self, company_name):
        """
        Geminië¡œ ê¸°ì—…ì˜ ì‚°ì—…êµ° ì¶”ë¡ 
        
        Args:
            company_name: íšŒì‚¬ëª…
            
        Returns:
            str: ì¶”ë¡ ëœ ì‚°ì—…êµ°
        """
        try:
            prompt = self.prompt_manager.get_prompt('industry_inference', company_name=company_name)
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 60)
            logger.info("ğŸ“ FULL PROMPT (Industry Inference)")
            logger.info("=" * 60)
            logger.info(prompt)
            logger.info("=" * 60)
            
            industry = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            print(f"   ğŸ¤– Gemini ì¶”ë¡  ì‚°ì—…êµ°: {industry}")
            return industry
            
        except Exception as e:
            print(f"   âš ï¸  Gemini ì‚°ì—…êµ° ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return "ì¼ë°˜"
    
    def _extract_industry_keywords(self, user_query, company_name, base_industry):
        """
        Geminië¡œ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì‚°ì—…ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            company_name: íšŒì‚¬ëª…
            base_industry: ê¸°ë³¸ ì‚°ì—…êµ°
            
        Returns:
            list: ì‚°ì—…ë¶„ì„ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ¤– Geminiì—ê²Œ ì‚°ì—…ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ ì¤‘...")
            
            prompt = self.prompt_manager.get_prompt(
                'industry_keywords_extraction',
                user_query=user_query,
                company_name=company_name,
                base_industry=base_industry
            )
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 60)
            logger.info("ğŸ“ FULL PROMPT (Industry Keywords Extraction)")
            logger.info("=" * 60)
            logger.info(prompt)
            logger.info("=" * 60)
            
            response_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            
            # JSON íŒŒì‹±
            import json
            import re
            
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_text = json_match.group(0)
                result = json.loads(json_text)
                
                keywords = result.get('keywords', [])
                reason = result.get('reason', '')
                
                print(f"   âœ… ì¶”ì¶œëœ ì‚°ì—… í‚¤ì›Œë“œ: {keywords}")
                print(f"   ğŸ’¡ ì´ìœ : {reason}")
                
                return keywords
            else:
                print(f"   âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì‚°ì—…êµ° ì‚¬ìš©")
                return [base_industry.split(',')[0].strip()]
                
        except Exception as e:
            print(f"   âš ï¸  ì‚°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ê¸°ë³¸ ì‚°ì—…êµ°ì˜ ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì‚¬ìš©
            return [base_industry.split(',')[0].strip()]
    
    def get_historical_annual_reports(self, corp_code, years=5):
        """
        ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ (ì¥ê¸° ì¶”ì„¸ ë¶„ì„ìš©)
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            years: ìˆ˜ì§‘í•  ì—°ìˆ˜
            
        Returns:
            list: ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡
        """
        print(f"ğŸ“… ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ ì¤‘... ({years}ë…„ì¹˜)")
        
        try:
            # ê²€ìƒ‰ ê¸°ê°„ ì„¤ì •
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')
            
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code,
                'bgn_de': start_date,
                'end_de': end_date,
                'page_no': 1,
                'page_count': 100
            }
            
            response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                all_reports = data.get('list', [])
                
                # ì‚¬ì—…ë³´ê³ ì„œë§Œ í•„í„°ë§
                annual_reports = []
                for report in all_reports:
                    report_name = report.get('report_nm', '')
                    if 'ì‚¬ì—…ë³´ê³ ì„œ' in report_name:
                        annual_reports.append(report)
                
                # ì—°ë„ë³„ë¡œ ê·¸ë£¹í™” (ê° ì—°ë„ë‹¹ 1ê°œì”©ë§Œ)
                reports_by_year = {}
                for report in annual_reports:
                    rcept_dt = report.get('rcept_dt', '')
                    if rcept_dt and len(rcept_dt) >= 4:
                        year = rcept_dt[:4]  # YYYYMMDDì—ì„œ YYYY ì¶”ì¶œ
                        # í•´ë‹¹ ì—°ë„ì˜ ì²« ë²ˆì§¸ ë³´ê³ ì„œë§Œ ì €ì¥ (ìµœì‹ ìˆœ ì •ë ¬ë˜ì–´ ìˆìŒ)
                        if year not in reports_by_year:
                            reports_by_year[year] = report
                
                # ì—°ë„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  â†’ ê³¼ê±°)
                sorted_reports = [reports_by_year[year] for year in sorted(reports_by_year.keys(), reverse=True)]
                
                # ìµœëŒ€ yearsê°œê¹Œì§€ë§Œ (ìµœì†Œ 2ê°œëŠ” ë³´ì¥)
                selected_reports = sorted_reports[:max(2, years)]
                
                print(f"   âœ… {len(selected_reports)}ê°œ ì—°ë„ì˜ ì‚¬ì—…ë³´ê³ ì„œ ë°œê²¬")
                for report in selected_reports:
                    rcept_dt = report.get('rcept_dt', '')
                    report_nm = report.get('report_nm', '')
                    print(f"      - {rcept_dt[:4]}ë…„: {report_nm}")
                
                return selected_reports
            else:
                print(f"   âŒ ì‚¬ì—…ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {data.get('message')}")
                return []
                
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_analyst_reports(self, corp_code, count=5, user_query=None, years=None):
        """
        ê´€ë ¨ ë³´ê³ ì„œ ì¡°íšŒ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ë³´ê³ ì„œ ì„ íƒ)
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            count: ì¡°íšŒí•  ë³´ê³ ì„œ ìˆ˜
            user_query: ì‚¬ìš©ì ì§ˆë¬¸ (ë³´ê³ ì„œ íƒ€ì… ìë™ ì„ íƒìš©)
            years: ê²€ìƒ‰í•  ì—°ìˆ˜ (Noneì´ë©´ user_queryì—ì„œ ìë™ ì¶”ì¶œ)
            
        Returns:
            list: ë³´ê³ ì„œ ëª©ë¡
        """
        print(f"ğŸ“Š ì¶”ê°€ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘... (ìµœëŒ€ {count}ê°œ)")
        
        # ì‹œê°„ ë²”ìœ„ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ user_queryì—ì„œ ì¶”ì¶œ
        if years is None and user_query:
            years = self._extract_time_range(user_query)
        elif years is None:
            years = 3  # ê¸°ë³¸ê°’ (ë©”ì¸ë³´ë‹¤ ë„“ê²Œ)
        
        # ë³´ê³ ì„œ íƒ€ì… ìë™ ì„ íƒ
        need_historical = False
        if user_query:
            print(f"   ğŸ’¡ ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œ íƒ€ì… ì„ íƒ")
            target_types, need_historical = self._recommend_report_types(user_query, years)
            
            # ê¸°ë³¸ ë³´ê³ ì„œë“¤ë„ í¬í•¨ (ì¤‘ë³µ ì œê±°)
            default_types = ['ë¶„ê¸°ë³´ê³ ì„œ', 'ë°˜ê¸°ë³´ê³ ì„œ', 'ì‚¬ì—…ë³´ê³ ì„œ', 'ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ', 'ê¸°íƒ€ê²½ì˜ì‚¬í•­(ììœ¨ê³µì‹œ)', 'ì¡°íšŒê³µì‹œìš”êµ¬']
            for dtype in default_types:
                if dtype not in target_types:
                    target_types.append(dtype)
        else:
            # ê¸°ë³¸ ë³´ê³ ì„œ íƒ€ì…
            target_types = [
                'ë¶„ê¸°ë³´ê³ ì„œ',
                'ë°˜ê¸°ë³´ê³ ì„œ', 
                'ì‚¬ì—…ë³´ê³ ì„œ',
                'ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ',
                'ê¸°íƒ€ê²½ì˜ì‚¬í•­(ììœ¨ê³µì‹œ)',
                'ì¡°íšŒê³µì‹œìš”êµ¬',
            ]
        
        try:
            # ì§€ì •ëœ ê¸°ê°„ ê²€ìƒ‰
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')
            
            print(f"   ğŸ“… ê²€ìƒ‰ ê¸°ê°„: {start_date} ~ {end_date} ({years}ë…„)")
            
            # ì¥ê¸° ì¶”ì„¸ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘
            if need_historical:
                print(f"   ğŸ“š ì¥ê¸° ì¶”ì„¸ ë¶„ì„ ê°ì§€: ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘")
                historical_reports = self.get_historical_annual_reports(corp_code, years)
                
                # ì¼ë°˜ ë³´ê³ ì„œë„ ìˆ˜ì§‘ (ì‚¬ì—…ë³´ê³ ì„œ ì œì™¸)
                params = {
                    'crtfc_key': self.dart_api_key,
                    'corp_code': corp_code,
                    'bgn_de': start_date,
                    'end_de': end_date,
                    'page_no': 1,
                    'page_count': 50
                }
                
                response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                regular_reports = []
                if data.get('status') == '000':
                    all_reports = data.get('list', [])
                    
                    # ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ íƒ€ì… í•„í„°ë§
                    for report in all_reports:
                        report_name = report.get('report_nm', '')
                        # ì‚¬ì—…ë³´ê³ ì„œëŠ” ì´ë¯¸ historical_reportsì— ìˆìœ¼ë¯€ë¡œ ì œì™¸
                        if 'ì‚¬ì—…ë³´ê³ ì„œ' not in report_name:
                            for report_type in target_types:
                                if report_type in report_name:
                                    regular_reports.append(report)
                                    break
                    
                    # ìµœì‹ ìˆœìœ¼ë¡œ ì œí•œ
                    regular_reports = regular_reports[:max(1, count - len(historical_reports))]
                
                # ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ + ì¼ë°˜ ë³´ê³ ì„œ í•©ì¹¨
                selected_reports = historical_reports + regular_reports
                
                # ìµœëŒ€ countê°œë¡œ ì œí•œ
                selected_reports = selected_reports[:count]
                
                print(f"âœ… ì´ {len(selected_reports)}ê°œì˜ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                print(f"   - ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ: {len(historical_reports)}ê°œ")
                print(f"   - ê¸°íƒ€ ë³´ê³ ì„œ: {len(regular_reports)}ê°œ")
                
                for i, report in enumerate(selected_reports, 1):
                    report_name = report.get('report_nm', '')
                    rcept_dt = report.get('rcept_dt', '')
                    marker = "ğŸ“…" if 'ì‚¬ì—…ë³´ê³ ì„œ' in report_name else "ğŸ“„"
                    print(f"   [{i}] {marker} {report_name} ({rcept_dt})")
                
                return selected_reports
            
            # ì¼ë°˜ì ì¸ ê²½ìš° (ì¥ê¸° ì¶”ì„¸ ì•„ë‹˜)
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code,
                'bgn_de': start_date,
                'end_de': end_date,
                'page_no': 1,
                'page_count': 100
            }
            
            response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                all_reports = data.get('list', [])
                
                # ë‹¤ì–‘í•œ ë³´ê³ ì„œ ìœ í˜• í•„í„°ë§
                # Gemini ì¶”ì²œ ë³´ê³ ì„œë¥¼ ìš°ì„  ìˆœìœ„ë¡œ ë°°ì¹˜
                filtered_reports = []
                for report in all_reports:
                    report_name = report.get('report_nm', '')
                    for report_type in target_types:
                        if report_type in report_name:
                            filtered_reports.append(report)
                            break
                
                # ìµœì‹ ìˆœìœ¼ë¡œ countê°œë§Œ ì„ íƒ (ìµœì†Œ 2ê°œëŠ” ë³´ì¥)
                selected_reports = filtered_reports[:max(2, count)]
                
                print(f"âœ… ì´ {len(selected_reports)}ê°œì˜ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                for i, report in enumerate(selected_reports, 1):
                    report_name = report.get('report_nm', '')
                    rcept_dt = report.get('rcept_dt', '')
                    marker = "ğŸ“„"
                    print(f"   [{i}] {marker} {report_name} ({rcept_dt})")
                
                return selected_reports
            else:
                print(f"âŒ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {data.get('message')}")
                return []
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return []
    
    def download_multiple_reports(self, reports, max_reports=5, company_name=None):
        """
        ì—¬ëŸ¬ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ë° ë‚´ìš© ì¶”ì¶œ
        
        Args:
            reports: ë³´ê³ ì„œ ëª©ë¡
            max_reports: ìµœëŒ€ ë‹¤ìš´ë¡œë“œ ìˆ˜
            company_name: íšŒì‚¬ëª… (ë²¡í„°DB ì €ì¥ìš©)
            
        Returns:
            list: [(report_name, content), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“¥ {min(len(reports), max_reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        
        downloaded_reports = []
        
        for i, report in enumerate(reports[:max_reports], 1):
            rcept_no = report.get('rcept_no')
            report_name = report.get('report_nm')
            rcept_dt = report.get('rcept_dt')
            
            print(f"\n[{i}/{min(len(reports), max_reports)}] {report_name} ({rcept_dt})")
            
            try:
                content, zip_path, xml_path = self.download_report(
                    rcept_no=rcept_no,
                    company_name=company_name,
                    report_name=report_name,
                    report_date=rcept_dt
                )
                
                if content:
                    downloaded_reports.append({
                        'name': report_name,
                        'date': rcept_dt,
                        'content': content,
                        'rcept_no': rcept_no
                    })
                    # VectorDBì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°ì™€ API ë‹¤ìš´ë¡œë“œ êµ¬ë¶„
                    if zip_path is None and xml_path is None:
                        print(f"âœ… VectorDB ê²€ìƒ‰ í™•ì¸: {len(content):,}ì")
                    else:
                        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(content):,}ì")
                else:
                    print(f"âš ï¸  ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\nâœ… ì´ {len(downloaded_reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        return downloaded_reports
    
    def analyze_with_gemini(self, company_name, report_content, user_query, additional_reports=None, exclude_opinions=False):
        """
        Gemini APIë¡œ ë³´ê³ ì„œ ë¶„ì„
        
        Args:
            company_name: íšŒì‚¬ëª…
            report_content: ë©”ì¸ ë³´ê³ ì„œ ë‚´ìš©
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            additional_reports: ì¶”ê°€ ë³´ê³ ì„œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            str: ë¶„ì„ ê²°ê³¼
        """
        print(f"ğŸ¤– Geminië¡œ ë¶„ì„ ì¤‘...")
        
        try:
            # Gemini 2.5 ProëŠ” 1ë°±ë§Œ í† í°ì„ ì§€ì›í•˜ë¯€ë¡œ ë³´ê³ ì„œ ì „ì²´ ì‚¬ìš© ê°€ëŠ¥
            # í•˜ì§€ë§Œ ë„ˆë¬´ í¬ë©´ ì‘ë‹µ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ì ˆíˆ ì œí•œ
            max_length_per_report = 200000  # ë³´ê³ ì„œë‹¹ ìµœëŒ€ 20ë§Œì
            
            # ë©”ì¸ ë³´ê³ ì„œ ì²˜ë¦¬
            original_length = len(report_content)
            print(f"   ğŸ“Š ë©”ì¸ ë³´ê³ ì„œ ì •ë³´:")
            print(f"      - í˜•ì‹: Markdown (XMLì—ì„œ ë³€í™˜ë¨)")
            print(f"      - í¬ê¸°: {len(report_content):,}ì")
            print(f"      - ì˜ˆìƒ í† í°: ì•½ {len(report_content) / 4:,.0f} í† í°")
            
            if len(report_content) > max_length_per_report:
                print(f"   âš ï¸  ë³´ê³ ì„œê°€ í½ë‹ˆë‹¤. {max_length_per_report:,}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                report_content = report_content[:max_length_per_report]
                print(f"   âœ… ë³´ê³ ì„œ í¬ê¸° ì¡°ì •: {len(report_content):,}ì")
            
            # ì¶”ê°€ ë³´ê³ ì„œ ì²˜ë¦¬
            additional_content = ""
            dart_reports = []
            naver_reports = []
            
            if additional_reports:
                print(f"\n   ğŸ“š ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports)}ê°œ ì²˜ë¦¬ ì¤‘...")
                for i, report in enumerate(additional_reports, 1):
                    report_name = report.get('name', f'ë³´ê³ ì„œ {i}')
                    report_date = report.get('date', '')
                    content = report.get('content', '')
                    
                    # ê° ì¶”ê°€ ë³´ê³ ì„œë„ ì œí•œ
                    if len(content) > max_length_per_report:
                        content = content[:max_length_per_report]
                    
                    # ë³´ê³ ì„œ ì¶œì²˜ êµ¬ë¶„ (ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì¸ì§€ í™•ì¸)
                    if report_name.startswith('[') and ']' in report_name:
                        # ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í˜•ì‹: [ì¦ê¶Œì‚¬ëª…] ì œëª©
                        naver_reports.append((report_name, report_date, content))
                    else:
                        # DART ë³´ê³ ì„œ
                        dart_reports.append((report_name, report_date, content))
                
                # DART ë³´ê³ ì„œ ì¶”ê°€
                if dart_reports:
                    additional_content += "\n\n" + "="*80 + "\n"
                    additional_content += "ğŸ“‹ DART ê³µì‹œ ë³´ê³ ì„œ\n"
                    additional_content += "="*80 + "\n"
                    
                    for idx, (name, date, content) in enumerate(dart_reports, 1):
                        additional_content += f"""

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[DART ë³´ê³ ì„œ {idx}: {name} ({date})]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{content}
"""
                        print(f"      âœ“ [DART {idx}] {name}: {len(content):,}ì")
                
                # ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ì¶”ê°€
                if naver_reports:
                    additional_content += "\n\n" + "="*80 + "\n"
                    additional_content += "ğŸ“Š ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ (ì‹œì¥ ë¶„ì„ ë° ì „ë§)\n"
                    additional_content += "="*80 + "\n"
                    
                    for idx, (name, date, content) in enumerate(naver_reports, 1):
                        additional_content += f"""

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ {idx}: {name} ({date})]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{content}
"""
                        print(f"      âœ“ [ì¦ê¶Œì‚¬ {idx}] {name}: {len(content):,}ì")
                
                print(f"   âœ… ì¶”ê°€ ë³´ê³ ì„œ í†µí•© ì™„ë£Œ: ì´ {len(additional_content):,}ì")
                print(f"      - DART ë³´ê³ ì„œ: {len(dart_reports)}ê°œ")
                print(f"      - ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸: {len(naver_reports)}ê°œ")
            
            # ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            print(f"   ğŸ“ Gemini í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            
            # ì¶”ê°€ ë³´ê³ ì„œ ì„¹ì…˜ ì¤€ë¹„
            additional_section = additional_content if additional_content else ""
            
            # ì˜ê²¬ì œì™¸ ì§€ì‹œì‚¬í•­ ìƒì„±
            exclude_opinions_instruction = ""
            if exclude_opinions:
                exclude_opinions_instruction = "\n6. **ì¤‘ìš”**: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë‚´ ì‘ì„±ìì˜ ì˜ê²¬, ì¶”ì²œ, íˆ¬ì ì˜ê²¬, ëª©í‘œì£¼ê°€, ë“±ê¸‰ ë“±ì„ ì œì™¸í•˜ê³  ì˜¤ì§ ì‚¬ì‹¤(Fact) ê¸°ë°˜ ë°ì´í„°ë§Œì„ ì°¸ì¡°í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”. ì˜ê²¬ì´ í¬í•¨ëœ ë¶€ë¶„ì€ ëª…ì‹œì ìœ¼ë¡œ ì œì™¸í•˜ê³  ê°ê´€ì  ë°ì´í„°ë§Œ í™œìš©í•˜ì„¸ìš”."
            
            prompt = self.prompt_manager.get_prompt(
                'full_analysis',
                company_name=company_name,
                user_query=user_query,
                main_report=report_content,
                additional_section=additional_section,
                exclude_opinions_instruction=exclude_opinions_instruction
            )
            
            # LLM Orchestratorë¡œ ë¶„ì„
            total_content_length = len(report_content) + len(additional_content)
            print(f"   ğŸš€ LLM Orchestrator í˜¸ì¶œ ì¤‘...")
            print(f"      ì…ë ¥: ì•½ {len(prompt) / 4:,.0f} í† í°")
            print(f"      ì´ ë³´ê³ ì„œ ë‚´ìš©: {total_content_length:,}ì")
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 80)
            logger.info("ğŸ“ FULL PROMPT (Full Analysis)")
            logger.info("=" * 80)
            logger.info(prompt)
            logger.info("=" * 80)
            
            result_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='long_context_analysis'
            )
            
            print(f"   âœ… LLM ë¶„ì„ ì™„ë£Œ!")
            print(f"      ì¶œë ¥: {len(result_text):,}ì (ì•½ {len(result_text) / 4:,.0f} í† í°)")
            
            return result_text
            
        except Exception as e:
            error_message = str(e)
            print(f"âŒ Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
            if "429" in error_message or "quota" in error_message.lower():
                return f"""
âš ï¸ Gemini API ë¬´ë£Œ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**ë¬¸ì œ**: Gemini API ë¬´ë£Œ í‹°ì–´ì˜ ì¼ì¼ ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. 24ì‹œê°„ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš” (í• ë‹¹ëŸ‰ì´ ë¦¬ì…‹ë©ë‹ˆë‹¤)
2. Google AI Studioì—ì„œ ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
3. API í‚¤ë¥¼ ì¬ë°œê¸‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ê³„ì •ì˜ í‚¤ ì‚¬ìš©

**í˜„ì¬ ìƒí™©**:
- íšŒì‚¬: {company_name}
- ë³´ê³ ì„œ: ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(report_content):,}ì)
- ì˜¤ë¥˜: ë¬´ë£Œ í• ë‹¹ëŸ‰ ì´ˆê³¼

ìì„¸í•œ ì •ë³´: https://ai.google.dev/gemini-api/docs/rate-limits
"""
            else:
                return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message[:500]}"
    
    def analyze_with_gemini_rag(self, company_name, user_query, relevant_context, num_chunks, exclude_opinions=False):
        """
        RAG ë°©ì‹ìœ¼ë¡œ Gemini AI ë¶„ì„ (VectorDBì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ë§Œ ì‚¬ìš©)
        
        Args:
            company_name: íšŒì‚¬ëª…
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            relevant_context: VectorDBì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ë“¤ (ê²°í•©ëœ ë¬¸ìì—´)
            num_chunks: ê²€ìƒ‰ëœ ì²­í¬ ê°œìˆ˜
            
        Returns:
            str: AI ë¶„ì„ ê²°ê³¼ (Markdown)
        """
        try:
            print(f"\nğŸ¤– Gemini AI ë¶„ì„ (RAG ëª¨ë“œ)")
            print(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°:")
            print(f"      - íšŒì‚¬ëª…: {company_name}")
            print(f"      - ê²€ìƒ‰ëœ ì²­í¬: {num_chunks}ê°œ")
            print(f"      - ì´ í…ìŠ¤íŠ¸: {len(relevant_context):,}ì")
            print(f"      - ì§ˆë¬¸: {user_query[:100]}...")
            
            # ì˜ê²¬ì œì™¸ ì§€ì‹œì‚¬í•­ ìƒì„±
            exclude_opinions_instruction = ""
            if exclude_opinions:
                exclude_opinions_instruction = "\n6. **ì¤‘ìš”**: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë‚´ ì‘ì„±ìì˜ ì˜ê²¬, ì¶”ì²œ, íˆ¬ì ì˜ê²¬, ëª©í‘œì£¼ê°€, ë“±ê¸‰ ë“±ì„ ì œì™¸í•˜ê³  ì˜¤ì§ ì‚¬ì‹¤(Fact) ê¸°ë°˜ ë°ì´í„°ë§Œì„ ì°¸ì¡°í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”. ì˜ê²¬ì´ í¬í•¨ëœ ë¶€ë¶„ì€ ëª…ì‹œì ìœ¼ë¡œ ì œì™¸í•˜ê³  ê°ê´€ì  ë°ì´í„°ë§Œ í™œìš©í•˜ì„¸ìš”."
            
            # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_manager.get_prompt(
                'rag_analysis',
                company_name=company_name,
                user_query=user_query,
                num_chunks=num_chunks,
                relevant_context=relevant_context,
                exclude_opinions_instruction=exclude_opinions_instruction
            )
            
            # LLM Orchestratorë¡œ RAG ë¶„ì„
            print(f"   ğŸš€ LLM Orchestrator í˜¸ì¶œ ì¤‘... (RAG ëª¨ë“œ)")
            print(f"      ì…ë ¥: ì•½ {len(prompt) / 4:,.0f} í† í°")
            print(f"      ê´€ë ¨ ë‚´ìš©: {len(relevant_context):,}ì")
            
            # Full prompt ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 80)
            logger.info("ğŸ“ FULL PROMPT (RAG Analysis)")
            logger.info("=" * 80)
            logger.info(prompt)
            logger.info("=" * 80)
            
            result_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='long_context_analysis'
            )
            
            print(f"   âœ… LLM ë¶„ì„ ì™„ë£Œ!")
            print(f"      ì¶œë ¥: {len(result_text):,}ì (ì•½ {len(result_text) / 4:,.0f} í† í°)")
            
            return result_text
            
        except Exception as e:
            error_message = str(e)
            print(f"âŒ Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
            if "429" in error_message or "quota" in error_message.lower():
                return f"""âš ï¸ Gemini API ë¬´ë£Œ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**ë¬¸ì œ**: Gemini API ë¬´ë£Œ í‹°ì–´ì˜ ì¼ì¼ ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. 24ì‹œê°„ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš” (í• ë‹¹ëŸ‰ì´ ë¦¬ì…‹ë©ë‹ˆë‹¤)
2. Google AI Studioì—ì„œ ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
3. API í‚¤ë¥¼ ì¬ë°œê¸‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ê³„ì •ì˜ í‚¤ ì‚¬ìš©

**í˜„ì¬ ìƒí™©**:
- íšŒì‚¬: {company_name}
- VectorDB ê²€ìƒ‰: ì™„ë£Œ ({num_chunks}ê°œ ì²­í¬)
- ì˜¤ë¥˜: ë¬´ë£Œ í• ë‹¹ëŸ‰ ì´ˆê³¼

ìì„¸í•œ ì •ë³´: https://ai.google.dev/gemini-api/docs/rate-limits
"""
            else:
                return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message[:500]}"
    
    def _create_simple_summary(self, company_name, report_content, user_query):
        """
        AI ì—†ì´ ê°„ë‹¨í•œ ë³´ê³ ì„œ ìš”ì•½ ìƒì„±
        
        Args:
            company_name: íšŒì‚¬ëª…
            report_content: ë³´ê³ ì„œ ë‚´ìš©
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            str: ìš”ì•½ ë‚´ìš©
        """
        # ë³´ê³ ì„œ ì•ë¶€ë¶„ 2000ì ì¶”ì¶œ
        preview = report_content[:2000]
        
        summary = f"""
ğŸ“Š {company_name} ê³µì‹œë³´ê³ ì„œ ë¶„ì„

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ ì‚¬ìš©ì ì§ˆë¬¸
{user_query}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“„ ë³´ê³ ì„œ ì •ë³´
â€¢ íšŒì‚¬ëª…: {company_name}
â€¢ ì „ì²´ ë‚´ìš© ê¸¸ì´: {len(report_content):,}ì
â€¢ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: âœ…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‘ ë³´ê³ ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 2,000ì)

{preview}

... (ê³„ì†)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ ì•ˆë‚´
ë³´ê³ ì„œ ì „ì²´ ë‚´ìš©ì€ ì•„ë˜ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ZIP íŒŒì¼ê³¼ ì••ì¶• í•´ì œëœ XML íŒŒì¼ì´ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.
XML ë·°ì–´(VS Code, XML Notepad ë“±)ë¡œ ì—´ì–´ì„œ í™•ì¸í•˜ì„¸ìš”.

ë˜ëŠ” ì˜¤í”ˆë‹¤íŠ¸ ì›¹ì‚¬ì´íŠ¸(https://dart.fss.or.kr)ì—ì„œ
ë” ë³´ê¸° ì¢‹ì€ HTML ë²„ì „ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ AI ë¶„ì„ ê¸°ëŠ¥ ì•ˆë‚´
í˜„ì¬ Gemini API ì—°ë™ì„ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”í–ˆìŠµë‹ˆë‹¤.
ë³´ê³ ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì§ì ‘ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
        return summary
    
    def analyze_company(self, company_name, user_query, status_callback=None, exclude_opinions=False):
        """
        íšŒì‚¬ ë¶„ì„ ì „ì²´ í”„ë¡œì„¸ìŠ¤
        
        Args:
            company_name: íšŒì‚¬ëª…
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            status_callback: ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        import logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)
        
        def update_status(message, component=None, status_type=None):
            """ìƒíƒœ ì—…ë°ì´íŠ¸ í—¬í¼"""
            logger.info(message)
            if status_callback:
                status_callback(message)
            
            # ë‹¤ì´ì–´ê·¸ë¨ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ìƒì„¸ ë¡œê¹…
            if component and status_type:
                logger.info(f"ğŸ¯ ë‹¤ì´ì–´ê·¸ë¨ ì—…ë°ì´íŠ¸: {component} â†’ {status_type}")
                if status_callback:
                    status_callback(f"ğŸ¯ {component}:{status_type}")
        
        # ìƒíƒœ ì½œë°±ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì • (download_reportì—ì„œ ì‚¬ìš©)
        self.status_callback = status_callback
        
        result = {
            'success': False,
            'company_name': company_name,
            'corp_code': None,
            'stock_code': None,
            'reports_found': [],
            'analysis': None,
            'error': None,
            'logs': []
        }
        
        try:
            # 1. íšŒì‚¬ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ
            update_status(f"ğŸ”µ DART ì‹œì‘: íšŒì‚¬ ì •ë³´ ì¡°íšŒ", "dart", "start")
            update_status(f"ğŸ“‹ 1ë‹¨ê³„: '{company_name}' íšŒì‚¬ ì •ë³´ ì¡°íšŒ ì¤‘...")
            logger.info(f"íšŒì‚¬ëª…ìœ¼ë¡œ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ ì‹œì‘: {company_name}")
            
            corp_info = self.get_corp_code(company_name)
            if not corp_info:
                error_msg = f"'{company_name}' íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                result['error'] = error_msg
                return result
            
            logger.info(f"ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ ì„±ê³µ: {corp_info}")
            
            corp_code, found_name, stock_code = corp_info
            result['company_name'] = found_name
            result['corp_code'] = corp_code
            result['stock_code'] = stock_code
            update_status(f"âœ… íšŒì‚¬ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {found_name} (ì¢…ëª©ì½”ë“œ: {stock_code})")
            update_status(f"âšª DART ì™„ë£Œ: íšŒì‚¬ ì •ë³´ ì¡°íšŒ", "dart", "complete")
            
            # 2. ë³´ê³ ì„œ ê²€ìƒ‰ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ìë™ ì„ íƒ)
            update_status(f"ğŸ”µ Gemini AI ì‹œì‘: ì§ˆë¬¸ ë¶„ì„", "gemini", "start")
            update_status(f"ğŸ”µ DART ì‹œì‘: ë³´ê³ ì„œ ê²€ìƒ‰", "dart", "start")
            update_status(f"ğŸ“Š 2ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ë° ì ì ˆí•œ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")
            logger.info(f"ê³ ìœ ë²ˆí˜¸ë¡œ ë³´ê³ ì„œ ê²€ìƒ‰: {corp_code}, ì§ˆë¬¸: {user_query}")
            
            # ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ
            years = self._extract_time_range(user_query)
            logger.info(f"ê²€ìƒ‰ ê¸°ê°„: ìµœê·¼ {years}ë…„")
            
            reports = self.get_reports(corp_code, report_types=None, user_query=user_query, years=years)
            if not reports:
                error_msg = "ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                result['error'] = error_msg
                return result
            
            logger.info(f"ë³´ê³ ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(reports)}ê°œ ë°œê²¬")
            update_status(f"âœ… {len(reports)}ê°œì˜ ì í•©í•œ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            update_status(f"âšª Gemini AI ì™„ë£Œ: ì§ˆë¬¸ ë¶„ì„", "gemini", "complete")
            update_status(f"âšª DART ì™„ë£Œ: ë³´ê³ ì„œ ê²€ìƒ‰", "dart", "complete")
            
            result['reports_found'] = [
                {
                    'report_nm': r.get('report_nm'),
                    'rcept_dt': r.get('rcept_dt'),
                    'rcept_no': r.get('rcept_no')
                }
                for r in reports[:5]  # ìµœëŒ€ 5ê°œë§Œ
            ]
            
            # 3. ê°€ì¥ ìµœê·¼ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ
            latest_report = reports[0]
            rcept_no = latest_report['rcept_no']
            report_name = latest_report['report_nm']
            report_date = latest_report.get('rcept_dt', '')
            
            update_status(f"ğŸ”µ VectorDB ì‹œì‘: ìºì‹œ í™•ì¸", "vectordb", "start")
            update_status(f"ğŸ“¥ 3ë‹¨ê³„: ë©”ì¸ ë³´ê³ ì„œ ìˆ˜ì§‘ ì¤‘... ({report_name})")
            logger.info(f"ë©”ì¸ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹œì‘: {rcept_no} - {report_name}")
            
            report_content, zip_path, xml_path = self.download_report(
                rcept_no=rcept_no,
                company_name=found_name,
                report_name=report_name,
                report_date=report_date
            )
            
            if not report_content:
                error_msg = "ë³´ê³ ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                result['error'] = error_msg
                return result
            
            # VectorDBì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°ì™€ API ë‹¤ìš´ë¡œë“œ êµ¬ë¶„
            if zip_path is None and xml_path is None:
                # VectorDBì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°
                logger.info(f"ë©”ì¸ ë³´ê³ ì„œ VectorDB ê²€ìƒ‰ í™•ì¸: {len(report_content)}ì")
                logger.info(f"ë³´ê³ ì„œ í˜•ì‹: Markdown (ìºì‹œë¨)")
                update_status(f"âœ… ë©”ì¸ ë³´ê³ ì„œ VectorDB ê²€ìƒ‰ í™•ì¸ ({len(report_content):,}ì, Markdown í˜•ì‹)")
                update_status(f"âšª VectorDB ì™„ë£Œ: ìºì‹œ í™•ì¸ (ì ì¤‘)", "vectordb", "complete")
            else:
                # APIì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ê²½ìš°
                update_status(f"ğŸ”µ DART ì‹œì‘: ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", "dart", "start")
                logger.info(f"ë©”ì¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(report_content)}ì")
                logger.info(f"ë³´ê³ ì„œ í˜•ì‹: Markdown ë³€í™˜ë¨")
                update_status(f"âœ… ë©”ì¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(report_content):,}ì, Markdown í˜•ì‹)")
                update_status(f"âšª DART ì™„ë£Œ: ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", "dart", "complete")
                update_status(f"âšª VectorDB ì™„ë£Œ: ìºì‹œ í™•ì¸ (ë¯¸ì ì¤‘)", "vectordb", "complete")
            
            # ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë³´ ì €ì¥
            result['downloaded_files'] = {
                'zip_path': zip_path,
                'xml_path': xml_path,
                'rcept_no': rcept_no,
                'report_name': report_name
            }
            
            # 3-1. ì¶”ê°€ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (ìµœê·¼ 5ê°œ, ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜)
            update_status(f"ğŸ“š 3-1ë‹¨ê³„: DART ì¶”ê°€ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘... (ìµœê·¼ {years}ë…„, ìµœëŒ€ 5ê°œ)")
            logger.info(f"ì¶”ê°€ ë³´ê³ ì„œ ê²€ìƒ‰ ì‹œì‘ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜, ê¸°ê°„: {years}ë…„)")
            
            additional_reports_list = self.get_analyst_reports(corp_code, count=5, user_query=user_query, years=years)
            additional_reports = []
            
            if additional_reports_list:
                update_status(f"ğŸ“¥ DART ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports_list)}ê°œ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 10~20ì´ˆ ì†Œìš”)")
                logger.info(f"ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports_list)}ê°œ ìˆ˜ì§‘ ì‹œì‘")
                
                additional_reports = self.download_multiple_reports(
                    additional_reports_list,
                    max_reports=5,
                    company_name=found_name
                )
                
                if additional_reports:
                    total_additional_chars = sum(len(r.get('content', '')) for r in additional_reports)
                    update_status(f"âœ… ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_additional_chars:,}ì)")
                    logger.info(f"ì¶”ê°€ ë³´ê³ ì„œ ìˆ˜ì§‘ ì™„ë£Œ: {len(additional_reports)}ê°œ")
                    
                    result['additional_reports'] = [
                        {
                            'name': r.get('name'),
                            'date': r.get('date'),
                            'rcept_no': r.get('rcept_no')
                        }
                        for r in additional_reports
                    ]
                else:
                    update_status(f"âš ï¸  ì¶”ê°€ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    logger.warning("ì¶”ê°€ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨")
            else:
                update_status(f"â„¹ï¸  ì¶”ê°€ ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                logger.info("ì¶”ê°€ ë³´ê³ ì„œ ì—†ìŒ")
            
            # 3-2. ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ì¢…ëª©ë¶„ì„ + ì‚°ì—…ë¶„ì„)
            update_status(f"ğŸ”µ Naver ì‹œì‘: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘")
            update_status(f"ğŸ“ˆ 3-2ë‹¨ê³„: ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì¤‘...")
            logger.info("ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì‚°ì—…êµ° íŒŒì•…
            industry = self.get_company_industry(corp_code, found_name)
            logger.info(f"ì‚°ì—…êµ°: {industry}")
            
            # ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
            naver_company_reports = []
            company_reports_from_cache = False  # ìºì‹œ ì—¬ë¶€ ì¶”ì 
            
            try:
                update_status(f"ğŸ“Š ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ í™•ì¸ ì¤‘... ({found_name})")
                
                # ë¨¼ì € VectorDB ìºì‹œ í™•ì¸
                update_status(f"   ğŸ” VectorDB ìºì‹œ í™•ì¸ ì¤‘...")
                cached_company_reports = self.vector_store.get_naver_reports_from_cache(found_name, "NAVER_COMPANY")
                
                if cached_company_reports and len(cached_company_reports) >= 3:
                    # ìºì‹œì— ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    naver_company_reports = cached_company_reports[:3]
                    company_reports_from_cache = True  # ìºì‹œ ì‚¬ìš©
                    total_chars = sum(len(r.get('content', '')) for r in naver_company_reports)
                    update_status(f"âœ… VectorDB ìºì‹œì—ì„œ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ë¡œë“œ (í¬ë¡¤ë§ ìƒëµ!)")
                    
                    for idx, report in enumerate(naver_company_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìºì‹œ ì‚¬ìš©: {len(naver_company_reports)}ê°œ")
                else:
                    # ìºì‹œì— ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ í¬ë¡¤ë§
                    if cached_company_reports:
                        update_status(f"   â„¹ï¸  ìºì‹œì— {len(cached_company_reports)}ê°œë§Œ ìˆìŒ, ì¶”ê°€ í¬ë¡¤ë§ í•„ìš”")
                    else:
                        update_status(f"   â„¹ï¸  ìºì‹œ ì—†ìŒ, ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì‹œì‘")
                    
                    update_status(f"   ğŸ” ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì¤‘... (Gemini ì¶”ì²œ ê²€ìƒ‰ì–´ í™œìš©)")
                    naver_company_reports = self.naver_crawler.search_company_reports(found_name, max_reports=3)
                
                if naver_company_reports:
                    total_chars = sum(len(r.get('content', '')) for r in naver_company_reports)
                    update_status(f"âœ… ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_chars:,}ì)")
                    
                    # ë°œê²¬ëœ ë¦¬í¬íŠ¸ ìƒì„¸ ì •ë³´ ì¶œë ¥
                    for idx, report in enumerate(naver_company_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(naver_company_reports)}ê°œ")
                else:
                    update_status(f"â„¹ï¸  ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                    logger.info("ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ì—†ìŒ")
            except Exception as e:
                logger.warning(f"ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                update_status(f"âš ï¸  ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}")
            
            # ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
            naver_industry_reports = []
            industry_reports_from_cache = False  # ìºì‹œ ì—¬ë¶€ ì¶”ì 
            
            try:
                update_status(f"ğŸ­ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ í™•ì¸ ì¤‘...")
                
                # Geminië¡œ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì‚°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ
                update_status(f"   ğŸ¤– Geminiê°€ ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ì¤‘... (ì ì ˆí•œ ì‚°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ)")
                industry_keywords = self._extract_industry_keywords(user_query, found_name, industry)
                update_status(f"   âœ… ì¶”ì¶œëœ ì‚°ì—… í‚¤ì›Œë“œ: {', '.join(industry_keywords)}")
                
                # ë¨¼ì € VectorDB ìºì‹œ í™•ì¸ (ì‚°ì—… í‚¤ì›Œë“œë¡œ ê²€ìƒ‰!)
                update_status(f"   ğŸ” VectorDB ìºì‹œ í™•ì¸ ì¤‘... (í‚¤ì›Œë“œ: {', '.join(industry_keywords)})")
                cached_industry_reports = self.vector_store.get_naver_reports_from_cache(
                    company_name=None,  # ì‚°ì—…ë¶„ì„ì€ íšŒì‚¬ëª… í•„í„°ë§ ì•ˆ í•¨
                    report_type="NAVER_INDUSTRY",
                    industry_keywords=industry_keywords
                )
                
                if cached_industry_reports and len(cached_industry_reports) >= 2:
                    # ìºì‹œì— ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    naver_industry_reports = cached_industry_reports[:2]
                    industry_reports_from_cache = True  # ìºì‹œ ì‚¬ìš©
                    total_chars = sum(len(r.get('content', '')) for r in naver_industry_reports)
                    update_status(f"âœ… VectorDB ìºì‹œì—ì„œ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ë¡œë“œ (í¬ë¡¤ë§ ìƒëµ!)")
                    
                    for idx, report in enumerate(naver_industry_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìºì‹œ ì‚¬ìš©: {len(naver_industry_reports)}ê°œ")
                else:
                    # ìºì‹œì— ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ í¬ë¡¤ë§
                    if cached_industry_reports:
                        update_status(f"   â„¹ï¸  ìºì‹œì— {len(cached_industry_reports)}ê°œë§Œ ìˆìŒ, ì¶”ê°€ í¬ë¡¤ë§ í•„ìš”")
                    else:
                        update_status(f"   â„¹ï¸  ìºì‹œ ì—†ìŒ, ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì‹œì‘")
                    
                    update_status(f"   ğŸ” ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì¤‘... (í‚¤ì›Œë“œ: {', '.join(industry_keywords)})")
                    naver_industry_reports = self.naver_crawler.search_industry_reports(industry_keywords, max_reports=2)
                
                if naver_industry_reports:
                    total_chars = sum(len(r.get('content', '')) for r in naver_industry_reports)
                    update_status(f"âœ… ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_chars:,}ì)")
                    
                    # ë°œê²¬ëœ ë¦¬í¬íŠ¸ ìƒì„¸ ì •ë³´ ì¶œë ¥
                    for idx, report in enumerate(naver_industry_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(naver_industry_reports)}ê°œ")
                else:
                    update_status(f"â„¹ï¸  ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                    logger.info("ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ì—†ìŒ")
            except Exception as e:
                logger.warning(f"ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                update_status(f"âš ï¸  ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}")
            
            update_status(f"âšª Naver ì™„ë£Œ: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘")
            
            # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ ê²°ê³¼ ì €ì¥
            result['naver_reports'] = {
                'company_reports': [
                    {
                        'name': r.get('name'),
                        'date': r.get('date'),
                        'url': r.get('url')
                    }
                    for r in naver_company_reports
                ],
                'industry_reports': [
                    {
                        'name': r.get('name'),
                        'date': r.get('date'),
                        'url': r.get('url')
                    }
                    for r in naver_industry_reports
                ]
            }
            
            # ëª¨ë“  ì¶”ê°€ ë³´ê³ ì„œ í†µí•©
            all_additional_reports = additional_reports + naver_company_reports + naver_industry_reports
            logger.info(f"ì „ì²´ ì¶”ê°€ ë³´ê³ ì„œ: {len(all_additional_reports)}ê°œ (DART: {len(additional_reports)}, ì¢…ëª©: {len(naver_company_reports)}, ì‚°ì—…: {len(naver_industry_reports)})")
            
            # 3-3. ëª¨ë“  ë¦¬í¬íŠ¸ë¥¼ VectorDBì— ì €ì¥
            update_status(f"ğŸ”µ VectorDB ì‹œì‘: ë¦¬í¬íŠ¸ ì €ì¥")
            update_status(f"ğŸ’¾ VectorDBì— ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘...")
            logger.info("VectorDB ì €ì¥ ì‹œì‘")
            
            try:
                # ë©”ì¸ DART ë³´ê³ ì„œ ì €ì¥
                if not self.vector_store.check_report_exists(rcept_no):
                    update_status(f"   ğŸ’¾ ë©”ì¸ ë³´ê³ ì„œ VectorDB ì €ì¥ ì¤‘...")
                    self.vector_store.add_report(
                        rcept_no=rcept_no,
                        report_name=report_name,
                        company_name=found_name,
                        report_date=report_date,
                        content=report_content
                    )
                    logger.info(f"VectorDB ì €ì¥: {report_name} ({len(report_content):,}ì)")
                
                # ì¶”ê°€ DART ë³´ê³ ì„œ ì €ì¥
                for add_report in additional_reports:
                    add_rcept_no = add_report.get('rcept_no')
                    add_content = add_report.get('content', '')
                    
                    if add_rcept_no and add_content and not self.vector_store.check_report_exists(add_rcept_no):
                        self.vector_store.add_report(
                            rcept_no=add_rcept_no,
                            report_name=add_report.get('name', ''),
                            company_name=found_name,
                            report_date=add_report.get('date', ''),
                            content=add_content
                        )
                        logger.info(f"VectorDB ì €ì¥: {add_report.get('name', '')[:50]}")
                
                # ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ì €ì¥ (ìƒˆë¡œ í¬ë¡¤ë§í•œ ê²ƒë§Œ)
                new_reports_to_save = []
                
                if naver_company_reports and not company_reports_from_cache:
                    new_reports_to_save.extend([(r, "NAVER_COMPANY") for r in naver_company_reports])
                
                if naver_industry_reports and not industry_reports_from_cache:
                    new_reports_to_save.extend([(r, "NAVER_INDUSTRY") for r in naver_industry_reports])
                
                if new_reports_to_save:
                    update_status(f"   ğŸ’¾ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ {len(new_reports_to_save)}ê°œ VectorDB ì €ì¥ ì¤‘...")
                    
                    for report, report_type_prefix in new_reports_to_save:
                        report_name = report.get('name', '')
                        report_date = report.get('date', '')
                        content = report.get('content', '')
                        
                        if content:
                            report_url = report.get('url', '')
                            rcept_no_hash = f"{report_type_prefix}_{hash(report_url)}"
                            
                            keywords_to_save = None
                            if report_type_prefix == "NAVER_INDUSTRY":
                                keywords_to_save = industry_keywords
                            
                            if not self.vector_store.check_report_exists(rcept_no_hash):
                                self.vector_store.add_report(
                                    rcept_no=rcept_no_hash,
                                    report_name=report_name,
                                    company_name=found_name,
                                    report_date=report_date,
                                    content=content,
                                    industry_keywords=keywords_to_save
                                )
                                logger.info(f"VectorDB ì €ì¥: {report_name[:50]}")
                
                update_status(f"âœ… ëª¨ë“  ë¦¬í¬íŠ¸ VectorDB ì €ì¥ ì™„ë£Œ")
                update_status(f"âšª VectorDB ì™„ë£Œ: ë¦¬í¬íŠ¸ ì €ì¥")
                logger.info("VectorDB ì €ì¥ ì™„ë£Œ")
                
            except Exception as ve:
                logger.warning(f"VectorDB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {ve}")
                update_status(f"âš ï¸  VectorDB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")
                update_status(f"âšª VectorDB ì™„ë£Œ: ë¦¬í¬íŠ¸ ì €ì¥ (ì‹¤íŒ¨)")
            
            # 4. Geminië¡œ ë¶„ì„
            if all_additional_reports:
                dart_count = len(additional_reports)
                company_count = len(naver_company_reports)
                industry_count = len(naver_industry_reports)
                
                report_summary = f"DART {dart_count}ê°œ"
                if company_count > 0:
                    report_summary += f" + ì¢…ëª©ë¶„ì„ {company_count}ê°œ"
                if industry_count > 0:
                    report_summary += f" + ì‚°ì—…ë¶„ì„ {industry_count}ê°œ"
                
                # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ë³´ê³ ì„œ ê°œìˆ˜ ë° í¬ê¸° ê¸°ë°˜)
                total_reports = 1 + len(all_additional_reports)  # ë©”ì¸ + ì¶”ê°€
                total_chars = len(report_content) + sum(len(r.get('content', '')) for r in all_additional_reports)
                
                # í† í° ìˆ˜ ì¶”ì • (í•œê¸€ì€ ì•½ 1ì = 2í† í°)
                estimated_tokens = total_chars * 2
                
                # ì‹œê°„ ì¶”ì •: 1ë°±ë§Œ í† í°ë‹¹ ì•½ 1ë¶„
                estimated_minutes = max(2, int(estimated_tokens / 500000))  # ìµœì†Œ 2ë¶„
                
                update_status(f"ğŸ¤– 4ë‹¨ê³„: Gemini AI ë¶„ì„ ì¤‘...")
                update_status(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ë©”ì¸ ë³´ê³ ì„œ + {report_summary}")
                update_status(f"   ğŸ“ ì´ í…ìŠ¤íŠ¸: {total_chars:,}ì (ì•½ {estimated_tokens:,} í† í°)")
                update_status(f"   â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {estimated_minutes}~{estimated_minutes+2}ë¶„")
                update_status(f"   ğŸ”„ Gemini API í˜¸ì¶œ ì¤‘... (ëŒ€ìš©ëŸ‰ ë³´ê³ ì„œ ì²˜ë¦¬)")
                logger.info(f"Gemini AI ë¶„ì„ ì‹œì‘ (ë©”ì¸ + {report_summary}, ì˜ˆìƒ: {estimated_minutes}ë¶„, ì˜ê²¬ì œì™¸: {exclude_opinions})")
            else:
                update_status(f"ğŸ¤– 4ë‹¨ê³„: Gemini AI ë¶„ì„ ì¤‘... (ë©”ì¸ ë³´ê³ ì„œë§Œ, ì•½ 1~2ë¶„ ì†Œìš”)")
                logger.info(f"Gemini AI ë¶„ì„ ì‹œì‘ (ì˜ê²¬ì œì™¸: {exclude_opinions})")
            
            # 4-1. VectorDBì—ì„œ ì§ˆë¬¸ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (RAG)
            update_status(f"ğŸ”µ VectorDB ì‹œì‘: RAG ê²€ìƒ‰")
            update_status(f"ğŸ” VectorDBì—ì„œ ì§ˆë¬¸ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰ ì¤‘...")
            logger.info(f"VectorDB ê²€ìƒ‰ ì‹œì‘: {user_query}")
            
            try:
                # ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 20ê°œ ì²­í¬)
                relevant_chunks = self.vector_store.search_similar_reports(
                    query=user_query,
                    company_name=found_name,
                    k=20  # ìƒìœ„ 20ê°œ ì²­í¬
                )
                
                if relevant_chunks:
                    total_chunk_chars = sum(len(chunk[0].page_content) for chunk in relevant_chunks)
                    update_status(f"âœ… {len(relevant_chunks)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬ (ì´ {total_chunk_chars:,}ì)")
                    update_status(f"âšª VectorDB ì™„ë£Œ: RAG ê²€ìƒ‰")
                    logger.info(f"VectorDB ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_chunks)}ê°œ ì²­í¬, {total_chunk_chars:,}ì")
                    
                    # ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•© (ë³´ê³ ì„œëª… ëª…í™•í•˜ê²Œ í‘œê¸°)
                    context_from_chunks = "\n\n".join([
                        f"â”â”â” ë³´ê³ ì„œ: {chunk[0].metadata.get('report_name', 'Unknown')} ({chunk[0].metadata.get('report_date', '')}) â”â”â”\n"
                        f"[Reference {idx+1}/{len(relevant_chunks)} - ì²­í¬ {chunk[0].metadata.get('chunk_index', '?')}/{chunk[0].metadata.get('total_chunks', '?')}]\n"
                        f"{chunk[0].page_content}"
                        for idx, chunk in enumerate(relevant_chunks)
                    ])
                    
                    # Gemini ë¶„ì„ (ê´€ë ¨ ì²­í¬ë§Œ ì‚¬ìš©)
                    update_status(f"ğŸ”µ Gemini AI ì‹œì‘: ë¶„ì„", "gemini", "start")
                    update_status(f"ğŸ¤– Gemini AI ë¶„ì„ ì¤‘... (ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš©ë§Œ ì‚¬ìš©)")
                    update_status(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°: {len(relevant_chunks)}ê°œ ì²­í¬, {total_chunk_chars:,}ì")
                    
                    analysis = self.analyze_with_gemini_rag(
                        company_name=found_name,
                        user_query=user_query,
                        relevant_context=context_from_chunks,
                        num_chunks=len(relevant_chunks),
                        exclude_opinions=exclude_opinions
                    )
                    
                    logger.info(f"Gemini AI ë¶„ì„ ì™„ë£Œ: {len(analysis)}ì")
                    update_status(f"âœ… AI ë¶„ì„ ì™„ë£Œ!")
                    update_status(f"âšª Gemini AI ì™„ë£Œ: ë¶„ì„", "gemini", "complete")
                    
                else:
                    update_status(f"âš ï¸  ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. VectorDBì— ë°ì´í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    analysis = "VectorDBì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                    
            except Exception as e:
                logger.error(f"VectorDB ê²€ìƒ‰ ë˜ëŠ” Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
                update_status(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
                analysis = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            result['analysis'] = analysis
            result['success'] = True
            logger.info("ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ (RAG ëª¨ë“œ)")
            
            # ===== ë””ë²„ê¹…ìš© MD íŒŒì¼ ì €ì¥ (ì„ íƒì ) =====
            # í™˜ê²½ ë³€ìˆ˜ SAVE_DEBUG_REPORTS=Trueë¡œ ì„¤ì •í•˜ë©´ í™œì„±í™”
            # ì•„ë˜ ì½”ë“œ ì „ì²´ê°€ ì£¼ì„ì²˜ë¦¬ë¨
            """
            if os.getenv('SAVE_DEBUG_REPORTS', 'False').lower() == 'true':
                # ë””ë²„ê¹… ì½”ë“œ...
                main_zip_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report_ORIGINAL.zip")
            extracted_dir = os.path.join("downloads", f"{rcept_no}_extracted")
            zip_source = os.path.join("downloads", f"{rcept_no}.zip")
            
            zip_saved = False
            if os.path.exists(zip_source):
                shutil.copy2(zip_source, main_zip_dest)
                zip_saved = True
                logger.info(f"ì›ë³¸ ZIP ë³µì‚¬: {zip_source} â†’ {main_zip_dest}")
            
            # 1-2. ì›ë³¸ XML íŒŒì¼ ë³µì‚¬
            main_xml_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report_ORIGINAL.xml")
            xml_source = os.path.join(extracted_dir, f"{rcept_no}.xml")
            
            xml_saved = False
            if os.path.exists(xml_source):
                shutil.copy2(xml_source, main_xml_dest)
                xml_saved = True
                logger.info(f"ì›ë³¸ XML ë³µì‚¬: {xml_source} â†’ {main_xml_dest}")
            
            # 1-3. ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì €ì¥ (XML â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼)
            main_raw_path = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report_CONVERTED.txt")
            with open(main_raw_path, 'w', encoding='utf-8') as f:
                f.write(f"=== {found_name} - ë©”ì¸ ë³´ê³ ì„œ (XML â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼) ===\n\n")
                f.write(f"ë³´ê³ ì„œëª…: {report_name}\n")
                f.write(f"ë³´ê³ ì„œ ì½”ë“œ: {rcept_no}\n")
                f.write(f"ë¬¸ì ìˆ˜: {len(report_content):,}ì\n\n")
                f.write("=" * 80 + "\n\n")
                f.write(report_content)
            
            # 1-4. MD íŒŒì¼ ì €ì¥ (ì½ê¸° ì‰½ê²Œ ì •ë¦¬ëœ ë²„ì „)
            main_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report.md")
            with open(main_report_path, 'w', encoding='utf-8') as f:
                f.write(f"# {found_name} - ë©”ì¸ ë³´ê³ ì„œ\n\n")
                f.write(f"**ë³´ê³ ì„œëª…**: {report_name}\n\n")
                f.write(f"**ë³´ê³ ì„œ ì½”ë“œ**: {rcept_no}\n\n")
                f.write(f"**ë¬¸ì ìˆ˜**: {len(report_content):,}ì\n\n")
                f.write("---\n\n")
                f.write(report_content)
            
            files_saved = []
            if zip_saved: files_saved.append("ZIP")
            if xml_saved: files_saved.append("XML")
            files_saved.extend(["TXT", "MD"])
            
                update_status(f"   âœ… ë©”ì¸ ë³´ê³ ì„œ ì €ì¥: {' + '.join(files_saved)}")
                logger.info(f"ë©”ì¸ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {' + '.join(files_saved)} ({len(report_content):,}ì)")
                
                # 2. ì¶”ê°€ DART ë³´ê³ ì„œ ì €ì¥ (ì›ë³¸ ZIP + XML + ë³€í™˜ í›„ í…ìŠ¤íŠ¸ + MD)
            if additional_reports:
                update_status(f"   ğŸ“„ ì¶”ê°€ DART ë³´ê³ ì„œ {len(additional_reports)}ê°œ ì €ì¥ ì¤‘...")
                for idx, add_report in enumerate(additional_reports, 1):
                    content = add_report.get('content', '')
                    add_rcept_no = add_report.get('rcept_no', 'N/A')
                    add_name = add_report.get('name', 'Unknown')
                    add_date = add_report.get('date', 'N/A')
                    
                    # 2-1. ì›ë³¸ ZIP íŒŒì¼ ë³µì‚¬
                    add_zip_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}_ORIGINAL.zip")
                    add_zip_source = os.path.join("downloads", f"{add_rcept_no}.zip")
                    
                    add_zip_saved = False
                    if os.path.exists(add_zip_source):
                        shutil.copy2(add_zip_source, add_zip_dest)
                        add_zip_saved = True
                    
                    # 2-2. ì›ë³¸ XML íŒŒì¼ ë³µì‚¬
                    add_xml_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}_ORIGINAL.xml")
                    add_extracted_dir = os.path.join("downloads", f"{add_rcept_no}_extracted")
                    add_xml_source = os.path.join(add_extracted_dir, f"{add_rcept_no}.xml")
                    
                    add_xml_saved = False
                    if os.path.exists(add_xml_source):
                        shutil.copy2(add_xml_source, add_xml_dest)
                        add_xml_saved = True
                    
                    # 2-3. ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì €ì¥
                    add_converted_path = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}_CONVERTED.txt")
                    with open(add_converted_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== {found_name} - DART ì¶”ê°€ ë³´ê³ ì„œ #{idx} (XML â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼) ===\n\n")
                        f.write(f"ë³´ê³ ì„œëª…: {add_name}\n")
                        f.write(f"ë³´ê³ ì„œ ì½”ë“œ: {add_rcept_no}\n")
                        f.write(f"ë³´ê³ ì„œ ë‚ ì§œ: {add_date}\n")
                        f.write(f"ë¬¸ì ìˆ˜: {len(content):,}ì\n\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(content)
                    
                    # 2-4. MD íŒŒì¼ ì €ì¥
                    add_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}.md")
                    with open(add_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {found_name} - DART ì¶”ê°€ ë³´ê³ ì„œ #{idx}\n\n")
                        f.write(f"**ë³´ê³ ì„œëª…**: {add_name}\n\n")
                        f.write(f"**ë³´ê³ ì„œ ì½”ë“œ**: {add_rcept_no}\n\n")
                        f.write(f"**ë³´ê³ ì„œ ë‚ ì§œ**: {add_date}\n\n")
                        f.write(f"**ë¬¸ì ìˆ˜**: {len(content):,}ì\n\n")
                        f.write("---\n\n")
                        f.write(content)
                    
                    add_files_saved = []
                    if add_zip_saved: add_files_saved.append("ZIP")
                    if add_xml_saved: add_files_saved.append("XML")
                    add_files_saved.extend(["TXT", "MD"])
                    
                    logger.info(f"DART ì¶”ê°€ #{idx}: {add_name[:50]} ({' + '.join(add_files_saved)}, {len(content):,}ì)")
                
                update_status(f"   âœ… DART ë³´ê³ ì„œ {len(additional_reports)}ê°œ ì €ì¥ ì™„ë£Œ (ZIP + XML + TXT + MD)")
            
            # 3. ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ (PDF ì¶”ì¶œ í…ìŠ¤íŠ¸ + MD)
            if naver_company_reports:
                update_status(f"   ğŸ“Š ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ì €ì¥ ì¤‘...")
                for idx, company_report in enumerate(naver_company_reports, 1):
                    content = company_report.get('content', '')
                    comp_name = company_report.get('name', 'Unknown')
                    comp_date = company_report.get('date', 'N/A')
                    comp_url = company_report.get('url', 'N/A')
                    
                    # 3-1. PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì €ì¥
                    company_converted_path = os.path.join(output_dir, f"{timestamp}_{found_name}_company_{idx}_CONVERTED.txt")
                    with open(company_converted_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== {found_name} - ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ #{idx} (PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼) ===\n\n")
                        f.write(f"ë¦¬í¬íŠ¸ëª…: {comp_name}\n")
                        f.write(f"ë°œí–‰ì¼: {comp_date}\n")
                        f.write(f"URL: {comp_url}\n")
                        f.write(f"ë¬¸ì ìˆ˜: {len(content):,}ì\n\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(content)
                    
                    # 3-2. MD íŒŒì¼ ì €ì¥
                    company_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_company_{idx}.md")
                    with open(company_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {found_name} - ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ #{idx}\n\n")
                        f.write(f"**ë¦¬í¬íŠ¸ëª…**: {comp_name}\n\n")
                        f.write(f"**ë°œí–‰ì¼**: {comp_date}\n\n")
                        f.write(f"**URL**: {comp_url}\n\n")
                        f.write(f"**ë¬¸ì ìˆ˜**: {len(content):,}ì\n\n")
                        f.write("---\n\n")
                        f.write(content)
                    
                    logger.info(f"ì¢…ëª©ë¶„ì„ #{idx}: {comp_name[:50]} (TXT + MD, {len(content):,}ì)")
                
                update_status(f"   âœ… ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ì €ì¥ ì™„ë£Œ (TXT + MD)")
            
            # 4. ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ (PDF ì¶”ì¶œ í…ìŠ¤íŠ¸ + MD)
            if naver_industry_reports:
                update_status(f"   ğŸ­ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ì €ì¥ ì¤‘...")
                for idx, industry_report in enumerate(naver_industry_reports, 1):
                    content = industry_report.get('content', '')
                    ind_name = industry_report.get('name', 'Unknown')
                    ind_date = industry_report.get('date', 'N/A')
                    ind_url = industry_report.get('url', 'N/A')
                    
                    # 4-1. PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì €ì¥
                    industry_converted_path = os.path.join(output_dir, f"{timestamp}_{found_name}_industry_{idx}_CONVERTED.txt")
                    with open(industry_converted_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== {found_name} - ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ #{idx} (PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼) ===\n\n")
                        f.write(f"ë¦¬í¬íŠ¸ëª…: {ind_name}\n")
                        f.write(f"ë°œí–‰ì¼: {ind_date}\n")
                        f.write(f"URL: {ind_url}\n")
                        f.write(f"ë¬¸ì ìˆ˜: {len(content):,}ì\n\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(content)
                    
                    # 4-2. MD íŒŒì¼ ì €ì¥
                    industry_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_industry_{idx}.md")
                    with open(industry_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {found_name} - ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ #{idx}\n\n")
                        f.write(f"**ë¦¬í¬íŠ¸ëª…**: {ind_name}\n\n")
                        f.write(f"**ë°œí–‰ì¼**: {ind_date}\n\n")
                        f.write(f"**URL**: {ind_url}\n\n")
                        f.write(f"**ë¬¸ì ìˆ˜**: {len(content):,}ì\n\n")
                        f.write("---\n\n")
                        f.write(content)
                    
                    logger.info(f"ì‚°ì—…ë¶„ì„ #{idx}: {ind_name[:50]} (TXT + MD, {len(content):,}ì)")
                
                update_status(f"   âœ… ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ì €ì¥ ì™„ë£Œ (TXT + MD)")
            
            # 5. ì „ì²´ í†µí•© ìš”ì•½ íŒŒì¼ ì €ì¥
            summary_path = os.path.join(output_dir, f"{timestamp}_{found_name}_SUMMARY.md")
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(f"# {found_name} - ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½\n\n")
                f.write(f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"**ì‚¬ìš©ì ì§ˆë¬¸**: {user_query}\n\n")
                f.write(f"---\n\n")
                
                # í†µê³„
                f.write(f"## ğŸ“Š ìˆ˜ì§‘ í†µê³„\n\n")
                f.write(f"| êµ¬ë¶„ | ê°œìˆ˜ | ì´ ë¬¸ì ìˆ˜ |\n")
                f.write(f"|------|------|------------|\n")
                f.write(f"| ë©”ì¸ DART ë³´ê³ ì„œ | 1ê°œ | {len(report_content):,}ì |\n")
                f.write(f"| ì¶”ê°€ DART ë³´ê³ ì„œ | {len(additional_reports)}ê°œ | {sum(len(r.get('content', '')) for r in additional_reports):,}ì |\n")
                f.write(f"| ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ | {len(naver_company_reports)}ê°œ | {sum(len(r.get('content', '')) for r in naver_company_reports):,}ì |\n")
                f.write(f"| ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ | {len(naver_industry_reports)}ê°œ | {sum(len(r.get('content', '')) for r in naver_industry_reports):,}ì |\n")
                f.write(f"| **ì „ì²´ í•©ê³„** | **{1 + len(all_additional_reports)}ê°œ** | **{total_chars:,}ì** |\n\n")
                
                f.write(f"**ì˜ˆìƒ í† í° ìˆ˜**: {estimated_tokens:,} í† í°\n\n")
                f.write(f"---\n\n")
                
                # ë©”ì¸ ë³´ê³ ì„œ ì •ë³´
                f.write(f"## ğŸ“„ ë©”ì¸ DART ë³´ê³ ì„œ\n\n")
                f.write(f"- **ZIP ì›ë³¸**: `{timestamp}_{found_name}_main_report_ORIGINAL.zip`\n")
                f.write(f"- **XML ì›ë³¸**: `{timestamp}_{found_name}_main_report_ORIGINAL.xml`\n")
                f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_main_report_CONVERTED.txt`\n")
                f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_main_report.md`\n")
                f.write(f"- **ë³´ê³ ì„œëª…**: {report_name}\n")
                f.write(f"- **ë³´ê³ ì„œ ì½”ë“œ**: {rcept_no}\n")
                f.write(f"- **ë¬¸ì ìˆ˜**: {len(report_content):,}ì\n\n")
                
                # ì¶”ê°€ DART ë³´ê³ ì„œ ëª©ë¡
                if additional_reports:
                    f.write(f"## ğŸ“š ì¶”ê°€ DART ë³´ê³ ì„œ ({len(additional_reports)}ê°œ)\n\n")
                    for idx, rep in enumerate(additional_reports, 1):
                        f.write(f"### {idx}. {rep.get('name', 'Unknown')}\n")
                        f.write(f"- **ZIP ì›ë³¸**: `{timestamp}_{found_name}_dart_{idx}_ORIGINAL.zip`\n")
                        f.write(f"- **XML ì›ë³¸**: `{timestamp}_{found_name}_dart_{idx}_ORIGINAL.xml`\n")
                        f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_dart_{idx}_CONVERTED.txt`\n")
                        f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_dart_{idx}.md`\n")
                        f.write(f"- **ë³´ê³ ì„œ ì½”ë“œ**: {rep.get('rcept_no', 'N/A')}\n")
                        f.write(f"- **ë‚ ì§œ**: {rep.get('date', 'N/A')}\n")
                        f.write(f"- **ë¬¸ì ìˆ˜**: {len(rep.get('content', '')):,}ì\n\n")
                
                # ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ëª©ë¡
                if naver_company_reports:
                    f.write(f"## ğŸ“ˆ ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ({len(naver_company_reports)}ê°œ)\n\n")
                    for idx, rep in enumerate(naver_company_reports, 1):
                        f.write(f"### {idx}. {rep.get('name', 'Unknown')}\n")
                        f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_company_{idx}_CONVERTED.txt`\n")
                        f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_company_{idx}.md`\n")
                        f.write(f"- **ë°œí–‰ì¼**: {rep.get('date', 'N/A')}\n")
                        f.write(f"- **URL**: {rep.get('url', 'N/A')}\n")
                        f.write(f"- **ë¬¸ì ìˆ˜**: {len(rep.get('content', '')):,}ì\n\n")
                
                # ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ëª©ë¡
                if naver_industry_reports:
                    f.write(f"## ğŸ­ ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ({len(naver_industry_reports)}ê°œ)\n\n")
                    for idx, rep in enumerate(naver_industry_reports, 1):
                        f.write(f"### {idx}. {rep.get('name', 'Unknown')}\n")
                        f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_industry_{idx}_CONVERTED.txt`\n")
                        f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_industry_{idx}.md`\n")
                        f.write(f"- **ë°œí–‰ì¼**: {rep.get('date', 'N/A')}\n")
                        f.write(f"- **URL**: {rep.get('url', 'N/A')}\n")
                        f.write(f"- **ë¬¸ì ìˆ˜**: {len(rep.get('content', '')):,}ì\n\n")
                
                # ì €ì¥ ìœ„ì¹˜
                f.write(f"---\n\n")
                f.write(f"## ğŸ“ ì €ì¥ ìœ„ì¹˜\n\n")
                f.write(f"ëª¨ë“  íŒŒì¼ì€ `{os.path.abspath(output_dir)}/` í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            
            update_status(f"âœ… ëª¨ë“  ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ!")
            update_status(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(output_dir)}/")
            update_status(f"   ğŸ“„ ìš”ì•½ íŒŒì¼: {summary_path}")
            update_status(f"   ì´ {1 + len(all_additional_reports)}ê°œ íŒŒì¼ ({total_chars:,}ì)")
            
            logger.info(f"ëª¨ë“  ë¦¬í¬íŠ¸ MD íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_dir}/")
            logger.info(f"  - ë©”ì¸: {len(report_content):,}ì")
            logger.info(f"  - DART ì¶”ê°€: {len(additional_reports)}ê°œ")
            logger.info(f"  - ì¢…ëª©ë¶„ì„: {len(naver_company_reports)}ê°œ")
            logger.info(f"  - ì‚°ì—…ë¶„ì„: {len(naver_industry_reports)}ê°œ")
            
                # ë””ë²„ê¹… ì½”ë“œ ë
                pass
            """
            # ë””ë²„ê¹…ìš© MD íŒŒì¼ ì €ì¥ ì½”ë“œ ë
            
            # ë¶„ì„ ì™„ë£Œ í›„ ì˜¤ë˜ëœ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ (ìµœì‹  5ê°œë§Œ ìœ ì§€)
            update_status(f"ğŸ§¹ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
            try:
                self.cleanup_downloads(keep_latest=5)
            except Exception as cleanup_error:
                logger.warning(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {cleanup_error}")
            
            return result
            
        except Exception as e:
            error_msg = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg, exc_info=True)
            result['error'] = error_msg
            return result

