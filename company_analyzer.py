#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íšŒì‚¬ ë³´ê³ ì„œ ë¶„ì„ê¸°
ì˜¤í”ˆë‹¤íŠ¸ APIë¡œ ë³´ê³ ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  Gemini APIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import requests
import zipfile
import xml.etree.ElementTree as ET
import os
import re
import google.generativeai as genai
from datetime import datetime, timedelta
import fitz  # PyMuPDF for PDF processing
from vector_store import VectorStore
from naver_finance import NaverFinanceCrawler
from config import config
import shutil
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
import time
import logging

# Logger ì„¤ì •
logger = logging.getLogger(__name__)

class CompanyAnalyzer:
    """íšŒì‚¬ ë³´ê³ ì„œ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, dart_api_key, llm_orchestrator):
        """
        ì´ˆê¸°í™”
        
        Args:
            dart_api_key: ì˜¤í”ˆë‹¤íŠ¸ API í‚¤
            llm_orchestrator: LLMOrchestrator ì¸ìŠ¤í„´ìŠ¤ (Dependency Injection)
        """
        self.dart_api_key = dart_api_key
        self.llm_orchestrator = llm_orchestrator
        self.base_url = config.DART_BASE_URL
        
        # VectorStoreëŠ” lazy initialization (í•„ìš”í•  ë•Œ ìƒì„±)
        self._vector_store = None
        
        # ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬ (lazy initialization)
        self._naver_crawler = None
        
        print("âœ… CompanyAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
    
    @property
    def vector_store(self):
        """VectorStore lazy initialization - ì²˜ìŒ ì ‘ê·¼í•  ë•Œë§Œ ìƒì„±"""
        if self._vector_store is None:
            print("ğŸ“¦ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            self._vector_store = VectorStore()
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        return self._vector_store
    
    @property
    def naver_crawler(self):
        """NaverFinanceCrawler lazy initialization - ì²˜ìŒ ì ‘ê·¼í•  ë•Œë§Œ ìƒì„±"""
        if self._naver_crawler is None:
            print("ğŸ“Š ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
            self._naver_crawler = NaverFinanceCrawler(llm_orchestrator=self.llm_orchestrator)
            print("âœ… ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        return self._naver_crawler
        
    def _get_company_name_variations(self, company_name):
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ íšŒì‚¬ëª…ì˜ ë‹¤ì–‘í•œ í‘œê¸° ê°€ì ¸ì˜¤ê¸°
        
        Args:
            company_name: ì…ë ¥ëœ íšŒì‚¬ëª…
            
        Returns:
            list: ê°€ëŠ¥í•œ íšŒì‚¬ëª… í‘œê¸° ë¦¬ìŠ¤íŠ¸
        """
        try:
            prompt = f"""
í•œêµ­ ìƒì¥íšŒì‚¬ "{company_name}"ì˜ ê³µì‹ ëª…ì¹­ê³¼ ê°€ëŠ¥í•œ ëª¨ë“  í‘œê¸° ë°©ë²•ì„ ë‚˜ì—´í•´ì£¼ì„¸ìš”. ê°€ëŠ¥í•œ í‘œê¸° ë°©ë²• ë‚´ì— ê³µë°±ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì˜ˆì‹œ:
- "KT" ì…ë ¥ â†’ ["KT", "ì¼€ì´í‹°", "ì£¼ì‹íšŒì‚¬ì¼€ì´í‹°", "ì£¼ì‹íšŒì‚¬KT"]
- "ì‚¼ì„±ì „ì" ì…ë ¥ â†’ ["ì‚¼ì„±ì „ì", "ì‚¼ì„±ì „ìì£¼ì‹íšŒì‚¬"]
- "SKí…”ë ˆì½¤" ì…ë ¥ â†’ ["SKí…”ë ˆì½¤", "ì—ìŠ¤ì¼€ì´í…”ë ˆì½¤"]

"{company_name}"ì— ëŒ€í•´ ê°€ëŠ¥í•œ ëª¨ë“  í‘œê¸°ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì˜¤ì§ íšŒì‚¬ëª… ë¦¬ìŠ¤íŠ¸ë§Œ ì‘ì„±í•˜ê³ , ì„¤ëª…ì€ ë„£ì§€ ë§ˆì„¸ìš”.
í˜•ì‹: í‘œê¸°1, í‘œê¸°2, í‘œê¸°3
"""
            
            variations_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='name_variation'
            ).strip()
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            variations = [v.strip() for v in variations_text.split(',')]
            
            # ì›ë˜ ì…ë ¥ê°’ë„ í¬í•¨
            if company_name not in variations:
                variations.insert(0, company_name)
            
            # ì‚¬ìš©ëœ LLM í‘œì‹œ
            used_llm = self.llm_orchestrator.select_provider('name_variation').get_name().upper()
            logger.info(f"   âœ… {used_llm} ì¶”ì²œ ê²€ìƒ‰ì–´: {variations}")
            return variations
            
        except Exception as e:
            logger.warning(f"   âš ï¸  LLM ê²€ìƒ‰ì–´ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë³€í˜•ë§Œ ë°˜í™˜
            return [company_name]
    
    def get_corp_code(self, company_name):
        """
        íšŒì‚¬ëª…ìœ¼ë¡œ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ
        
        Args:
            company_name: íšŒì‚¬ëª…
            
        Returns:
            tuple: (corp_code, corp_name, stock_code)
        """
        logger.info(f"ğŸ” '{company_name}' íšŒì‚¬ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ ì¤‘...")
        
        # 1. LLMìœ¼ë¡œ íšŒì‚¬ëª… ë³€í˜• ê°€ì ¸ì˜¤ê¸° (í•­ìƒ ì‹¤í–‰)
        logger.info(f"ğŸ¤– LLMì—ê²Œ '{company_name}'ì˜ ë‹¤ì–‘í•œ í‘œê¸° í™•ì¸ ìš”ì²­ ì¤‘...")
        search_variations = self._get_company_name_variations(company_name)
        
        try:
            # ê¸°ì—… ê³ ìœ ë²ˆí˜¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            corp_code_url = f'{self.base_url}/corpCode.xml?crtfc_key={self.dart_api_key}'
            response = requests.get(corp_code_url, timeout=30)
            response.raise_for_status()
            
            # ZIP íŒŒì¼ ì €ì¥ ë° ì••ì¶• í•´ì œ
            with open('corp_code.zip', 'wb') as f:
                f.write(response.content)
            
            with zipfile.ZipFile('corp_code.zip', 'r') as zip_ref:
                zip_ref.extractall()
            
            # XML íŒŒì¼ íŒŒì‹±
            tree = ET.parse('CORPCODE.xml')
            root = tree.getroot()
            
            # íšŒì‚¬ ê²€ìƒ‰ - LLM ì¶”ì²œ ê²€ìƒ‰ì–´ë¡œ ë‹¤ë‹¨ê³„ ê²€ìƒ‰
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: ì´ {len(search_variations)}ê°œ í‘œê¸°ë¡œ ê²€ìƒ‰")
            exact_matches = []
            
            # ëª¨ë“  ë³€í˜•ì— ëŒ€í•´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” íšŒì‚¬ ê²€ìƒ‰ (ëª¨ë“  ë™ëª… íšŒì‚¬ ìˆ˜ì§‘)
            for variation in search_variations:
                logger.info(f"   ğŸ“Œ ê²€ìƒ‰ì–´: '{variation}'")
                
                found_count = 0
                for child in root:
                    corp_name_elem = child.find('corp_name')
                    corp_code_elem = child.find('corp_code')
                    stock_code_elem = child.find('stock_code')
                    
                    if corp_name_elem is not None and corp_code_elem is not None:
                        name = corp_name_elem.text
                        code = corp_code_elem.text
                        stock = stock_code_elem.text if stock_code_elem is not None and stock_code_elem.text else ''
                        
                        # ì •í™•íˆ ì¼ì¹˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                        if name.upper() == variation.upper():
                            # ì¤‘ë³µ ë°©ì§€
                            if not any(c[0] == code for c in exact_matches):
                                exact_matches.append((code, name, stock))
                                stock_display = stock if stock else 'ì—†ìŒ'
                                logger.info(f"      âœ… ë°œê²¬: {name} (ê³ ìœ ë²ˆí˜¸: {code}, ì¢…ëª©: {stock_display})")
                                found_count += 1
                
                # ì´ ê²€ìƒ‰ì–´ë¡œ ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ ê²€ìƒ‰ì–´ëŠ” ì‹œë„ ì•ˆ í•¨
                if found_count > 0:
                    break
            
            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” íšŒì‚¬ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
            if exact_matches:
                logger.info(f"\n   âœ… ì´ {len(exact_matches)}ê°œì˜ ë™ëª… íšŒì‚¬ ë°œê²¬")
                
                # ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ê²ƒë§Œ í•„í„°ë§
                with_stock = [c for c in exact_matches if c[2] and c[2].strip()]
                
                if with_stock:
                    # ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ê²ƒ ì¤‘ ì²« ë²ˆì§¸
                    result = with_stock[0]
                    print(f"   âœ… ìµœì¢… ì„ íƒ (ì¢…ëª©ì½”ë“œ ìˆìŒ): {result[1]} (ê³ ìœ ë²ˆí˜¸: {result[0]}, ì¢…ëª©: {result[2]})")
                else:
                    # ì¢…ëª©ì½”ë“œê°€ ì—†ì–´ë„ ì²« ë²ˆì§¸ ì„ íƒ
                    result = exact_matches[0]
                    print(f"   âš ï¸  ìµœì¢… ì„ íƒ (ì¢…ëª©ì½”ë“œ ì—†ìŒ): {result[1]} (ê³ ìœ ë²ˆí˜¸: {result[0]})")
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for file in ['corp_code.zip', 'CORPCODE.xml']:
                    if os.path.exists(file):
                        os.remove(file)
                
                return result
            
            # 2ë‹¨ê³„: ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ í¬í•¨ë˜ëŠ” ê²½ìš° ê²€ìƒ‰
            print(f"2ë‹¨ê³„: ë³€í˜• ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•˜ëŠ” íšŒì‚¬ ê²€ìƒ‰ ì¤‘...")
            contains_matches = []
            
            # ëª¨ë“  ë³€í˜•ì— ëŒ€í•´ í¬í•¨ ê²€ìƒ‰
            for variation in search_variations:
                for child in root:
                    corp_name_elem = child.find('corp_name')
                    corp_code_elem = child.find('corp_code')
                    stock_code_elem = child.find('stock_code')
                    
                    if corp_name_elem is not None and corp_code_elem is not None:
                        name = corp_name_elem.text
                        code = corp_code_elem.text
                        stock = stock_code_elem.text if stock_code_elem is not None else ''
                        
                        # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ê²½ìš°
                        if variation.upper() in name.upper():
                            # ì¤‘ë³µ ë°©ì§€
                            if not any(c[0] == code for c in contains_matches):
                                contains_matches.append((code, name, stock))
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for file in ['corp_code.zip', 'CORPCODE.xml']:
                if os.path.exists(file):
                    os.remove(file)
            
            if contains_matches:
                print(f"âš ï¸  ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” '{company_name}'ëŠ” ì—†ìŠµë‹ˆë‹¤.")
                print(f"   ìœ ì‚¬í•œ íšŒì‚¬ {len(contains_matches)}ê°œ ë°œê²¬:")
                
                for i, c in enumerate(contains_matches[:5], 1):
                    print(f"   [{i}] {c[1]} (ì¢…ëª©ì½”ë“œ: {c[2]})")
                
                # ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ê²ƒ ìš°ì„ 
                with_stock = [c for c in contains_matches if c[2] and c[2].strip()]
                if with_stock:
                    result = with_stock[0]
                else:
                    result = contains_matches[0]
                
                print(f"   â†’ ì²« ë²ˆì§¸ í›„ë³´ ì„ íƒ: {result[1]}")
                return result
            else:
                print(f"âŒ '{company_name}' íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_time_range(self, user_query):
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            int: ê²€ìƒ‰í•  ì—°ìˆ˜ (ê¸°ë³¸ê°’: 3ë…„)
        """
        try:
            print(f"ğŸ“… Geminiì—ê²Œ ì‹œê°„ ë²”ìœ„ ë¶„ì„ ìš”ì²­ ì¤‘...")
            
            prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ì–´ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì‹œê°„ ë²”ìœ„ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸:**
{user_query}

**ì¶œë ¥ í˜•ì‹:**
ì§ˆë¬¸ì—ì„œ ì‹œê°„ ë²”ìœ„ë¥¼ ì°¾ì•„ì„œ ì—°ìˆ˜(ë…„)ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:

{{
  "years": ìˆ«ì,
  "reason": "ì¶”ì¶œ ì´ìœ "
}}

**ì˜ˆì‹œ:**
- "ìµœê·¼ 5ë…„ê°„ ì‹¤ì ì€?" â†’ {{"years": 5, "reason": "ëª…ì‹œì ìœ¼ë¡œ 5ë…„ ì–¸ê¸‰"}}
- "ê³¼ê±° 10ë…„ ë™ì•ˆ" â†’ {{"years": 10, "reason": "ëª…ì‹œì ìœ¼ë¡œ 10ë…„ ì–¸ê¸‰"}}
- "ì§€ë‚œ 3ë…„" â†’ {{"years": 3, "reason": "ëª…ì‹œì ìœ¼ë¡œ 3ë…„ ì–¸ê¸‰"}}
- "2020ë…„ë¶€í„° í˜„ì¬ê¹Œì§€" â†’ {{"years": 5, "reason": "2020ë…„ë¶€í„° í˜„ì¬(2025)ê¹Œì§€"}}
- "ìµœê·¼ ì¬ë¬´ ìƒíƒœëŠ”?" â†’ {{"years": 3, "reason": "íŠ¹ì • ê¸°ê°„ ì—†ìŒ, ê¸°ë³¸ê°’ 3ë…„"}}
- "ì—­ì‚¬ì ìœ¼ë¡œ" â†’ {{"years": 10, "reason": "ì—­ì‚¬ì  = ì¥ê¸°ê°„ìœ¼ë¡œ í•´ì„"}}

**ì°¸ê³ :**
- ëª…ì‹œì ì¸ ê¸°ê°„ì´ ì—†ìœ¼ë©´ 3ë…„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
- "ì¥ê¸°", "ì—­ì‚¬ì ", "ì „ì²´" ë“±ì˜ í‘œí˜„ì€ 10ë…„ìœ¼ë¡œ í•´ì„
- ìµœëŒ€ 10ë…„ê¹Œì§€ë§Œ ê²€ìƒ‰ ê°€ëŠ¥
"""
            
            response_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            
            # JSON íŒŒì‹±
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_text = json_match.group(0)
                result = json.loads(json_text)
                
                years = result.get('years', 3)
                reason = result.get('reason', '')
                
                # ìµœëŒ€ 10ë…„ìœ¼ë¡œ ì œí•œ
                years = min(years, 10)
                years = max(years, 1)  # ìµœì†Œ 1ë…„
                
                print(f"   âœ… ì¶”ì¶œëœ ê¸°ê°„: {years}ë…„")
                print(f"   ğŸ’¡ ì´ìœ : {reason}")
                
                return years
            else:
                print(f"   âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ 3ë…„ ì‚¬ìš©")
                return 3
                
        except Exception as e:
            print(f"   âš ï¸  ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 3  # ê¸°ë³¸ê°’
    
    def _recommend_report_types(self, user_query, years=3):
        """
        Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ ë³´ê³ ì„œ íƒ€ì… ì¶”ì²œ
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            years: ê²€ìƒ‰ ì—°ìˆ˜ (ì¥ê¸° ì¶”ì„¸ ë¶„ì„ ì‹œ ì°¸ê³ )
            
        Returns:
            list: ì¶”ì²œëœ ë³´ê³ ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ¤– Geminiì—ê²Œ ì ì ˆí•œ ë³´ê³ ì„œ íƒ€ì… ì¶”ì²œ ìš”ì²­ ì¤‘...")
            
            # ì¥ê¸° ì¶”ì„¸ ë¶„ì„ ì—¬ë¶€ íŒë‹¨
            long_term_hint = ""
            if years >= 5:
                long_term_hint = f"""

**âš ï¸ ì¤‘ìš”: ì¥ê¸° ì¶”ì„¸ ë¶„ì„ (ê²€ìƒ‰ ê¸°ê°„: {years}ë…„)**
ì‚¬ìš©ìê°€ {years}ë…„ì´ë¼ëŠ” ê¸´ ê¸°ê°„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
í•œ ë³´ê³ ì„œì—ëŠ” 1-2ë…„ì¹˜ ë°ì´í„°ë§Œ í¬í•¨ë˜ë¯€ë¡œ, **ì—¬ëŸ¬ í•´ì˜ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì¶”ì²œ**í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ:
- "ê³¼ê±° 5ë…„ê°„ ë§¤ì¶œ ì¶”ì´" â†’ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì¶”ì²œí•˜ê³ , ì¶”ê°€ë¡œ ë§¤ë…„ì˜ ì‚¬ì—…ë³´ê³ ì„œê°€ í•„ìš”í•¨ì„ ëª…ì‹œ
- "ì§€ë‚œ 10ë…„ê°„ íˆ¬ì ë‚´ì—­" â†’ ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ + ë§¤ë…„ì˜ ì‚¬ì—…ë³´ê³ ì„œ

ì¶”ì²œ ì‹œ **"ì‚¬ì—…ë³´ê³ ì„œ"ë¥¼ ë°˜ë“œì‹œ í¬í•¨**í•˜ê³ , reasonì— "ì—¬ëŸ¬ í•´ì˜ ì‚¬ì—…ë³´ê³ ì„œ í•„ìš”"ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""
            
            prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ê¸ˆìœµê°ë…ì› ì „ìê³µì‹œì‹œìŠ¤í…œ(DART) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ê³µì‹œë³´ê³ ì„œ íƒ€ì…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ì‚¬ìš© ê°€ëŠ¥í•œ ë³´ê³ ì„œ íƒ€ì…:**
1. ì‚¬ì—…ë³´ê³ ì„œ - íšŒì‚¬ì˜ ì—°ê°„ ì‚¬ì—… ë‚´ìš©, ì¬ë¬´ì œí‘œ, ê²½ì˜ ì „ë°˜ì— ëŒ€í•œ ì¢…í•© ë³´ê³ ì„œ (ë§¤ë…„ ë°œí–‰)
2. ë°˜ê¸°ë³´ê³ ì„œ - ë°˜ê¸°(6ê°œì›”) ë‹¨ìœ„ ì¬ë¬´ ë° ì‚¬ì—… í˜„í™©
3. ë¶„ê¸°ë³´ê³ ì„œ - ë¶„ê¸°(3ê°œì›”) ë‹¨ìœ„ ì¬ë¬´ ë° ì‚¬ì—… í˜„í™©
4. ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ - ì¤‘ìš”í•œ ê²½ì˜ì‚¬í•­ ë³€ë™(í•©ë³‘, ë¶„í• , ìì‚°ì–‘ìˆ˜ë„, ì˜ì—…ì–‘ìˆ˜ë„ ë“±)
5. ê¸°íƒ€ê²½ì˜ì‚¬í•­(ììœ¨ê³µì‹œ) - íšŒì‚¬ê°€ ììœ¨ì ìœ¼ë¡œ ê³µì‹œí•˜ëŠ” ê²½ì˜ ì •ë³´
6. ì¡°íšŒê³µì‹œìš”êµ¬ - ê±°ë˜ì†Œì˜ ìš”êµ¬ë¡œ ê³µì‹œí•˜ëŠ” ì‚¬í•­
7. ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³  - ì£¼ì£¼ì´íšŒ ê°œìµœ ì•ˆë‚´ ë° ì•ˆê±´
8. ì¦ê¶Œë°œí–‰ì‹¤ì (ììœ¨ê³µì‹œ) - ì¦ê¶Œ ë°œí–‰ ë‚´ì—­
9. ìˆ˜ì‹œê³µì‹œ - ì¤‘ìš”í•œ ì‚¬ê±´ ë°œìƒ ì‹œ ì¦‰ì‹œ ê³µì‹œ
10. ê³µì •ê³µì‹œ - íˆ¬ììì—ê²Œ ê³µì •í•˜ê²Œ ì œê³µí•˜ëŠ” ì •ë³´
{long_term_hint}

**ì‚¬ìš©ì ì§ˆë¬¸:**
{user_query}

**ì¶œë ¥ í˜•ì‹:**
ì§ˆë¬¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë³´ê³ ì„œ íƒ€ì…ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ì²œí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:

{{
  "recommended_types": ["ë³´ê³ ì„œíƒ€ì…1", "ë³´ê³ ì„œíƒ€ì…2"],
  "reason": "ì¶”ì²œ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ",
  "need_historical_reports": true/false
}}

- need_historical_reports: ì¥ê¸° ì¶”ì„¸ ë¶„ì„ì„ ìœ„í•´ ì—¬ëŸ¬ í•´ì˜ ì‚¬ì—…ë³´ê³ ì„œê°€ í•„ìš”í•œ ê²½ìš° true

ì˜ˆì‹œ:
- ì§ˆë¬¸: "ìµœê·¼ ì¬ë¬´ ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”?" 
  â†’ {{"recommended_types": ["ë°˜ê¸°ë³´ê³ ì„œ", "ë¶„ê¸°ë³´ê³ ì„œ", "ì‚¬ì—…ë³´ê³ ì„œ"], "reason": "ì¬ë¬´ ìƒíƒœ ë¶„ì„ì—ëŠ” ì •ê¸° ì¬ë¬´ì œí‘œê°€ í¬í•¨ëœ ë³´ê³ ì„œê°€ ì í•©í•©ë‹ˆë‹¤.", "need_historical_reports": false}}

- ì§ˆë¬¸: "ê³¼ê±° 5ë…„ê°„ ë§¤ì¶œ ì¶”ì´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
  â†’ {{"recommended_types": ["ì‚¬ì—…ë³´ê³ ì„œ", "ë°˜ê¸°ë³´ê³ ì„œ"], "reason": "5ë…„ ì¶”ì„¸ ë¶„ì„ì„ ìœ„í•´ ì—¬ëŸ¬ í•´ì˜ ì‚¬ì—…ë³´ê³ ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.", "need_historical_reports": true}}

- ì§ˆë¬¸: "ìµœê·¼ íˆ¬ìë‚˜ ì¸ìˆ˜í•©ë³‘ì´ ìˆì—ˆë‚˜ìš”?"
  â†’ {{"recommended_types": ["ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ", "ê¸°íƒ€ê²½ì˜ì‚¬í•­(ììœ¨ê³µì‹œ)", "ìˆ˜ì‹œê³µì‹œ"], "reason": "íˆ¬ìì™€ M&A ì •ë³´ëŠ” ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œì™€ ììœ¨ê³µì‹œì— ì£¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤.", "need_historical_reports": false}}
"""
            
            response_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            
            # JSON íŒŒì‹±
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` í˜•ì‹ ì²˜ë¦¬)
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_text = json_match.group(0)
                recommendation = json.loads(json_text)
                
                recommended_types = recommendation.get('recommended_types', [])
                reason = recommendation.get('reason', '')
                need_historical = recommendation.get('need_historical_reports', False)
                
                print(f"   âœ… ì¶”ì²œëœ ë³´ê³ ì„œ íƒ€ì…: {recommended_types}")
                print(f"   ğŸ’¡ ì´ìœ : {reason}")
                if need_historical:
                    print(f"   ğŸ“š ì—¬ëŸ¬ í•´ì˜ ë³´ê³ ì„œ í•„ìš”: Yes")
                
                # íŠœí”Œë¡œ ë°˜í™˜ (íƒ€ì… ë¦¬ìŠ¤íŠ¸, ì—°ë„ë³„ ë³´ê³ ì„œ í•„ìš” ì—¬ë¶€)
                return recommended_types, need_historical
            else:
                print(f"   âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ë³´ê³ ì„œ íƒ€ì… ì‚¬ìš©")
                return ['ë°˜ê¸°ë³´ê³ ì„œ', 'ì‚¬ì—…ë³´ê³ ì„œ'], False
                
        except Exception as e:
            print(f"   âš ï¸  ë³´ê³ ì„œ íƒ€ì… ì¶”ì²œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ['ë°˜ê¸°ë³´ê³ ì„œ', 'ì‚¬ì—…ë³´ê³ ì„œ'], False
    
    def get_reports(self, corp_code, report_types=None, user_query=None, years=None):
        """
        íŠ¹ì • íšŒì‚¬ì˜ ë³´ê³ ì„œ ëª©ë¡ ì¡°íšŒ
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            report_types: ì¡°íšŒí•  ë³´ê³ ì„œ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ user_query ê¸°ë°˜ ìë™ ì¶”ì²œ)
            user_query: ì‚¬ìš©ì ì§ˆë¬¸ (report_typesê°€ Noneì¼ ë•Œ ì‚¬ìš©)
            years: ê²€ìƒ‰í•  ì—°ìˆ˜ (Noneì´ë©´ user_queryì—ì„œ ìë™ ì¶”ì¶œ)
            
        Returns:
            list: ë³´ê³ ì„œ ëª©ë¡
        """
        # ë³´ê³ ì„œ íƒ€ì…ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ Geminiì—ê²Œ ì¶”ì²œë°›ê¸°
        if report_types is None and user_query:
            print(f"ğŸ“‹ ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ë³´ê³ ì„œ íƒ€ì… ìë™ ì„ íƒ ì¤‘...")
            report_types, _ = self._recommend_report_types(user_query, years if years else 3)
        elif report_types is None:
            # ê¸°ë³¸ê°’
            report_types = ['ë°˜ê¸°ë³´ê³ ì„œ', 'ì‚¬ì—…ë³´ê³ ì„œ']
        
        # ì‹œê°„ ë²”ìœ„ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ user_queryì—ì„œ ì¶”ì¶œ
        if years is None and user_query:
            years = self._extract_time_range(user_query)
        elif years is None:
            years = 3  # ê¸°ë³¸ê°’
        
        print(f"ğŸ“‹ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘... (íƒ€ì…: {', '.join(report_types)}, ê¸°ê°„: ìµœê·¼ {years}ë…„)")
        
        try:
            # ì§€ì •ëœ ê¸°ê°„ ê²€ìƒ‰
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')
            
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code,
                'bgn_de': start_date,
                'end_de': end_date,
                'page_no': 1,
                'page_count': 100
            }
            
            response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                all_reports = data.get('list', [])
                
                # ì›í•˜ëŠ” ë³´ê³ ì„œ ìœ í˜•ë§Œ í•„í„°ë§
                filtered_reports = []
                for report in all_reports:
                    report_name = report.get('report_nm', '')
                    for report_type in report_types:
                        if report_type in report_name:
                            filtered_reports.append(report)
                            break
                
                print(f"âœ… ì´ {len(filtered_reports)}ê°œì˜ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                if filtered_reports:
                    print(f"   ì˜ˆì‹œ: {filtered_reports[0].get('report_nm')} ({filtered_reports[0].get('rcept_dt')})")
                return filtered_reports
            else:
                print(f"âŒ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {data.get('message')}")
                return []
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return []
    
    def download_report(self, rcept_no, save_path=None, company_name=None, report_name=None, report_date=None):
        """
        ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ë° ë‚´ìš© ì¶”ì¶œ (ë²¡í„°DB ìºì‹œ ìš°ì„ )
        
        Args:
            rcept_no: ì ‘ìˆ˜ë²ˆí˜¸
            save_path: ì €ì¥ ê²½ë¡œ (ì§€ì •í•˜ë©´ íŒŒì¼ë¡œ ì €ì¥)
            company_name: íšŒì‚¬ëª… (ë²¡í„°DB ì €ì¥ìš©)
            report_name: ë³´ê³ ì„œëª… (ë²¡í„°DB ì €ì¥ìš©)
            report_date: ë³´ê³ ì„œ ë‚ ì§œ (ë²¡í„°DB ì €ì¥ìš©)
            
        Returns:
            tuple: (text_content, saved_file_path, extracted_path)
        """
        print(f"ğŸ“¥ ë³´ê³ ì„œ ì¡°íšŒ ì¤‘... (ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no})")
        
        # 1. ë²¡í„°DBì—ì„œ ë¨¼ì € í™•ì¸
        print("   ğŸ” VectorDB ìºì‹œ í™•ì¸ ì¤‘...")
        if self.vector_store.check_report_exists(rcept_no):
            cached_content = self.vector_store.get_report_from_cache(rcept_no)
            if cached_content:
                print(f"   âœ… VectorDB ìºì‹œ ì‚¬ìš© (API í˜¸ì¶œ ìƒëµ)")
                print(f"   ğŸ’¾ ìºì‹œëœ ë‚´ìš©: {len(cached_content):,}ì")
                # ìºì‹œëœ ë‚´ìš© ë°˜í™˜ (íŒŒì¼ ê²½ë¡œëŠ” None)
                return cached_content, None, None
        
        # 2. ë²¡í„°DBì— ì—†ìœ¼ë©´ APIë¡œ ë‹¤ìš´ë¡œë“œ
        print("   âš ï¸  VectorDBì— ì—†ìŒ â†’ DART APIì—ì„œ ë‹¤ìš´ë¡œë“œ")
        
        try:
            # ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ
            params = {
                'crtfc_key': self.dart_api_key,
                'rcept_no': rcept_no
            }
            
            response = requests.get(f'{self.base_url}/document.xml', params=params, timeout=60)
            response.raise_for_status()
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            if not os.path.exists('downloads'):
                os.makedirs('downloads')
            
            # íŒŒì¼ ì €ì¥
            if not save_path:
                save_path = f'downloads/{rcept_no}.zip'
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            print(f"ğŸ“ ë³´ê³ ì„œ ì €ì¥: {save_path} ({len(response.content):,} bytes)")
            
            # ZIP íŒŒì¼ì¸ì§€ í™•ì¸í•˜ê³  ì••ì¶• í•´ì œ
            content = ""
            extracted_path = None
            
            if zipfile.is_zipfile(save_path):
                # ZIP ì••ì¶• í•´ì œ
                extract_dir = save_path.replace('.zip', '_extracted')
                os.makedirs(extract_dir, exist_ok=True)
                
                with zipfile.ZipFile(save_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                    file_list = zip_ref.namelist()
                    
                    if file_list:
                        extracted_path = os.path.join(extract_dir, file_list[0])
                        # ì²« ë²ˆì§¸ XML íŒŒì¼ ì½ê¸°
                        with zip_ref.open(file_list[0]) as xml_file:
                            content = xml_file.read().decode('utf-8', errors='ignore')
                        
                        print(f"ğŸ“‚ ì••ì¶• í•´ì œ: {extract_dir}")
            else:
                # ì§ì ‘ XML ì½ê¸°
                with open(save_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                extracted_path = save_path
            
            # XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° Markdown ë³€í™˜
            print(f"ğŸ“ XML â†’ Markdown ë³€í™˜ ì‹œì‘ (ì›ë³¸: {len(content):,}ì)")
            text_content = self._extract_text_from_xml(content)
            
            print(f"âœ… ë³´ê³ ì„œ ë³€í™˜ ì™„ë£Œ (Markdown: {len(text_content):,}ì)")
            
            # 3. VectorDBì— ì €ì¥
            if text_content and company_name and report_name and report_date:
                print(f"ğŸ’¾ VectorDBì— ë³´ê³ ì„œ ì €ì¥ ì¤‘...")
                try:
                    self.vector_store.add_report(
                        rcept_no=rcept_no,
                        report_name=report_name,
                        company_name=company_name,
                        report_date=report_date,
                        content=text_content
                    )
                    print(f"âœ… VectorDB ì €ì¥ ì™„ë£Œ")
                except Exception as ve:
                    print(f"âš ï¸  VectorDB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {ve}")
            
            return text_content, save_path, extracted_path
            
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return "", None, None
    
    def _extract_text_from_xml(self, xml_content):
        """
        XML íŒŒì¼ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (XML íƒœê·¸ë§Œ ì œê±°, ëª¨ë“  ë‚´ìš© ë³´ì¡´)
        
        Args:
            xml_content: XML ë‚´ìš©
            
        Returns:
            str: ìˆœìˆ˜ í…ìŠ¤íŠ¸ (ì„œì‹ ì—†ìŒ, XML íƒœê·¸ë§Œ ì œê±°)
        """
        try:
            from bs4 import BeautifulSoup
            
            original_size = len(xml_content)
            print(f"      ğŸ“ ì›ë³¸ XML í¬ê¸°: {original_size:,}ì ({original_size/1024:.1f}KB)")
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            print(f"      ğŸ”§ XML íŒŒì‹± ì¤‘ (BeautifulSoup)...")
            try:
                soup = BeautifulSoup(xml_content, 'xml')
                print(f"      âœ… XML íŒŒì‹± ì„±ê³µ")
            except:
                print(f"      âš ï¸  XML íŒŒì„œ ì‹¤íŒ¨, lxml ì‹œë„...")
                try:
                    soup = BeautifulSoup(xml_content, 'lxml')
                    print(f"      âœ… lxml íŒŒì‹± ì„±ê³µ")
                except:
                    print(f"      âš ï¸  ëª¨ë“  íŒŒì„œ ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ ì „í™˜")
                    return self._simple_text_extraction(xml_content)
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (íƒœê·¸ ì œê±°, ë‚´ìš© ë³´ì¡´, ì œí•œ ì—†ìŒ)
            print(f"      ğŸ“ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (ì œí•œ ì—†ìŒ)")
            extracted_text = soup.get_text(separator='\n', strip=False)
            
            # ì •ë¦¬
            print(f"      ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘...")
            
            # 1. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ (ì¤„ë°”ê¿ˆì€ ë³´ì¡´)
            extracted_text = re.sub(r'[ \t]+', ' ', extracted_text)
            
            # 2. 3ê°œ ì´ìƒì˜ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
            extracted_text = re.sub(r'\n{3,}', '\n\n', extracted_text)
            
            # 3. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
            lines = extracted_text.split('\n')
            lines = [line.strip() for line in lines if line.strip()]  # ë¹ˆ ì¤„ ì œê±°
            extracted_text = '\n'.join(lines)
            
            # 4. ì „ì²´ ì•ë’¤ ê³µë°± ì œê±°
            extracted_text = extracted_text.strip()
            
            extracted_size = len(extracted_text)
            
            print(f"   âœ… XML â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
            print(f"      ì›ë³¸ í¬ê¸°: {original_size:,}ì ({original_size/1024:.1f}KB)")
            print(f"      ì¶”ì¶œ í›„: {extracted_size:,}ì ({extracted_size/1024:.1f}KB)")
            print(f"      ë³´ì¡´ìœ¨: {(extracted_size/original_size*100):.1f}%")
            
            if not extracted_text or len(extracted_text) < 1000:
                print(f"   âš ï¸  ì¶”ì¶œ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìŒ ({len(extracted_text)}ì), ì •ê·œì‹ìœ¼ë¡œ ì¬ì‹œë„")
                return self._simple_text_extraction(xml_content)
            
            return extracted_text
            
        except Exception as e:
            print(f"   âš ï¸  XML íŒŒì‹± ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ ì „í™˜: {e}")
            return self._simple_text_extraction(xml_content)
    
    def _parse_table_to_markdown(self, table_element):
        """í…Œì´ë¸”ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            rows = []
            
            # TR íƒœê·¸ ì°¾ê¸°
            for tr in table_element.iter():
                if 'TR' in tr.tag.upper():
                    cells = []
                    for td in tr:
                        if 'TD' in td.tag.upper() or 'TH' in td.tag.upper() or 'TU' in td.tag.upper():
                            cell_text = td.text.strip() if td.text else ''
                            cells.append(cell_text)
                    
                    if cells:
                        rows.append(cells)
            
            if not rows:
                return ""
            
            # Markdown í…Œì´ë¸” ìƒì„±
            md_table = []
            
            # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ
            if rows:
                header = ' | '.join(rows[0])
                md_table.append(f"| {header} |")
                md_table.append('|' + ' --- |' * len(rows[0]))
                
                # ë‚˜ë¨¸ì§€ í–‰
                for row in rows[1:6]:  # ìµœëŒ€ 5ê°œ í–‰ë§Œ
                    row_text = ' | '.join(row)
                    md_table.append(f"| {row_text} |")
            
            return '\n'.join(md_table) + '\n\n'
            
        except:
            return ""
    
    def _simple_text_extraction(self, xml_content):
        """
        ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°±ì—…ìš©) - ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ XML íƒœê·¸ë§Œ ì œê±°
        ëª¨ë“  ë‚´ìš©ì„ ë³´ì¡´í•˜ê³  XML íƒœê·¸ë§Œ ì œê±°
        """
        print(f"      ğŸ”§ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“œ (ì •ê·œì‹)")
        
        # 1. XML íƒœê·¸ ì œê±° (ë‚´ìš©ì€ ë³´ì¡´)
        # <tag>content</tag> â†’ content
        text = re.sub(r'<[^>]+>', '\n', xml_content)
        
        # 2. XML ì„ ì–¸, DOCTYPE ë“± ì œê±°
        text = re.sub(r'<\?xml[^>]*\?>', '', text)
        text = re.sub(r'<!DOCTYPE[^>]*>', '', text)
        
        # 3. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ (ë‹¨, ì¤„ë°”ê¿ˆì€ ë³´ì¡´)
        text = re.sub(r'[ \t]+', ' ', text)
        
        # 4. 3ê°œ ì´ìƒì˜ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 5. ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        text = '\n'.join(lines)
        
        # 6. ë¹ˆ ì¤„ ì—°ì† ì œê±°
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 7. ì „ì²´ ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        print(f"      ì¶”ì¶œ ê²°ê³¼: {len(text):,}ì ({len(text)/1024:.1f}KB)")
        
        return text
    
    def cleanup_downloads(self, keep_latest=0):
        """
        downloads í´ë”ì˜ íŒŒì¼ë“¤ì„ ì •ë¦¬
        ë²¡í„°DBì— ì €ì¥ëœ ë³´ê³ ì„œëŠ” ì‚­ì œí•´ë„ ì•ˆì „í•¨
        
        Args:
            keep_latest: ìµœì‹  íŒŒì¼ ëª‡ ê°œë¥¼ ë‚¨ê¸¸ì§€ (0ì´ë©´ ëª¨ë‘ ì‚­ì œ)
        """
        downloads_dir = 'downloads'
        
        if not os.path.exists(downloads_dir):
            print(f"â„¹ï¸  '{downloads_dir}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ§¹ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
        print(f"   ê²½ë¡œ: {downloads_dir}")
        
        try:
            # ëª¨ë“  íŒŒì¼ ë° í´ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            items = []
            for item in os.listdir(downloads_dir):
                item_path = os.path.join(downloads_dir, item)
                if os.path.isfile(item_path) or os.path.isdir(item_path):
                    # ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
                    mtime = os.path.getmtime(item_path)
                    items.append((item_path, mtime))
            
            # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
            items.sort(key=lambda x: x[1], reverse=True)
            
            print(f"   ì´ {len(items)}ê°œ í•­ëª© ë°œê²¬")
            
            # ì‚­ì œí•  í•­ëª© ê²°ì •
            items_to_delete = items[keep_latest:] if keep_latest > 0 else items
            items_to_keep = items[:keep_latest] if keep_latest > 0 else []
            
            if items_to_keep:
                print(f"   ğŸ”’ ìµœì‹  {len(items_to_keep)}ê°œ í•­ëª© ìœ ì§€:")
                for path, _ in items_to_keep[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                    print(f"      - {os.path.basename(path)}")
            
            if items_to_delete:
                print(f"   ğŸ—‘ï¸  {len(items_to_delete)}ê°œ í•­ëª© ì‚­ì œ ì¤‘...")
                
                deleted_count = 0
                freed_size = 0
                
                for item_path, _ in items_to_delete:
                    try:
                        # í¬ê¸° ê³„ì‚°
                        if os.path.isfile(item_path):
                            size = os.path.getsize(item_path)
                            os.remove(item_path)
                            deleted_count += 1
                            freed_size += size
                        elif os.path.isdir(item_path):
                            size = sum(
                                os.path.getsize(os.path.join(dirpath, filename))
                                for dirpath, dirnames, filenames in os.walk(item_path)
                                for filename in filenames
                            )
                            shutil.rmtree(item_path)
                            deleted_count += 1
                            freed_size += size
                    except Exception as e:
                        print(f"      âš ï¸  ì‚­ì œ ì‹¤íŒ¨: {os.path.basename(item_path)} - {e}")
                
                print(f"   âœ… ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ í•­ëª© ì‚­ì œ")
                print(f"   ğŸ’¾ í™•ë³´ëœ ê³µê°„: {freed_size / 1024 / 1024:.2f} MB")
            else:
                print(f"   â„¹ï¸  ì‚­ì œí•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def extract_text_from_pdf(self, pdf_path):
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pdf_path}")
            text = ""
            
            with fitz.open(pdf_path) as doc:
                total_pages = len(doc)
                print(f"   ğŸ“– ì´ {total_pages}í˜ì´ì§€")
                
                for page_num, page in enumerate(doc, 1):
                    page_text = page.get_text()
                    text += f"\n--- í˜ì´ì§€ {page_num} ---\n{page_text}"
                    
                    if page_num % 10 == 0:
                        print(f"   âœ“ {page_num}/{total_pages} í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
                
                print(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text):,}ì")
                return text
                
        except Exception as e:
            print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def get_company_industry(self, corp_code, company_name):
        """
        DART APIë¡œ ê¸°ì—…ì˜ ì‚°ì—…êµ° íŒŒì•…
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            company_name: íšŒì‚¬ëª…
            
        Returns:
            str: ì‚°ì—…êµ° ì •ë³´
        """
        try:
            print(f"ğŸ¢ {company_name}ì˜ ì‚°ì—…êµ° íŒŒì•… ì¤‘...")
            
            # DART ê¸°ì—…ê°œí™© API í˜¸ì¶œ
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code
            }
            
            response = requests.get(f'{self.base_url}/company.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                industry = data.get('induty_code', '')
                
                if industry:
                    print(f"   âœ… ì‚°ì—…êµ°: {industry}")
                    return industry
                else:
                    print(f"   âš ï¸  ì‚°ì—…êµ° ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Geminië¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
                    # Geminië¡œ ì‚°ì—…êµ° ì¶”ë¡ 
                    return self._infer_industry_with_gemini(company_name)
            else:
                print(f"   âš ï¸  API í˜¸ì¶œ ì‹¤íŒ¨, Geminië¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
                return self._infer_industry_with_gemini(company_name)
                
        except Exception as e:
            print(f"   âš ï¸  ì‚°ì—…êµ° ì¡°íšŒ ì‹¤íŒ¨: {e}, Geminië¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.")
            return self._infer_industry_with_gemini(company_name)
    
    def _infer_industry_with_gemini(self, company_name):
        """
        Geminië¡œ ê¸°ì—…ì˜ ì‚°ì—…êµ° ì¶”ë¡ 
        
        Args:
            company_name: íšŒì‚¬ëª…
            
        Returns:
            str: ì¶”ë¡ ëœ ì‚°ì—…êµ°
        """
        try:
            prompt = f"""
í•œêµ­ ê¸°ì—… "{company_name}"ì´(ê°€) ì†í•œ ì‚°ì—…êµ°/ì—…ì¢…ì„ ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
- ì‚¼ì„±ì „ì â†’ ë°˜ë„ì²´, ì „ì
- í˜„ëŒ€ìë™ì°¨ â†’ ìë™ì°¨
- ë„¤ì´ë²„ â†’ ITì„œë¹„ìŠ¤, ì¸í„°ë„·í”Œë«í¼
- ì¹´ì¹´ì˜¤ â†’ ITì„œë¹„ìŠ¤, ì¸í„°ë„·í”Œë«í¼
- SKí•˜ì´ë‹‰ìŠ¤ â†’ ë°˜ë„ì²´

"{company_name}"ì˜ ì‚°ì—…êµ°ì„ 2-3ë‹¨ì–´ë¡œ ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì„¤ëª… ì—†ì´ ì‚°ì—…êµ° ëª…ì¹­ë§Œ ì‘ì„±í•˜ì„¸ìš”.
"""
            
            industry = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            print(f"   ğŸ¤– Gemini ì¶”ë¡  ì‚°ì—…êµ°: {industry}")
            return industry
            
        except Exception as e:
            print(f"   âš ï¸  Gemini ì‚°ì—…êµ° ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return "ì¼ë°˜"
    
    def _extract_industry_keywords(self, user_query, company_name, base_industry):
        """
        Geminië¡œ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì‚°ì—…ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            company_name: íšŒì‚¬ëª…
            base_industry: ê¸°ë³¸ ì‚°ì—…êµ°
            
        Returns:
            list: ì‚°ì—…ë¶„ì„ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ¤– Geminiì—ê²Œ ì‚°ì—…ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ ì¤‘...")
            
            prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ì¦ê¶Œ ì‹œì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**íšŒì‚¬**: {company_name}
**ê¸°ë³¸ ì‚°ì—…êµ° ì½”ë“œ**: {base_industry}
**ì‚¬ìš©ì ì§ˆë¬¸**: {user_query}

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ë„¤ì´ë²„ ê¸ˆìœµì˜ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•  ìµœì ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ì¤‘ìš”**: 
- ì‚°ì—…êµ° ì½”ë“œê°€ ì£¼ì–´ì§„ ê²½ìš°, ì´ë¥¼ ì‹¤ì œ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì‚°ì—…ëª…/í‚¤ì›Œë“œë¡œ ë³€í™˜í•˜ì„¸ìš”
- ì˜ˆ: "612" â†’ ["ì² ê°•", "ê¸ˆì†"]
- ì˜ˆ: "264" â†’ ["í†µì‹ ", "ì´ë™í†µì‹ "]
- ì˜ˆ: "730" â†’ ["ITì„œë¹„ìŠ¤", "ì†Œí”„íŠ¸ì›¨ì–´"]
- ìˆ«ì ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!

**ë¶„ì„ ì§€ì¹¨**:
1. ì‚¬ìš©ìê°€ íŠ¹ì • ì‚°ì—…/ì‚¬ì—…ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´ ê·¸ ì‚°ì—… í‚¤ì›Œë“œë¥¼ ìš°ì„  ì‚¬ìš©
   ì˜ˆ: "ë°˜ë„ì²´ ì‚¬ì—… ì „ë§" â†’ ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬"]
   ì˜ˆ: "ì „ê¸°ì°¨ ì‹œì¥ ë¶„ì„" â†’ ["ì „ê¸°ì°¨", "ìë™ì°¨", "ë°°í„°ë¦¬"]
   
2. ì¼ë°˜ì ì¸ ê¸°ì—… ë¶„ì„ì´ë©´ íšŒì‚¬ì˜ ì£¼ë ¥ ì‚°ì—… í‚¤ì›Œë“œ ì‚¬ìš©
   ì˜ˆ: "ì¬ë¬´ ìƒíƒœ ë¶„ì„" â†’ íšŒì‚¬ì˜ ì£¼ë ¥ ì‚°ì—… (ì½”ë“œë¥¼ ì‚°ì—…ëª…ìœ¼ë¡œ ë³€í™˜)
   ì˜ˆ: "ì‹¤ì  ì „ë§" â†’ íšŒì‚¬ì˜ ì£¼ë ¥ ì‚°ì—… (ì½”ë“œë¥¼ ì‚°ì—…ëª…ìœ¼ë¡œ ë³€í™˜)

3. ì—¬ëŸ¬ ì‚¬ì—…/ì‚°ì—…ì´ ì–¸ê¸‰ë˜ë©´ ëª¨ë‘ í¬í•¨
   ì˜ˆ: "AIì™€ í´ë¼ìš°ë“œ ì‚¬ì—…" â†’ ["AI", "ì¸ê³µì§€ëŠ¥", "í´ë¼ìš°ë“œ", "ITì„œë¹„ìŠ¤"]

**ì¶œë ¥ í˜•ì‹** (JSONë§Œ ì‘ì„±):
{{
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "reason": "ì„ íƒ ì´ìœ "
}}

**ì˜ˆì‹œ**:
- íšŒì‚¬: ì‚¼ì„±ì „ì, ì‚°ì—…ì½”ë“œ: 264, ì§ˆë¬¸: "ë°˜ë„ì²´ ì‚¬ì—… ì „ë§ì€?" 
  â†’ {{"keywords": ["ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "ì „ì"], "reason": "ë°˜ë„ì²´ ì‚¬ì—…ì— ëŒ€í•œ ì§ˆë¬¸"}}

- íšŒì‚¬: í˜„ëŒ€ì°¨, ì‚°ì—…ì½”ë“œ: 301, ì§ˆë¬¸: "ì „ê¸°ì°¨ ì‹œì¥ ê²½ìŸë ¥ì€?"
  â†’ {{"keywords": ["ì „ê¸°ì°¨", "ìë™ì°¨", "ë°°í„°ë¦¬"], "reason": "ì „ê¸°ì°¨ ì‹œì¥ì— ëŒ€í•œ ì§ˆë¬¸"}}

- íšŒì‚¬: KT, ì‚°ì—…ì½”ë“œ: 264, ì§ˆë¬¸: "ìµœê·¼ ì¬ë¬´ ìƒíƒœëŠ”?"
  â†’ {{"keywords": ["í†µì‹ ", "ì´ë™í†µì‹ ", "5G"], "reason": "ì¼ë°˜ì  ì§ˆë¬¸ì´ë¯€ë¡œ íšŒì‚¬ì˜ ì£¼ë ¥ ì‚°ì—…ì¸ í†µì‹  ê´€ë ¨ í‚¤ì›Œë“œ ì‚¬ìš©"}}

ë‹µë³€ì€ JSONë§Œ ì‘ì„±í•˜ì„¸ìš”. ì ˆëŒ€ë¡œ ìˆ«ì ì½”ë“œë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!
"""
            
            response_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='query_analysis'
            ).strip()
            
            # JSON íŒŒì‹±
            import json
            import re
            
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_text = json_match.group(0)
                result = json.loads(json_text)
                
                keywords = result.get('keywords', [])
                reason = result.get('reason', '')
                
                print(f"   âœ… ì¶”ì¶œëœ ì‚°ì—… í‚¤ì›Œë“œ: {keywords}")
                print(f"   ğŸ’¡ ì´ìœ : {reason}")
                
                return keywords
            else:
                print(f"   âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì‚°ì—…êµ° ì‚¬ìš©")
                return [base_industry.split(',')[0].strip()]
                
        except Exception as e:
            print(f"   âš ï¸  ì‚°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ê¸°ë³¸ ì‚°ì—…êµ°ì˜ ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì‚¬ìš©
            return [base_industry.split(',')[0].strip()]
    
    def get_historical_annual_reports(self, corp_code, years=5):
        """
        ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ (ì¥ê¸° ì¶”ì„¸ ë¶„ì„ìš©)
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            years: ìˆ˜ì§‘í•  ì—°ìˆ˜
            
        Returns:
            list: ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡
        """
        print(f"ğŸ“… ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ ì¤‘... ({years}ë…„ì¹˜)")
        
        try:
            # ê²€ìƒ‰ ê¸°ê°„ ì„¤ì •
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')
            
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code,
                'bgn_de': start_date,
                'end_de': end_date,
                'page_no': 1,
                'page_count': 100
            }
            
            response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                all_reports = data.get('list', [])
                
                # ì‚¬ì—…ë³´ê³ ì„œë§Œ í•„í„°ë§
                annual_reports = []
                for report in all_reports:
                    report_name = report.get('report_nm', '')
                    if 'ì‚¬ì—…ë³´ê³ ì„œ' in report_name:
                        annual_reports.append(report)
                
                # ì—°ë„ë³„ë¡œ ê·¸ë£¹í™” (ê° ì—°ë„ë‹¹ 1ê°œì”©ë§Œ)
                reports_by_year = {}
                for report in annual_reports:
                    rcept_dt = report.get('rcept_dt', '')
                    if rcept_dt and len(rcept_dt) >= 4:
                        year = rcept_dt[:4]  # YYYYMMDDì—ì„œ YYYY ì¶”ì¶œ
                        # í•´ë‹¹ ì—°ë„ì˜ ì²« ë²ˆì§¸ ë³´ê³ ì„œë§Œ ì €ì¥ (ìµœì‹ ìˆœ ì •ë ¬ë˜ì–´ ìˆìŒ)
                        if year not in reports_by_year:
                            reports_by_year[year] = report
                
                # ì—°ë„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  â†’ ê³¼ê±°)
                sorted_reports = [reports_by_year[year] for year in sorted(reports_by_year.keys(), reverse=True)]
                
                # ìµœëŒ€ yearsê°œê¹Œì§€ë§Œ
                selected_reports = sorted_reports[:years]
                
                print(f"   âœ… {len(selected_reports)}ê°œ ì—°ë„ì˜ ì‚¬ì—…ë³´ê³ ì„œ ë°œê²¬")
                for report in selected_reports:
                    rcept_dt = report.get('rcept_dt', '')
                    report_nm = report.get('report_nm', '')
                    print(f"      - {rcept_dt[:4]}ë…„: {report_nm}")
                
                return selected_reports
            else:
                print(f"   âŒ ì‚¬ì—…ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {data.get('message')}")
                return []
                
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_analyst_reports(self, corp_code, count=5, user_query=None, years=None):
        """
        ê´€ë ¨ ë³´ê³ ì„œ ì¡°íšŒ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ë³´ê³ ì„œ ì„ íƒ)
        
        Args:
            corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸
            count: ì¡°íšŒí•  ë³´ê³ ì„œ ìˆ˜
            user_query: ì‚¬ìš©ì ì§ˆë¬¸ (ë³´ê³ ì„œ íƒ€ì… ìë™ ì„ íƒìš©)
            years: ê²€ìƒ‰í•  ì—°ìˆ˜ (Noneì´ë©´ user_queryì—ì„œ ìë™ ì¶”ì¶œ)
            
        Returns:
            list: ë³´ê³ ì„œ ëª©ë¡
        """
        print(f"ğŸ“Š ì¶”ê°€ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘... (ìµœëŒ€ {count}ê°œ)")
        
        # ì‹œê°„ ë²”ìœ„ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ user_queryì—ì„œ ì¶”ì¶œ
        if years is None and user_query:
            years = self._extract_time_range(user_query)
        elif years is None:
            years = 3  # ê¸°ë³¸ê°’ (ë©”ì¸ë³´ë‹¤ ë„“ê²Œ)
        
        # ë³´ê³ ì„œ íƒ€ì… ìë™ ì„ íƒ
        need_historical = False
        if user_query:
            print(f"   ğŸ’¡ ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œ íƒ€ì… ì„ íƒ")
            target_types, need_historical = self._recommend_report_types(user_query, years)
            
            # ê¸°ë³¸ ë³´ê³ ì„œë“¤ë„ í¬í•¨ (ì¤‘ë³µ ì œê±°)
            default_types = ['ë¶„ê¸°ë³´ê³ ì„œ', 'ë°˜ê¸°ë³´ê³ ì„œ', 'ì‚¬ì—…ë³´ê³ ì„œ', 'ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ', 'ê¸°íƒ€ê²½ì˜ì‚¬í•­(ììœ¨ê³µì‹œ)', 'ì¡°íšŒê³µì‹œìš”êµ¬']
            for dtype in default_types:
                if dtype not in target_types:
                    target_types.append(dtype)
        else:
            # ê¸°ë³¸ ë³´ê³ ì„œ íƒ€ì…
            target_types = [
                'ë¶„ê¸°ë³´ê³ ì„œ',
                'ë°˜ê¸°ë³´ê³ ì„œ', 
                'ì‚¬ì—…ë³´ê³ ì„œ',
                'ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ',
                'ê¸°íƒ€ê²½ì˜ì‚¬í•­(ììœ¨ê³µì‹œ)',
                'ì¡°íšŒê³µì‹œìš”êµ¬',
            ]
        
        try:
            # ì§€ì •ëœ ê¸°ê°„ ê²€ìƒ‰
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')
            
            print(f"   ğŸ“… ê²€ìƒ‰ ê¸°ê°„: {start_date} ~ {end_date} ({years}ë…„)")
            
            # ì¥ê¸° ì¶”ì„¸ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘
            if need_historical and years >= 5:
                print(f"   ğŸ“š ì¥ê¸° ì¶”ì„¸ ë¶„ì„ ê°ì§€: ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘")
                historical_reports = self.get_historical_annual_reports(corp_code, years)
                
                # ì¼ë°˜ ë³´ê³ ì„œë„ ìˆ˜ì§‘ (ì‚¬ì—…ë³´ê³ ì„œ ì œì™¸)
                params = {
                    'crtfc_key': self.dart_api_key,
                    'corp_code': corp_code,
                    'bgn_de': start_date,
                    'end_de': end_date,
                    'page_no': 1,
                    'page_count': 50
                }
                
                response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                regular_reports = []
                if data.get('status') == '000':
                    all_reports = data.get('list', [])
                    
                    # ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ íƒ€ì… í•„í„°ë§
                    for report in all_reports:
                        report_name = report.get('report_nm', '')
                        # ì‚¬ì—…ë³´ê³ ì„œëŠ” ì´ë¯¸ historical_reportsì— ìˆìœ¼ë¯€ë¡œ ì œì™¸
                        if 'ì‚¬ì—…ë³´ê³ ì„œ' not in report_name:
                            for report_type in target_types:
                                if report_type in report_name:
                                    regular_reports.append(report)
                                    break
                    
                    # ìµœì‹ ìˆœìœ¼ë¡œ ì œí•œ
                    regular_reports = regular_reports[:max(1, count - len(historical_reports))]
                
                # ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ + ì¼ë°˜ ë³´ê³ ì„œ í•©ì¹¨
                selected_reports = historical_reports + regular_reports
                
                print(f"âœ… ì´ {len(selected_reports)}ê°œì˜ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                print(f"   - ì—°ë„ë³„ ì‚¬ì—…ë³´ê³ ì„œ: {len(historical_reports)}ê°œ")
                print(f"   - ê¸°íƒ€ ë³´ê³ ì„œ: {len(regular_reports)}ê°œ")
                
                for i, report in enumerate(selected_reports, 1):
                    report_name = report.get('report_nm', '')
                    rcept_dt = report.get('rcept_dt', '')
                    marker = "ğŸ“…" if 'ì‚¬ì—…ë³´ê³ ì„œ' in report_name else "ğŸ“„"
                    print(f"   [{i}] {marker} {report_name} ({rcept_dt})")
                
                return selected_reports
            
            # ì¼ë°˜ì ì¸ ê²½ìš° (ì¥ê¸° ì¶”ì„¸ ì•„ë‹˜)
            params = {
                'crtfc_key': self.dart_api_key,
                'corp_code': corp_code,
                'bgn_de': start_date,
                'end_de': end_date,
                'page_no': 1,
                'page_count': 100
            }
            
            response = requests.get(f'{self.base_url}/list.json', params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == '000':
                all_reports = data.get('list', [])
                
                # ë‹¤ì–‘í•œ ë³´ê³ ì„œ ìœ í˜• í•„í„°ë§
                # Gemini ì¶”ì²œ ë³´ê³ ì„œë¥¼ ìš°ì„  ìˆœìœ„ë¡œ ë°°ì¹˜
                filtered_reports = []
                for report in all_reports:
                    report_name = report.get('report_nm', '')
                    for report_type in target_types:
                        if report_type in report_name:
                            filtered_reports.append(report)
                            break
                
                # ìµœì‹ ìˆœìœ¼ë¡œ countê°œë§Œ ì„ íƒ
                selected_reports = filtered_reports[:count]
                
                print(f"âœ… ì´ {len(selected_reports)}ê°œì˜ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                for i, report in enumerate(selected_reports, 1):
                    print(f"   [{i}] {report.get('report_nm')} ({report.get('rcept_dt')})")
                
                return selected_reports
            else:
                print(f"âŒ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {data.get('message')}")
                return []
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return []
    
    def download_multiple_reports(self, reports, max_reports=5, company_name=None):
        """
        ì—¬ëŸ¬ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ë° ë‚´ìš© ì¶”ì¶œ
        
        Args:
            reports: ë³´ê³ ì„œ ëª©ë¡
            max_reports: ìµœëŒ€ ë‹¤ìš´ë¡œë“œ ìˆ˜
            company_name: íšŒì‚¬ëª… (ë²¡í„°DB ì €ì¥ìš©)
            
        Returns:
            list: [(report_name, content), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“¥ {min(len(reports), max_reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        
        downloaded_reports = []
        
        for i, report in enumerate(reports[:max_reports], 1):
            rcept_no = report.get('rcept_no')
            report_name = report.get('report_nm')
            rcept_dt = report.get('rcept_dt')
            
            print(f"\n[{i}/{min(len(reports), max_reports)}] {report_name} ({rcept_dt})")
            
            try:
                content, zip_path, xml_path = self.download_report(
                    rcept_no=rcept_no,
                    company_name=company_name,
                    report_name=report_name,
                    report_date=rcept_dt
                )
                
                if content:
                    downloaded_reports.append({
                        'name': report_name,
                        'date': rcept_dt,
                        'content': content,
                        'rcept_no': rcept_no
                    })
                    # VectorDBì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°ì™€ API ë‹¤ìš´ë¡œë“œ êµ¬ë¶„
                    if zip_path is None and xml_path is None:
                        print(f"âœ… VectorDB ê²€ìƒ‰ í™•ì¸: {len(content):,}ì")
                    else:
                        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(content):,}ì")
                else:
                    print(f"âš ï¸  ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\nâœ… ì´ {len(downloaded_reports)}ê°œ ë³´ê³ ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        return downloaded_reports
    
    def analyze_with_gemini(self, company_name, report_content, user_query, additional_reports=None):
        """
        Gemini APIë¡œ ë³´ê³ ì„œ ë¶„ì„
        
        Args:
            company_name: íšŒì‚¬ëª…
            report_content: ë©”ì¸ ë³´ê³ ì„œ ë‚´ìš©
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            additional_reports: ì¶”ê°€ ë³´ê³ ì„œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            str: ë¶„ì„ ê²°ê³¼
        """
        print(f"ğŸ¤– Geminië¡œ ë¶„ì„ ì¤‘...")
        
        try:
            # Gemini 2.5 ProëŠ” 1ë°±ë§Œ í† í°ì„ ì§€ì›í•˜ë¯€ë¡œ ë³´ê³ ì„œ ì „ì²´ ì‚¬ìš© ê°€ëŠ¥
            # í•˜ì§€ë§Œ ë„ˆë¬´ í¬ë©´ ì‘ë‹µ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ì ˆíˆ ì œí•œ
            max_length_per_report = 200000  # ë³´ê³ ì„œë‹¹ ìµœëŒ€ 20ë§Œì
            
            # ë©”ì¸ ë³´ê³ ì„œ ì²˜ë¦¬
            original_length = len(report_content)
            print(f"   ğŸ“Š ë©”ì¸ ë³´ê³ ì„œ ì •ë³´:")
            print(f"      - í˜•ì‹: Markdown (XMLì—ì„œ ë³€í™˜ë¨)")
            print(f"      - í¬ê¸°: {len(report_content):,}ì")
            print(f"      - ì˜ˆìƒ í† í°: ì•½ {len(report_content) / 4:,.0f} í† í°")
            
            if len(report_content) > max_length_per_report:
                print(f"   âš ï¸  ë³´ê³ ì„œê°€ í½ë‹ˆë‹¤. {max_length_per_report:,}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                report_content = report_content[:max_length_per_report]
                print(f"   âœ… ë³´ê³ ì„œ í¬ê¸° ì¡°ì •: {len(report_content):,}ì")
            
            # ì¶”ê°€ ë³´ê³ ì„œ ì²˜ë¦¬
            additional_content = ""
            dart_reports = []
            naver_reports = []
            
            if additional_reports:
                print(f"\n   ğŸ“š ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports)}ê°œ ì²˜ë¦¬ ì¤‘...")
                for i, report in enumerate(additional_reports, 1):
                    report_name = report.get('name', f'ë³´ê³ ì„œ {i}')
                    report_date = report.get('date', '')
                    content = report.get('content', '')
                    
                    # ê° ì¶”ê°€ ë³´ê³ ì„œë„ ì œí•œ
                    if len(content) > max_length_per_report:
                        content = content[:max_length_per_report]
                    
                    # ë³´ê³ ì„œ ì¶œì²˜ êµ¬ë¶„ (ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì¸ì§€ í™•ì¸)
                    if report_name.startswith('[') and ']' in report_name:
                        # ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í˜•ì‹: [ì¦ê¶Œì‚¬ëª…] ì œëª©
                        naver_reports.append((report_name, report_date, content))
                    else:
                        # DART ë³´ê³ ì„œ
                        dart_reports.append((report_name, report_date, content))
                
                # DART ë³´ê³ ì„œ ì¶”ê°€
                if dart_reports:
                    additional_content += "\n\n" + "="*80 + "\n"
                    additional_content += "ğŸ“‹ DART ê³µì‹œ ë³´ê³ ì„œ\n"
                    additional_content += "="*80 + "\n"
                    
                    for idx, (name, date, content) in enumerate(dart_reports, 1):
                        additional_content += f"""

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[DART ë³´ê³ ì„œ {idx}: {name} ({date})]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{content}
"""
                        print(f"      âœ“ [DART {idx}] {name}: {len(content):,}ì")
                
                # ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ì¶”ê°€
                if naver_reports:
                    additional_content += "\n\n" + "="*80 + "\n"
                    additional_content += "ğŸ“Š ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ (ì‹œì¥ ë¶„ì„ ë° ì „ë§)\n"
                    additional_content += "="*80 + "\n"
                    
                    for idx, (name, date, content) in enumerate(naver_reports, 1):
                        additional_content += f"""

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ {idx}: {name} ({date})]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{content}
"""
                        print(f"      âœ“ [ì¦ê¶Œì‚¬ {idx}] {name}: {len(content):,}ì")
                
                print(f"   âœ… ì¶”ê°€ ë³´ê³ ì„œ í†µí•© ì™„ë£Œ: ì´ {len(additional_content):,}ì")
                print(f"      - DART ë³´ê³ ì„œ: {len(dart_reports)}ê°œ")
                print(f"      - ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸: {len(naver_reports)}ê°œ")
            
            # ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            print(f"   ğŸ“ Gemini í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            
            additional_instruction = ""
            if additional_reports:
                report_breakdown = []
                if dart_reports:
                    report_breakdown.append(f"{len(dart_reports)}ê°œì˜ DART ê³µì‹œ ë³´ê³ ì„œ")
                if naver_reports:
                    report_breakdown.append(f"{len(naver_reports)}ê°œì˜ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸")
                
                report_list = " ë° ".join(report_breakdown)
                
                additional_instruction = f"""

**ì¤‘ìš”**: ë©”ì¸ ë³´ê³ ì„œ ì™¸ì— {report_list}ê°€ ì œê³µë©ë‹ˆë‹¤.

**ë³´ê³ ì„œ ë¶„ì„ ê°€ì´ë“œ**:
1. **DART ê³µì‹œ ë³´ê³ ì„œ**: ê¸°ì—…ì˜ ê³µì‹ì ì¸ ì¬ë¬´ ì •ë³´ ë° ì‚¬ì—… ë‚´ìš© (ê°ê´€ì  ë°ì´í„°)
2. **ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸**: ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì¥ ë¶„ì„, ì‚°ì—… ë™í–¥, íˆ¬ì ì˜ê²¬ (ì „ë¬¸ê°€ ê²¬í•´)

ëª¨ë“  ë³´ê³ ì„œì˜ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
- DART ë³´ê³ ì„œë¡œ ê¸°ì—…ì˜ ì‹¤ì œ ì¬ë¬´ ìƒíƒœì™€ ì‚¬ì—… í˜„í™©ì„ íŒŒì•…
- ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ë¡œ ì‹œì¥ ë§¥ë½, ì‚°ì—… ë™í–¥, ê²½ìŸ í™˜ê²½ì„ ì´í•´
- ë‘ ì¶œì²˜ë¥¼ ê²°í•©í•˜ì—¬ ë” ì™„ì „í•œ ê·¸ë¦¼ì„ ì œì‹œ
- ì—¬ëŸ¬ ë³´ê³ ì„œì—ì„œ ì¼ê´€ëœ íŒ¨í„´ì´ë‚˜ ë³€í™” ì¶”ì„¸ë¥¼ íŒŒì•…
"""
            
            prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ì—… ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ {company_name}ì˜ ê³µì‹œ ë³´ê³ ì„œë“¤ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œ ë‚´ìš©ì…ë‹ˆë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[{company_name} ë©”ì¸ ê³µì‹œ ë³´ê³ ì„œ]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{report_content}

{additional_content}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì‚¬ìš©ì ì§ˆë¬¸]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{user_query}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ë¶„ì„ ìš”ì²­ì‚¬í•­]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{additional_instruction}

ìœ„ ë³´ê³ ì„œë“¤ì˜ ì „ì²´ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ìš”ì•½** (3-5ì¤„)
   - ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ

2. **ìƒì„¸ ë¶„ì„**
   - ë³´ê³ ì„œì—ì„œ ì°¾ì€ ê´€ë ¨ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ ìƒì„¸íˆ ì„¤ëª…
   - êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„°ë¥¼ ì¸ìš©
   - ì„¹ì…˜ë³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
   - ì—¬ëŸ¬ ë³´ê³ ì„œê°€ ìˆë‹¤ë©´ ì‹œê°„ì— ë”°ë¥¸ ë³€í™” ì¶”ì„¸ë„ ë¶„ì„

3. **ì£¼ìš” ìˆ˜ì¹˜ ë° ì§€í‘œ**
   - ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ ë“± í•µì‹¬ ì¬ë¬´ì§€í‘œ
   - ì „ë…„ ëŒ€ë¹„ ì¦ê°ë¥ 
   - ê¸°íƒ€ ì¤‘ìš”í•œ ì •ëŸ‰ì  ë°ì´í„°
   - ì—¬ëŸ¬ ë³´ê³ ì„œì˜ ë°ì´í„°ë¥¼ ë¹„êµ ë¶„ì„

4. **ê²°ë¡  ë° ì‹œì‚¬ì **
   - ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ê²°ë¡ 
   - íˆ¬ììë‚˜ ì´í•´ê´€ê³„ìì—ê²Œ ì£¼ëŠ” ì‹œì‚¬ì 
   - í–¥í›„ ì „ë§ì´ë‚˜ ì£¼ì˜ì‚¬í•­

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë³´ê³ ì„œì˜ ì •í™•í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì¶”ì¸¡ì´ë‚˜ ë³´ê³ ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ëª…ì‹œí•´ì£¼ì„¸ìš”.
"""
            
            # LLM Orchestratorë¡œ ë¶„ì„
            total_content_length = len(report_content) + len(additional_content)
            print(f"   ğŸš€ LLM Orchestrator í˜¸ì¶œ ì¤‘...")
            print(f"      ì…ë ¥: ì•½ {len(prompt) / 4:,.0f} í† í°")
            print(f"      ì´ ë³´ê³ ì„œ ë‚´ìš©: {total_content_length:,}ì")
            
            result_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='long_context_analysis'
            )
            
            print(f"   âœ… LLM ë¶„ì„ ì™„ë£Œ!")
            print(f"      ì¶œë ¥: {len(result_text):,}ì (ì•½ {len(result_text) / 4:,.0f} í† í°)")
            
            return result_text
            
        except Exception as e:
            error_message = str(e)
            print(f"âŒ Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
            if "429" in error_message or "quota" in error_message.lower():
                return f"""
âš ï¸ Gemini API ë¬´ë£Œ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**ë¬¸ì œ**: Gemini API ë¬´ë£Œ í‹°ì–´ì˜ ì¼ì¼ ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. 24ì‹œê°„ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš” (í• ë‹¹ëŸ‰ì´ ë¦¬ì…‹ë©ë‹ˆë‹¤)
2. Google AI Studioì—ì„œ ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
3. API í‚¤ë¥¼ ì¬ë°œê¸‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ê³„ì •ì˜ í‚¤ ì‚¬ìš©

**í˜„ì¬ ìƒí™©**:
- íšŒì‚¬: {company_name}
- ë³´ê³ ì„œ: ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(report_content):,}ì)
- ì˜¤ë¥˜: ë¬´ë£Œ í• ë‹¹ëŸ‰ ì´ˆê³¼

ìì„¸í•œ ì •ë³´: https://ai.google.dev/gemini-api/docs/rate-limits
"""
            else:
                return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message[:500]}"
    
    def analyze_with_gemini_rag(self, company_name, user_query, relevant_context, num_chunks):
        """
        RAG ë°©ì‹ìœ¼ë¡œ Gemini AI ë¶„ì„ (VectorDBì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ë§Œ ì‚¬ìš©)
        
        Args:
            company_name: íšŒì‚¬ëª…
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            relevant_context: VectorDBì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ë“¤ (ê²°í•©ëœ ë¬¸ìì—´)
            num_chunks: ê²€ìƒ‰ëœ ì²­í¬ ê°œìˆ˜
            
        Returns:
            str: AI ë¶„ì„ ê²°ê³¼ (Markdown)
        """
        try:
            print(f"\nğŸ¤– Gemini AI ë¶„ì„ (RAG ëª¨ë“œ)")
            print(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°:")
            print(f"      - íšŒì‚¬ëª…: {company_name}")
            print(f"      - ê²€ìƒ‰ëœ ì²­í¬: {num_chunks}ê°œ")
            print(f"      - ì´ í…ìŠ¤íŠ¸: {len(relevant_context):,}ì")
            print(f"      - ì§ˆë¬¸: {user_query[:100]}...")
            
            # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ì—… ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” {company_name}ì˜ ê³µì‹œ ë³´ê³ ì„œ ë° ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ë°œì·Œí•œ ê²ƒì…ë‹ˆë‹¤.
VectorDB ê²€ìƒ‰ì„ í†µí•´ {num_chunks}ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš© - {num_chunks}ê°œ ì²­í¬]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{relevant_context}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ì‚¬ìš©ì ì§ˆë¬¸]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{user_query}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[ë¶„ì„ ìš”ì²­ì‚¬í•­]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ìœ„ì˜ ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ êµ¬ì¡°**:

1. **í•µì‹¬ ìš”ì•½** (3-5ì¤„)
   - ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ

2. **ìƒì„¸ ë¶„ì„**
   - ê²€ìƒ‰ëœ ë‚´ìš©ì—ì„œ ì°¾ì€ ê´€ë ¨ ì •ë³´ë¥¼ ê·¼ê±°ë¡œ ìƒì„¸íˆ ì„¤ëª…
   - êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„°ë¥¼ ì¸ìš©
   - **ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ë³´ê³ ì„œëª…ê³¼ í•¨ê»˜ ëª…ì‹œ** (ì˜ˆ: "[ë°˜ê¸°ë³´ê³ ì„œ (2025.06) - Reference 3]ì— ë”°ë¥´ë©´..." ë˜ëŠ” "[í•˜ë‚˜ì¦ê¶Œ ë¦¬í¬íŠ¸ - Reference 5]ì— ë”°ë¥´ë©´...")
   - ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì¼ê´€ëœ íŒ¨í„´ì´ë‚˜ ë³€í™” ì¶”ì„¸ë¥¼ íŒŒì•…

3. **ì£¼ìš” ìˆ˜ì¹˜ ë° ì§€í‘œ**
   - ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ ë“± í•µì‹¬ ì¬ë¬´ì§€í‘œ
   - ì „ë…„ ëŒ€ë¹„ ì¦ê°ë¥ 
   - ê¸°íƒ€ ì¤‘ìš”í•œ ì •ëŸ‰ì  ë°ì´í„°

4. **ê²°ë¡  ë° ì‹œì‚¬ì **
   - ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ê²°ë¡ 
   - íˆ¬ììë‚˜ ì´í•´ê´€ê³„ìì—ê²Œ ì£¼ëŠ” ì‹œì‚¬ì 
   - í–¥í›„ ì „ë§ì´ë‚˜ ì£¼ì˜ì‚¬í•­

**ì£¼ì˜ì‚¬í•­**:
- ê²€ìƒ‰ëœ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” "ì œê³µëœ ìë£Œì—ì„œëŠ” í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œ
- ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì„ í”¼í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•œ ë¶„ì„ë§Œ ì œì‹œ
- ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…

ìœ„ ì§€ì¹¨ì— ë”°ë¼ ë¶„ì„ ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            
            # LLM Orchestratorë¡œ RAG ë¶„ì„
            print(f"   ğŸš€ LLM Orchestrator í˜¸ì¶œ ì¤‘... (RAG ëª¨ë“œ)")
            print(f"      ì…ë ¥: ì•½ {len(prompt) / 4:,.0f} í† í°")
            print(f"      ê´€ë ¨ ë‚´ìš©: {len(relevant_context):,}ì")
            
            result_text = self.llm_orchestrator.generate(
                prompt=prompt,
                task_type='long_context_analysis'
            )
            
            print(f"   âœ… LLM ë¶„ì„ ì™„ë£Œ!")
            print(f"      ì¶œë ¥: {len(result_text):,}ì (ì•½ {len(result_text) / 4:,.0f} í† í°)")
            
            return result_text
            
        except Exception as e:
            error_message = str(e)
            print(f"âŒ Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
            if "429" in error_message or "quota" in error_message.lower():
                return f"""âš ï¸ Gemini API ë¬´ë£Œ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**ë¬¸ì œ**: Gemini API ë¬´ë£Œ í‹°ì–´ì˜ ì¼ì¼ ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

**í•´ê²° ë°©ë²•**:
1. 24ì‹œê°„ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš” (í• ë‹¹ëŸ‰ì´ ë¦¬ì…‹ë©ë‹ˆë‹¤)
2. Google AI Studioì—ì„œ ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
3. API í‚¤ë¥¼ ì¬ë°œê¸‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ê³„ì •ì˜ í‚¤ ì‚¬ìš©

**í˜„ì¬ ìƒí™©**:
- íšŒì‚¬: {company_name}
- VectorDB ê²€ìƒ‰: ì™„ë£Œ ({num_chunks}ê°œ ì²­í¬)
- ì˜¤ë¥˜: ë¬´ë£Œ í• ë‹¹ëŸ‰ ì´ˆê³¼

ìì„¸í•œ ì •ë³´: https://ai.google.dev/gemini-api/docs/rate-limits
"""
            else:
                return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_message[:500]}"
    
    def _create_simple_summary(self, company_name, report_content, user_query):
        """
        AI ì—†ì´ ê°„ë‹¨í•œ ë³´ê³ ì„œ ìš”ì•½ ìƒì„±
        
        Args:
            company_name: íšŒì‚¬ëª…
            report_content: ë³´ê³ ì„œ ë‚´ìš©
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            str: ìš”ì•½ ë‚´ìš©
        """
        # ë³´ê³ ì„œ ì•ë¶€ë¶„ 2000ì ì¶”ì¶œ
        preview = report_content[:2000]
        
        summary = f"""
ğŸ“Š {company_name} ê³µì‹œë³´ê³ ì„œ ë¶„ì„

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ ì‚¬ìš©ì ì§ˆë¬¸
{user_query}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“„ ë³´ê³ ì„œ ì •ë³´
â€¢ íšŒì‚¬ëª…: {company_name}
â€¢ ì „ì²´ ë‚´ìš© ê¸¸ì´: {len(report_content):,}ì
â€¢ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: âœ…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‘ ë³´ê³ ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 2,000ì)

{preview}

... (ê³„ì†)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ ì•ˆë‚´
ë³´ê³ ì„œ ì „ì²´ ë‚´ìš©ì€ ì•„ë˜ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ZIP íŒŒì¼ê³¼ ì••ì¶• í•´ì œëœ XML íŒŒì¼ì´ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.
XML ë·°ì–´(VS Code, XML Notepad ë“±)ë¡œ ì—´ì–´ì„œ í™•ì¸í•˜ì„¸ìš”.

ë˜ëŠ” ì˜¤í”ˆë‹¤íŠ¸ ì›¹ì‚¬ì´íŠ¸(https://dart.fss.or.kr)ì—ì„œ
ë” ë³´ê¸° ì¢‹ì€ HTML ë²„ì „ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ AI ë¶„ì„ ê¸°ëŠ¥ ì•ˆë‚´
í˜„ì¬ Gemini API ì—°ë™ì„ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”í–ˆìŠµë‹ˆë‹¤.
ë³´ê³ ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì§ì ‘ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
        return summary
    
    def analyze_company(self, company_name, user_query, status_callback=None):
        """
        íšŒì‚¬ ë¶„ì„ ì „ì²´ í”„ë¡œì„¸ìŠ¤
        
        Args:
            company_name: íšŒì‚¬ëª…
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            status_callback: ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        import logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)
        
        def update_status(message):
            """ìƒíƒœ ì—…ë°ì´íŠ¸ í—¬í¼"""
            logger.info(message)
            if status_callback:
                status_callback(message)
        
        result = {
            'success': False,
            'company_name': company_name,
            'corp_code': None,
            'stock_code': None,
            'reports_found': [],
            'analysis': None,
            'error': None,
            'logs': []
        }
        
        try:
            # 1. íšŒì‚¬ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ
            update_status(f"ğŸ“‹ 1ë‹¨ê³„: '{company_name}' íšŒì‚¬ ì •ë³´ ì¡°íšŒ ì¤‘...")
            logger.info(f"íšŒì‚¬ëª…ìœ¼ë¡œ ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ ì‹œì‘: {company_name}")
            
            corp_info = self.get_corp_code(company_name)
            if not corp_info:
                error_msg = f"'{company_name}' íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                result['error'] = error_msg
                return result
            
            logger.info(f"ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ ì„±ê³µ: {corp_info}")
            
            corp_code, found_name, stock_code = corp_info
            result['company_name'] = found_name
            result['corp_code'] = corp_code
            result['stock_code'] = stock_code
            update_status(f"âœ… íšŒì‚¬ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {found_name} (ì¢…ëª©ì½”ë“œ: {stock_code})")
            
            # 2. ë³´ê³ ì„œ ê²€ìƒ‰ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ìë™ ì„ íƒ)
            update_status(f"ğŸ“Š 2ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ë° ì ì ˆí•œ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")
            logger.info(f"ê³ ìœ ë²ˆí˜¸ë¡œ ë³´ê³ ì„œ ê²€ìƒ‰: {corp_code}, ì§ˆë¬¸: {user_query}")
            
            # ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ
            years = self._extract_time_range(user_query)
            logger.info(f"ê²€ìƒ‰ ê¸°ê°„: ìµœê·¼ {years}ë…„")
            
            reports = self.get_reports(corp_code, report_types=None, user_query=user_query, years=years)
            if not reports:
                error_msg = "ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                result['error'] = error_msg
                return result
            
            logger.info(f"ë³´ê³ ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(reports)}ê°œ ë°œê²¬")
            update_status(f"âœ… {len(reports)}ê°œì˜ ì í•©í•œ ë³´ê³ ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            result['reports_found'] = [
                {
                    'report_nm': r.get('report_nm'),
                    'rcept_dt': r.get('rcept_dt'),
                    'rcept_no': r.get('rcept_no')
                }
                for r in reports[:5]  # ìµœëŒ€ 5ê°œë§Œ
            ]
            
            # 3. ê°€ì¥ ìµœê·¼ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ
            latest_report = reports[0]
            rcept_no = latest_report['rcept_no']
            report_name = latest_report['report_nm']
            report_date = latest_report.get('rcept_dt', '')
            
            update_status(f"ğŸ“¥ 3ë‹¨ê³„: ë©”ì¸ ë³´ê³ ì„œ ìˆ˜ì§‘ ì¤‘... ({report_name})")
            logger.info(f"ë©”ì¸ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹œì‘: {rcept_no} - {report_name}")
            
            report_content, zip_path, xml_path = self.download_report(
                rcept_no=rcept_no,
                company_name=found_name,
                report_name=report_name,
                report_date=report_date
            )
            
            if not report_content:
                error_msg = "ë³´ê³ ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error(error_msg)
                result['error'] = error_msg
                return result
            
            # VectorDBì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°ì™€ API ë‹¤ìš´ë¡œë“œ êµ¬ë¶„
            if zip_path is None and xml_path is None:
                # VectorDBì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°
                logger.info(f"ë©”ì¸ ë³´ê³ ì„œ VectorDB ê²€ìƒ‰ í™•ì¸: {len(report_content)}ì")
                logger.info(f"ë³´ê³ ì„œ í˜•ì‹: Markdown (ìºì‹œë¨)")
                update_status(f"âœ… ë©”ì¸ ë³´ê³ ì„œ VectorDB ê²€ìƒ‰ í™•ì¸ ({len(report_content):,}ì, Markdown í˜•ì‹)")
            else:
                # APIì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ê²½ìš°
                logger.info(f"ë©”ì¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(report_content)}ì")
                logger.info(f"ë³´ê³ ì„œ í˜•ì‹: Markdown ë³€í™˜ë¨")
                update_status(f"âœ… ë©”ì¸ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(report_content):,}ì, Markdown í˜•ì‹)")
            
            # ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë³´ ì €ì¥
            result['downloaded_files'] = {
                'zip_path': zip_path,
                'xml_path': xml_path,
                'rcept_no': rcept_no,
                'report_name': report_name
            }
            
            # 3-1. ì¶”ê°€ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (ìµœê·¼ 5ê°œ, ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜)
            update_status(f"ğŸ“š 3-1ë‹¨ê³„: DART ì¶”ê°€ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘... (ìµœê·¼ {years}ë…„, ìµœëŒ€ 5ê°œ)")
            logger.info(f"ì¶”ê°€ ë³´ê³ ì„œ ê²€ìƒ‰ ì‹œì‘ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜, ê¸°ê°„: {years}ë…„)")
            
            additional_reports_list = self.get_analyst_reports(corp_code, count=5, user_query=user_query, years=years)
            additional_reports = []
            
            if additional_reports_list:
                update_status(f"ğŸ“¥ DART ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports_list)}ê°œ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 10~20ì´ˆ ì†Œìš”)")
                logger.info(f"ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports_list)}ê°œ ìˆ˜ì§‘ ì‹œì‘")
                
                additional_reports = self.download_multiple_reports(
                    additional_reports_list,
                    max_reports=5,
                    company_name=found_name
                )
                
                if additional_reports:
                    total_additional_chars = sum(len(r.get('content', '')) for r in additional_reports)
                    update_status(f"âœ… ì¶”ê°€ ë³´ê³ ì„œ {len(additional_reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_additional_chars:,}ì)")
                    logger.info(f"ì¶”ê°€ ë³´ê³ ì„œ ìˆ˜ì§‘ ì™„ë£Œ: {len(additional_reports)}ê°œ")
                    
                    result['additional_reports'] = [
                        {
                            'name': r.get('name'),
                            'date': r.get('date'),
                            'rcept_no': r.get('rcept_no')
                        }
                        for r in additional_reports
                    ]
                else:
                    update_status(f"âš ï¸  ì¶”ê°€ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨")
                    logger.warning("ì¶”ê°€ ë³´ê³ ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨")
            else:
                update_status(f"â„¹ï¸  ì¶”ê°€ ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                logger.info("ì¶”ê°€ ë³´ê³ ì„œ ì—†ìŒ")
            
            # 3-2. ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ (ì¢…ëª©ë¶„ì„ + ì‚°ì—…ë¶„ì„)
            update_status(f"ğŸ“ˆ 3-2ë‹¨ê³„: ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì¤‘...")
            logger.info("ë„¤ì´ë²„ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì‚°ì—…êµ° íŒŒì•…
            industry = self.get_company_industry(corp_code, found_name)
            logger.info(f"ì‚°ì—…êµ°: {industry}")
            
            # ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
            naver_company_reports = []
            company_reports_from_cache = False  # ìºì‹œ ì—¬ë¶€ ì¶”ì 
            
            try:
                update_status(f"ğŸ“Š ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ í™•ì¸ ì¤‘... ({found_name})")
                
                # ë¨¼ì € VectorDB ìºì‹œ í™•ì¸
                update_status(f"   ğŸ” VectorDB ìºì‹œ í™•ì¸ ì¤‘...")
                cached_company_reports = self.vector_store.get_naver_reports_from_cache(found_name, "NAVER_COMPANY")
                
                if cached_company_reports and len(cached_company_reports) >= 3:
                    # ìºì‹œì— ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    naver_company_reports = cached_company_reports[:3]
                    company_reports_from_cache = True  # ìºì‹œ ì‚¬ìš©
                    total_chars = sum(len(r.get('content', '')) for r in naver_company_reports)
                    update_status(f"âœ… VectorDB ìºì‹œì—ì„œ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ë¡œë“œ (í¬ë¡¤ë§ ìƒëµ!)")
                    
                    for idx, report in enumerate(naver_company_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìºì‹œ ì‚¬ìš©: {len(naver_company_reports)}ê°œ")
                else:
                    # ìºì‹œì— ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ í¬ë¡¤ë§
                    if cached_company_reports:
                        update_status(f"   â„¹ï¸  ìºì‹œì— {len(cached_company_reports)}ê°œë§Œ ìˆìŒ, ì¶”ê°€ í¬ë¡¤ë§ í•„ìš”")
                    else:
                        update_status(f"   â„¹ï¸  ìºì‹œ ì—†ìŒ, ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì‹œì‘")
                    
                    update_status(f"   ğŸ” ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì¤‘... (Gemini ì¶”ì²œ ê²€ìƒ‰ì–´ í™œìš©)")
                    naver_company_reports = self.naver_crawler.search_company_reports(found_name, max_reports=3)
                
                if naver_company_reports:
                    total_chars = sum(len(r.get('content', '')) for r in naver_company_reports)
                    update_status(f"âœ… ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_chars:,}ì)")
                    
                    # ë°œê²¬ëœ ë¦¬í¬íŠ¸ ìƒì„¸ ì •ë³´ ì¶œë ¥
                    for idx, report in enumerate(naver_company_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(naver_company_reports)}ê°œ")
                else:
                    update_status(f"â„¹ï¸  ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                    logger.info("ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ì—†ìŒ")
            except Exception as e:
                logger.warning(f"ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                update_status(f"âš ï¸  ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}")
            
            # ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘
            naver_industry_reports = []
            industry_reports_from_cache = False  # ìºì‹œ ì—¬ë¶€ ì¶”ì 
            
            try:
                update_status(f"ğŸ­ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ í™•ì¸ ì¤‘...")
                
                # Geminië¡œ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì‚°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ
                update_status(f"   ğŸ¤– Geminiê°€ ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ì¤‘... (ì ì ˆí•œ ì‚°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ)")
                industry_keywords = self._extract_industry_keywords(user_query, found_name, industry)
                update_status(f"   âœ… ì¶”ì¶œëœ ì‚°ì—… í‚¤ì›Œë“œ: {', '.join(industry_keywords)}")
                
                # ë¨¼ì € VectorDB ìºì‹œ í™•ì¸ (ì‚°ì—… í‚¤ì›Œë“œë¡œ ê²€ìƒ‰!)
                update_status(f"   ğŸ” VectorDB ìºì‹œ í™•ì¸ ì¤‘... (í‚¤ì›Œë“œ: {', '.join(industry_keywords)})")
                cached_industry_reports = self.vector_store.get_naver_reports_from_cache(
                    company_name=None,  # ì‚°ì—…ë¶„ì„ì€ íšŒì‚¬ëª… í•„í„°ë§ ì•ˆ í•¨
                    report_type="NAVER_INDUSTRY",
                    industry_keywords=industry_keywords
                )
                
                if cached_industry_reports and len(cached_industry_reports) >= 2:
                    # ìºì‹œì— ì¶©ë¶„í•œ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    naver_industry_reports = cached_industry_reports[:2]
                    industry_reports_from_cache = True  # ìºì‹œ ì‚¬ìš©
                    total_chars = sum(len(r.get('content', '')) for r in naver_industry_reports)
                    update_status(f"âœ… VectorDB ìºì‹œì—ì„œ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ë¡œë“œ (í¬ë¡¤ë§ ìƒëµ!)")
                    
                    for idx, report in enumerate(naver_industry_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìºì‹œ ì‚¬ìš©: {len(naver_industry_reports)}ê°œ")
                else:
                    # ìºì‹œì— ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ í¬ë¡¤ë§
                    if cached_industry_reports:
                        update_status(f"   â„¹ï¸  ìºì‹œì— {len(cached_industry_reports)}ê°œë§Œ ìˆìŒ, ì¶”ê°€ í¬ë¡¤ë§ í•„ìš”")
                    else:
                        update_status(f"   â„¹ï¸  ìºì‹œ ì—†ìŒ, ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì‹œì‘")
                    
                    update_status(f"   ğŸ” ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ ì¤‘... (í‚¤ì›Œë“œ: {', '.join(industry_keywords)})")
                    naver_industry_reports = self.naver_crawler.search_industry_reports(industry_keywords, max_reports=2)
                
                if naver_industry_reports:
                    total_chars = sum(len(r.get('content', '')) for r in naver_industry_reports)
                    update_status(f"âœ… ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_chars:,}ì)")
                    
                    # ë°œê²¬ëœ ë¦¬í¬íŠ¸ ìƒì„¸ ì •ë³´ ì¶œë ¥
                    for idx, report in enumerate(naver_industry_reports, 1):
                        update_status(f"   [{idx}] {report.get('name', '')} ({report.get('date', '')})")
                    
                    logger.info(f"ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(naver_industry_reports)}ê°œ")
                else:
                    update_status(f"â„¹ï¸  ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                    logger.info("ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ì—†ìŒ")
            except Exception as e:
                logger.warning(f"ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                update_status(f"âš ï¸  ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}")
            
            # ë„¤ì´ë²„ ë¦¬í¬íŠ¸ ê²°ê³¼ ì €ì¥
            result['naver_reports'] = {
                'company_reports': [
                    {
                        'name': r.get('name'),
                        'date': r.get('date'),
                        'url': r.get('url')
                    }
                    for r in naver_company_reports
                ],
                'industry_reports': [
                    {
                        'name': r.get('name'),
                        'date': r.get('date'),
                        'url': r.get('url')
                    }
                    for r in naver_industry_reports
                ]
            }
            
            # ëª¨ë“  ì¶”ê°€ ë³´ê³ ì„œ í†µí•©
            all_additional_reports = additional_reports + naver_company_reports + naver_industry_reports
            logger.info(f"ì „ì²´ ì¶”ê°€ ë³´ê³ ì„œ: {len(all_additional_reports)}ê°œ (DART: {len(additional_reports)}, ì¢…ëª©: {len(naver_company_reports)}, ì‚°ì—…: {len(naver_industry_reports)})")
            
            # 3-3. ëª¨ë“  ë¦¬í¬íŠ¸ë¥¼ VectorDBì— ì €ì¥
            update_status(f"ğŸ’¾ VectorDBì— ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘...")
            logger.info("VectorDB ì €ì¥ ì‹œì‘")
            
            try:
                # ë©”ì¸ DART ë³´ê³ ì„œ ì €ì¥
                if not self.vector_store.check_report_exists(rcept_no):
                    update_status(f"   ğŸ’¾ ë©”ì¸ ë³´ê³ ì„œ VectorDB ì €ì¥ ì¤‘...")
                    self.vector_store.add_report(
                        rcept_no=rcept_no,
                        report_name=report_name,
                        company_name=found_name,
                        report_date=report_date,
                        content=report_content
                    )
                    logger.info(f"VectorDB ì €ì¥: {report_name} ({len(report_content):,}ì)")
                
                # ì¶”ê°€ DART ë³´ê³ ì„œ ì €ì¥
                for add_report in additional_reports:
                    add_rcept_no = add_report.get('rcept_no')
                    add_content = add_report.get('content', '')
                    
                    if add_rcept_no and add_content and not self.vector_store.check_report_exists(add_rcept_no):
                        self.vector_store.add_report(
                            rcept_no=add_rcept_no,
                            report_name=add_report.get('name', ''),
                            company_name=found_name,
                            report_date=add_report.get('date', ''),
                            content=add_content
                        )
                        logger.info(f"VectorDB ì €ì¥: {add_report.get('name', '')[:50]}")
                
                # ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ì €ì¥ (ìƒˆë¡œ í¬ë¡¤ë§í•œ ê²ƒë§Œ)
                new_reports_to_save = []
                
                if naver_company_reports and not company_reports_from_cache:
                    new_reports_to_save.extend([(r, "NAVER_COMPANY") for r in naver_company_reports])
                
                if naver_industry_reports and not industry_reports_from_cache:
                    new_reports_to_save.extend([(r, "NAVER_INDUSTRY") for r in naver_industry_reports])
                
                if new_reports_to_save:
                    update_status(f"   ğŸ’¾ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ {len(new_reports_to_save)}ê°œ VectorDB ì €ì¥ ì¤‘...")
                    
                    for report, report_type_prefix in new_reports_to_save:
                        report_name = report.get('name', '')
                        report_date = report.get('date', '')
                        content = report.get('content', '')
                        
                        if content:
                            report_url = report.get('url', '')
                            rcept_no_hash = f"{report_type_prefix}_{hash(report_url)}"
                            
                            keywords_to_save = None
                            if report_type_prefix == "NAVER_INDUSTRY":
                                keywords_to_save = industry_keywords
                            
                            if not self.vector_store.check_report_exists(rcept_no_hash):
                                self.vector_store.add_report(
                                    rcept_no=rcept_no_hash,
                                    report_name=report_name,
                                    company_name=found_name,
                                    report_date=report_date,
                                    content=content,
                                    industry_keywords=keywords_to_save
                                )
                                logger.info(f"VectorDB ì €ì¥: {report_name[:50]}")
                
                update_status(f"âœ… ëª¨ë“  ë¦¬í¬íŠ¸ VectorDB ì €ì¥ ì™„ë£Œ")
                logger.info("VectorDB ì €ì¥ ì™„ë£Œ")
                
            except Exception as ve:
                logger.warning(f"VectorDB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {ve}")
                update_status(f"âš ï¸  VectorDB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")
            
            # 4. Geminië¡œ ë¶„ì„
            if all_additional_reports:
                dart_count = len(additional_reports)
                company_count = len(naver_company_reports)
                industry_count = len(naver_industry_reports)
                
                report_summary = f"DART {dart_count}ê°œ"
                if company_count > 0:
                    report_summary += f" + ì¢…ëª©ë¶„ì„ {company_count}ê°œ"
                if industry_count > 0:
                    report_summary += f" + ì‚°ì—…ë¶„ì„ {industry_count}ê°œ"
                
                # ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ë³´ê³ ì„œ ê°œìˆ˜ ë° í¬ê¸° ê¸°ë°˜)
                total_reports = 1 + len(all_additional_reports)  # ë©”ì¸ + ì¶”ê°€
                total_chars = len(report_content) + sum(len(r.get('content', '')) for r in all_additional_reports)
                
                # í† í° ìˆ˜ ì¶”ì • (í•œê¸€ì€ ì•½ 1ì = 2í† í°)
                estimated_tokens = total_chars * 2
                
                # ì‹œê°„ ì¶”ì •: 1ë°±ë§Œ í† í°ë‹¹ ì•½ 1ë¶„
                estimated_minutes = max(2, int(estimated_tokens / 500000))  # ìµœì†Œ 2ë¶„
                
                update_status(f"ğŸ¤– 4ë‹¨ê³„: Gemini AI ë¶„ì„ ì¤‘...")
                update_status(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ë©”ì¸ ë³´ê³ ì„œ + {report_summary}")
                update_status(f"   ğŸ“ ì´ í…ìŠ¤íŠ¸: {total_chars:,}ì (ì•½ {estimated_tokens:,} í† í°)")
                update_status(f"   â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {estimated_minutes}~{estimated_minutes+2}ë¶„")
                update_status(f"   ğŸ”„ Gemini API í˜¸ì¶œ ì¤‘... (ëŒ€ìš©ëŸ‰ ë³´ê³ ì„œ ì²˜ë¦¬)")
                logger.info(f"Gemini AI ë¶„ì„ ì‹œì‘ (ë©”ì¸ + {report_summary}, ì˜ˆìƒ: {estimated_minutes}ë¶„)")
            else:
                update_status(f"ğŸ¤– 4ë‹¨ê³„: Gemini AI ë¶„ì„ ì¤‘... (ë©”ì¸ ë³´ê³ ì„œë§Œ, ì•½ 1~2ë¶„ ì†Œìš”)")
                logger.info("Gemini AI ë¶„ì„ ì‹œì‘")
            
            # 4-1. VectorDBì—ì„œ ì§ˆë¬¸ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (RAG)
            update_status(f"ğŸ” VectorDBì—ì„œ ì§ˆë¬¸ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰ ì¤‘...")
            logger.info(f"VectorDB ê²€ìƒ‰ ì‹œì‘: {user_query}")
            
            try:
                # ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 20ê°œ ì²­í¬)
                relevant_chunks = self.vector_store.search_similar_reports(
                    query=user_query,
                    company_name=found_name,
                    k=20  # ìƒìœ„ 20ê°œ ì²­í¬
                )
                
                if relevant_chunks:
                    total_chunk_chars = sum(len(chunk[0].page_content) for chunk in relevant_chunks)
                    update_status(f"âœ… {len(relevant_chunks)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬ (ì´ {total_chunk_chars:,}ì)")
                    logger.info(f"VectorDB ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_chunks)}ê°œ ì²­í¬, {total_chunk_chars:,}ì")
                    
                    # ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•© (ë³´ê³ ì„œëª… ëª…í™•í•˜ê²Œ í‘œê¸°)
                    context_from_chunks = "\n\n".join([
                        f"â”â”â” ë³´ê³ ì„œ: {chunk[0].metadata.get('report_name', 'Unknown')} ({chunk[0].metadata.get('report_date', '')}) â”â”â”\n"
                        f"[Reference {idx+1}/{len(relevant_chunks)} - ì²­í¬ {chunk[0].metadata.get('chunk_index', '?')}/{chunk[0].metadata.get('total_chunks', '?')}]\n"
                        f"{chunk[0].page_content}"
                        for idx, chunk in enumerate(relevant_chunks)
                    ])
                    
                    # Gemini ë¶„ì„ (ê´€ë ¨ ì²­í¬ë§Œ ì‚¬ìš©)
                    update_status(f"ğŸ¤– Gemini AI ë¶„ì„ ì¤‘... (ê²€ìƒ‰ëœ ê´€ë ¨ ë‚´ìš©ë§Œ ì‚¬ìš©)")
                    update_status(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°: {len(relevant_chunks)}ê°œ ì²­í¬, {total_chunk_chars:,}ì")
                    
                    analysis = self.analyze_with_gemini_rag(
                        company_name=found_name,
                        user_query=user_query,
                        relevant_context=context_from_chunks,
                        num_chunks=len(relevant_chunks)
                    )
                    
                    logger.info(f"Gemini AI ë¶„ì„ ì™„ë£Œ: {len(analysis)}ì")
                    update_status(f"âœ… AI ë¶„ì„ ì™„ë£Œ!")
                    
                else:
                    update_status(f"âš ï¸  ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. VectorDBì— ë°ì´í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    analysis = "VectorDBì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¦¬í¬íŠ¸ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                    
            except Exception as e:
                logger.error(f"VectorDB ê²€ìƒ‰ ë˜ëŠ” Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
                update_status(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}")
                analysis = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            result['analysis'] = analysis
            result['success'] = True
            logger.info("ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ (RAG ëª¨ë“œ)")
            
            # ===== ë””ë²„ê¹…ìš© MD íŒŒì¼ ì €ì¥ (ì„ íƒì ) =====
            # í™˜ê²½ ë³€ìˆ˜ SAVE_DEBUG_REPORTS=Trueë¡œ ì„¤ì •í•˜ë©´ í™œì„±í™”
            # ì•„ë˜ ì½”ë“œ ì „ì²´ê°€ ì£¼ì„ì²˜ë¦¬ë¨
            """
            if os.getenv('SAVE_DEBUG_REPORTS', 'False').lower() == 'true':
                # ë””ë²„ê¹… ì½”ë“œ...
                main_zip_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report_ORIGINAL.zip")
            extracted_dir = os.path.join("downloads", f"{rcept_no}_extracted")
            zip_source = os.path.join("downloads", f"{rcept_no}.zip")
            
            zip_saved = False
            if os.path.exists(zip_source):
                shutil.copy2(zip_source, main_zip_dest)
                zip_saved = True
                logger.info(f"ì›ë³¸ ZIP ë³µì‚¬: {zip_source} â†’ {main_zip_dest}")
            
            # 1-2. ì›ë³¸ XML íŒŒì¼ ë³µì‚¬
            main_xml_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report_ORIGINAL.xml")
            xml_source = os.path.join(extracted_dir, f"{rcept_no}.xml")
            
            xml_saved = False
            if os.path.exists(xml_source):
                shutil.copy2(xml_source, main_xml_dest)
                xml_saved = True
                logger.info(f"ì›ë³¸ XML ë³µì‚¬: {xml_source} â†’ {main_xml_dest}")
            
            # 1-3. ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì €ì¥ (XML â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼)
            main_raw_path = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report_CONVERTED.txt")
            with open(main_raw_path, 'w', encoding='utf-8') as f:
                f.write(f"=== {found_name} - ë©”ì¸ ë³´ê³ ì„œ (XML â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼) ===\n\n")
                f.write(f"ë³´ê³ ì„œëª…: {report_name}\n")
                f.write(f"ë³´ê³ ì„œ ì½”ë“œ: {rcept_no}\n")
                f.write(f"ë¬¸ì ìˆ˜: {len(report_content):,}ì\n\n")
                f.write("=" * 80 + "\n\n")
                f.write(report_content)
            
            # 1-4. MD íŒŒì¼ ì €ì¥ (ì½ê¸° ì‰½ê²Œ ì •ë¦¬ëœ ë²„ì „)
            main_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_main_report.md")
            with open(main_report_path, 'w', encoding='utf-8') as f:
                f.write(f"# {found_name} - ë©”ì¸ ë³´ê³ ì„œ\n\n")
                f.write(f"**ë³´ê³ ì„œëª…**: {report_name}\n\n")
                f.write(f"**ë³´ê³ ì„œ ì½”ë“œ**: {rcept_no}\n\n")
                f.write(f"**ë¬¸ì ìˆ˜**: {len(report_content):,}ì\n\n")
                f.write("---\n\n")
                f.write(report_content)
            
            files_saved = []
            if zip_saved: files_saved.append("ZIP")
            if xml_saved: files_saved.append("XML")
            files_saved.extend(["TXT", "MD"])
            
                update_status(f"   âœ… ë©”ì¸ ë³´ê³ ì„œ ì €ì¥: {' + '.join(files_saved)}")
                logger.info(f"ë©”ì¸ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {' + '.join(files_saved)} ({len(report_content):,}ì)")
                
                # 2. ì¶”ê°€ DART ë³´ê³ ì„œ ì €ì¥ (ì›ë³¸ ZIP + XML + ë³€í™˜ í›„ í…ìŠ¤íŠ¸ + MD)
            if additional_reports:
                update_status(f"   ğŸ“„ ì¶”ê°€ DART ë³´ê³ ì„œ {len(additional_reports)}ê°œ ì €ì¥ ì¤‘...")
                for idx, add_report in enumerate(additional_reports, 1):
                    content = add_report.get('content', '')
                    add_rcept_no = add_report.get('rcept_no', 'N/A')
                    add_name = add_report.get('name', 'Unknown')
                    add_date = add_report.get('date', 'N/A')
                    
                    # 2-1. ì›ë³¸ ZIP íŒŒì¼ ë³µì‚¬
                    add_zip_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}_ORIGINAL.zip")
                    add_zip_source = os.path.join("downloads", f"{add_rcept_no}.zip")
                    
                    add_zip_saved = False
                    if os.path.exists(add_zip_source):
                        shutil.copy2(add_zip_source, add_zip_dest)
                        add_zip_saved = True
                    
                    # 2-2. ì›ë³¸ XML íŒŒì¼ ë³µì‚¬
                    add_xml_dest = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}_ORIGINAL.xml")
                    add_extracted_dir = os.path.join("downloads", f"{add_rcept_no}_extracted")
                    add_xml_source = os.path.join(add_extracted_dir, f"{add_rcept_no}.xml")
                    
                    add_xml_saved = False
                    if os.path.exists(add_xml_source):
                        shutil.copy2(add_xml_source, add_xml_dest)
                        add_xml_saved = True
                    
                    # 2-3. ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì €ì¥
                    add_converted_path = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}_CONVERTED.txt")
                    with open(add_converted_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== {found_name} - DART ì¶”ê°€ ë³´ê³ ì„œ #{idx} (XML â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼) ===\n\n")
                        f.write(f"ë³´ê³ ì„œëª…: {add_name}\n")
                        f.write(f"ë³´ê³ ì„œ ì½”ë“œ: {add_rcept_no}\n")
                        f.write(f"ë³´ê³ ì„œ ë‚ ì§œ: {add_date}\n")
                        f.write(f"ë¬¸ì ìˆ˜: {len(content):,}ì\n\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(content)
                    
                    # 2-4. MD íŒŒì¼ ì €ì¥
                    add_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_dart_{idx}.md")
                    with open(add_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {found_name} - DART ì¶”ê°€ ë³´ê³ ì„œ #{idx}\n\n")
                        f.write(f"**ë³´ê³ ì„œëª…**: {add_name}\n\n")
                        f.write(f"**ë³´ê³ ì„œ ì½”ë“œ**: {add_rcept_no}\n\n")
                        f.write(f"**ë³´ê³ ì„œ ë‚ ì§œ**: {add_date}\n\n")
                        f.write(f"**ë¬¸ì ìˆ˜**: {len(content):,}ì\n\n")
                        f.write("---\n\n")
                        f.write(content)
                    
                    add_files_saved = []
                    if add_zip_saved: add_files_saved.append("ZIP")
                    if add_xml_saved: add_files_saved.append("XML")
                    add_files_saved.extend(["TXT", "MD"])
                    
                    logger.info(f"DART ì¶”ê°€ #{idx}: {add_name[:50]} ({' + '.join(add_files_saved)}, {len(content):,}ì)")
                
                update_status(f"   âœ… DART ë³´ê³ ì„œ {len(additional_reports)}ê°œ ì €ì¥ ì™„ë£Œ (ZIP + XML + TXT + MD)")
            
            # 3. ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ (PDF ì¶”ì¶œ í…ìŠ¤íŠ¸ + MD)
            if naver_company_reports:
                update_status(f"   ğŸ“Š ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ì €ì¥ ì¤‘...")
                for idx, company_report in enumerate(naver_company_reports, 1):
                    content = company_report.get('content', '')
                    comp_name = company_report.get('name', 'Unknown')
                    comp_date = company_report.get('date', 'N/A')
                    comp_url = company_report.get('url', 'N/A')
                    
                    # 3-1. PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì €ì¥
                    company_converted_path = os.path.join(output_dir, f"{timestamp}_{found_name}_company_{idx}_CONVERTED.txt")
                    with open(company_converted_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== {found_name} - ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ #{idx} (PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼) ===\n\n")
                        f.write(f"ë¦¬í¬íŠ¸ëª…: {comp_name}\n")
                        f.write(f"ë°œí–‰ì¼: {comp_date}\n")
                        f.write(f"URL: {comp_url}\n")
                        f.write(f"ë¬¸ì ìˆ˜: {len(content):,}ì\n\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(content)
                    
                    # 3-2. MD íŒŒì¼ ì €ì¥
                    company_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_company_{idx}.md")
                    with open(company_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {found_name} - ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ #{idx}\n\n")
                        f.write(f"**ë¦¬í¬íŠ¸ëª…**: {comp_name}\n\n")
                        f.write(f"**ë°œí–‰ì¼**: {comp_date}\n\n")
                        f.write(f"**URL**: {comp_url}\n\n")
                        f.write(f"**ë¬¸ì ìˆ˜**: {len(content):,}ì\n\n")
                        f.write("---\n\n")
                        f.write(content)
                    
                    logger.info(f"ì¢…ëª©ë¶„ì„ #{idx}: {comp_name[:50]} (TXT + MD, {len(content):,}ì)")
                
                update_status(f"   âœ… ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_company_reports)}ê°œ ì €ì¥ ì™„ë£Œ (TXT + MD)")
            
            # 4. ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ (PDF ì¶”ì¶œ í…ìŠ¤íŠ¸ + MD)
            if naver_industry_reports:
                update_status(f"   ğŸ­ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ì €ì¥ ì¤‘...")
                for idx, industry_report in enumerate(naver_industry_reports, 1):
                    content = industry_report.get('content', '')
                    ind_name = industry_report.get('name', 'Unknown')
                    ind_date = industry_report.get('date', 'N/A')
                    ind_url = industry_report.get('url', 'N/A')
                    
                    # 4-1. PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì €ì¥
                    industry_converted_path = os.path.join(output_dir, f"{timestamp}_{found_name}_industry_{idx}_CONVERTED.txt")
                    with open(industry_converted_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== {found_name} - ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ #{idx} (PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼) ===\n\n")
                        f.write(f"ë¦¬í¬íŠ¸ëª…: {ind_name}\n")
                        f.write(f"ë°œí–‰ì¼: {ind_date}\n")
                        f.write(f"URL: {ind_url}\n")
                        f.write(f"ë¬¸ì ìˆ˜: {len(content):,}ì\n\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(content)
                    
                    # 4-2. MD íŒŒì¼ ì €ì¥
                    industry_report_path = os.path.join(output_dir, f"{timestamp}_{found_name}_industry_{idx}.md")
                    with open(industry_report_path, 'w', encoding='utf-8') as f:
                        f.write(f"# {found_name} - ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ #{idx}\n\n")
                        f.write(f"**ë¦¬í¬íŠ¸ëª…**: {ind_name}\n\n")
                        f.write(f"**ë°œí–‰ì¼**: {ind_date}\n\n")
                        f.write(f"**URL**: {ind_url}\n\n")
                        f.write(f"**ë¬¸ì ìˆ˜**: {len(content):,}ì\n\n")
                        f.write("---\n\n")
                        f.write(content)
                    
                    logger.info(f"ì‚°ì—…ë¶„ì„ #{idx}: {ind_name[:50]} (TXT + MD, {len(content):,}ì)")
                
                update_status(f"   âœ… ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ {len(naver_industry_reports)}ê°œ ì €ì¥ ì™„ë£Œ (TXT + MD)")
            
            # 5. ì „ì²´ í†µí•© ìš”ì•½ íŒŒì¼ ì €ì¥
            summary_path = os.path.join(output_dir, f"{timestamp}_{found_name}_SUMMARY.md")
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(f"# {found_name} - ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½\n\n")
                f.write(f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"**ì‚¬ìš©ì ì§ˆë¬¸**: {user_query}\n\n")
                f.write(f"---\n\n")
                
                # í†µê³„
                f.write(f"## ğŸ“Š ìˆ˜ì§‘ í†µê³„\n\n")
                f.write(f"| êµ¬ë¶„ | ê°œìˆ˜ | ì´ ë¬¸ì ìˆ˜ |\n")
                f.write(f"|------|------|------------|\n")
                f.write(f"| ë©”ì¸ DART ë³´ê³ ì„œ | 1ê°œ | {len(report_content):,}ì |\n")
                f.write(f"| ì¶”ê°€ DART ë³´ê³ ì„œ | {len(additional_reports)}ê°œ | {sum(len(r.get('content', '')) for r in additional_reports):,}ì |\n")
                f.write(f"| ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ | {len(naver_company_reports)}ê°œ | {sum(len(r.get('content', '')) for r in naver_company_reports):,}ì |\n")
                f.write(f"| ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ | {len(naver_industry_reports)}ê°œ | {sum(len(r.get('content', '')) for r in naver_industry_reports):,}ì |\n")
                f.write(f"| **ì „ì²´ í•©ê³„** | **{1 + len(all_additional_reports)}ê°œ** | **{total_chars:,}ì** |\n\n")
                
                f.write(f"**ì˜ˆìƒ í† í° ìˆ˜**: {estimated_tokens:,} í† í°\n\n")
                f.write(f"---\n\n")
                
                # ë©”ì¸ ë³´ê³ ì„œ ì •ë³´
                f.write(f"## ğŸ“„ ë©”ì¸ DART ë³´ê³ ì„œ\n\n")
                f.write(f"- **ZIP ì›ë³¸**: `{timestamp}_{found_name}_main_report_ORIGINAL.zip`\n")
                f.write(f"- **XML ì›ë³¸**: `{timestamp}_{found_name}_main_report_ORIGINAL.xml`\n")
                f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_main_report_CONVERTED.txt`\n")
                f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_main_report.md`\n")
                f.write(f"- **ë³´ê³ ì„œëª…**: {report_name}\n")
                f.write(f"- **ë³´ê³ ì„œ ì½”ë“œ**: {rcept_no}\n")
                f.write(f"- **ë¬¸ì ìˆ˜**: {len(report_content):,}ì\n\n")
                
                # ì¶”ê°€ DART ë³´ê³ ì„œ ëª©ë¡
                if additional_reports:
                    f.write(f"## ğŸ“š ì¶”ê°€ DART ë³´ê³ ì„œ ({len(additional_reports)}ê°œ)\n\n")
                    for idx, rep in enumerate(additional_reports, 1):
                        f.write(f"### {idx}. {rep.get('name', 'Unknown')}\n")
                        f.write(f"- **ZIP ì›ë³¸**: `{timestamp}_{found_name}_dart_{idx}_ORIGINAL.zip`\n")
                        f.write(f"- **XML ì›ë³¸**: `{timestamp}_{found_name}_dart_{idx}_ORIGINAL.xml`\n")
                        f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_dart_{idx}_CONVERTED.txt`\n")
                        f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_dart_{idx}.md`\n")
                        f.write(f"- **ë³´ê³ ì„œ ì½”ë“œ**: {rep.get('rcept_no', 'N/A')}\n")
                        f.write(f"- **ë‚ ì§œ**: {rep.get('date', 'N/A')}\n")
                        f.write(f"- **ë¬¸ì ìˆ˜**: {len(rep.get('content', '')):,}ì\n\n")
                
                # ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ëª©ë¡
                if naver_company_reports:
                    f.write(f"## ğŸ“ˆ ì¦ê¶Œì‚¬ ì¢…ëª©ë¶„ì„ ë¦¬í¬íŠ¸ ({len(naver_company_reports)}ê°œ)\n\n")
                    for idx, rep in enumerate(naver_company_reports, 1):
                        f.write(f"### {idx}. {rep.get('name', 'Unknown')}\n")
                        f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_company_{idx}_CONVERTED.txt`\n")
                        f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_company_{idx}.md`\n")
                        f.write(f"- **ë°œí–‰ì¼**: {rep.get('date', 'N/A')}\n")
                        f.write(f"- **URL**: {rep.get('url', 'N/A')}\n")
                        f.write(f"- **ë¬¸ì ìˆ˜**: {len(rep.get('content', '')):,}ì\n\n")
                
                # ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ëª©ë¡
                if naver_industry_reports:
                    f.write(f"## ğŸ­ ì¦ê¶Œì‚¬ ì‚°ì—…ë¶„ì„ ë¦¬í¬íŠ¸ ({len(naver_industry_reports)}ê°œ)\n\n")
                    for idx, rep in enumerate(naver_industry_reports, 1):
                        f.write(f"### {idx}. {rep.get('name', 'Unknown')}\n")
                        f.write(f"- **ë³€í™˜ í›„ í…ìŠ¤íŠ¸**: `{timestamp}_{found_name}_industry_{idx}_CONVERTED.txt`\n")
                        f.write(f"- **MD íŒŒì¼**: `{timestamp}_{found_name}_industry_{idx}.md`\n")
                        f.write(f"- **ë°œí–‰ì¼**: {rep.get('date', 'N/A')}\n")
                        f.write(f"- **URL**: {rep.get('url', 'N/A')}\n")
                        f.write(f"- **ë¬¸ì ìˆ˜**: {len(rep.get('content', '')):,}ì\n\n")
                
                # ì €ì¥ ìœ„ì¹˜
                f.write(f"---\n\n")
                f.write(f"## ğŸ“ ì €ì¥ ìœ„ì¹˜\n\n")
                f.write(f"ëª¨ë“  íŒŒì¼ì€ `{os.path.abspath(output_dir)}/` í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            
            update_status(f"âœ… ëª¨ë“  ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ!")
            update_status(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(output_dir)}/")
            update_status(f"   ğŸ“„ ìš”ì•½ íŒŒì¼: {summary_path}")
            update_status(f"   ì´ {1 + len(all_additional_reports)}ê°œ íŒŒì¼ ({total_chars:,}ì)")
            
            logger.info(f"ëª¨ë“  ë¦¬í¬íŠ¸ MD íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_dir}/")
            logger.info(f"  - ë©”ì¸: {len(report_content):,}ì")
            logger.info(f"  - DART ì¶”ê°€: {len(additional_reports)}ê°œ")
            logger.info(f"  - ì¢…ëª©ë¶„ì„: {len(naver_company_reports)}ê°œ")
            logger.info(f"  - ì‚°ì—…ë¶„ì„: {len(naver_industry_reports)}ê°œ")
            
                # ë””ë²„ê¹… ì½”ë“œ ë
                pass
            """
            # ë””ë²„ê¹…ìš© MD íŒŒì¼ ì €ì¥ ì½”ë“œ ë
            
            # ë¶„ì„ ì™„ë£Œ í›„ ì˜¤ë˜ëœ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ (ìµœì‹  5ê°œë§Œ ìœ ì§€)
            update_status(f"ğŸ§¹ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
            try:
                self.cleanup_downloads(keep_latest=5)
            except Exception as cleanup_error:
                logger.warning(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {cleanup_error}")
            
            return result
            
        except Exception as e:
            error_msg = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg, exc_info=True)
            result['error'] = error_msg
            return result

